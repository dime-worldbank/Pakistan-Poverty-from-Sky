{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Satellite Imagery to Survey Locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>To authorize access needed by Earth Engine, open the following\n",
       "        URL in a web browser and follow the instructions:</p>\n",
       "        <p><a href=https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=IoFQzsT_pgT5cpoY8OXYPxabMwLDLhkh4niLxdocn5A&tc=H4coJcYAohbwYq9u0Dho81dGBVU8eT3q-1p75etbhD4&cc=csdMQHb0faubmJVGhGeb5xAFVJiOWMvLln09dUjbvWE>https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=IoFQzsT_pgT5cpoY8OXYPxabMwLDLhkh4niLxdocn5A&tc=H4coJcYAohbwYq9u0Dho81dGBVU8eT3q-1p75etbhD4&cc=csdMQHb0faubmJVGhGeb5xAFVJiOWMvLln09dUjbvWE</a></p>\n",
       "        <p>The authorization workflow will generate a code, which you\n",
       "        should paste in the box below</p>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter verification code:  4/1AX4XfWgoMXPqPGR3GqiXj4N_nhIR56fntLBQIXOtCqtmvZDZ0Np6lrgkszU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully saved authorization token.\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "ee.Authenticate()\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import geetools\n",
    "from geetools import ui, cloud_mask\n",
    "import os, datetime\n",
    "import config as cf\n",
    "import pandas as pd\n",
    "import time\n",
    "import geopandas as gpd\n",
    "#import eeconvert\n",
    "\n",
    "cloud_mask_landsatSR = cloud_mask.landsatSR()\n",
    "cloud_mask_sentinel2 = cloud_mask.sentinel2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SURVEY_NAME = 'DHS' # 'DHS', 'PAK_POINTS'\n",
    "REEXTRACT_IF_FILE_EXISTS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gis.stackexchange.com/questions/257727/iterate-over-imagecollection-returning-pandas-dataframe-using-earth-engine-pyt\n",
    "def fc2df(fc):\n",
    "    # Convert a FeatureCollection into a pandas DataFrame\n",
    "    # Features is a list of dict with the output\n",
    "    features = fc.getInfo()['features']\n",
    "\n",
    "    dictarr = []\n",
    "\n",
    "    for f in features:\n",
    "        # Store all attributes in a dict\n",
    "        attr = f['properties']\n",
    "        # and treat geometry separately\n",
    "        attr['geometry'] = f['geometry']  # GeoJSON Feature!\n",
    "        # attr['geometrytype'] = f['geometry']['type']\n",
    "        dictarr.append(attr)\n",
    "\n",
    "    df = gpd.GeoDataFrame(dictarr)\n",
    "    # Convert GeoJSON features to shape\n",
    "    df = df.drop(columns=['geometry'])\n",
    "    return df\n",
    "\n",
    "def survey_to_fc_buffer(survey_df, buffer_size_urban, buffer_size_rural):\n",
    "    '''\n",
    "    Convert pandas dataframe of survey locations to a feature collection. \n",
    "    \n",
    "    Inputs:\n",
    "        survey_df: pandas dataframe of survey locations. Function assumes \n",
    "                   the dataframe contains (1) latitude, (2) longitude and\n",
    "                   (3) uid variables. Assumes coordinates in WGS84.\n",
    "    Returns:\n",
    "        (feature collection)\n",
    "    '''\n",
    "    \n",
    "    survey_fc_list = []\n",
    "    \n",
    "    n_rows = survey_df.shape[0]\n",
    "    for i in range(0, n_rows):\n",
    "        survey_df_i = survey_df.iloc[[i]]\n",
    "        \n",
    "        #ur = survey_df_i['urban_rural'].iloc[0]\n",
    "        #if ur == 'U':\n",
    "        #    buffer_size = buffer_size_urban\n",
    "        #elif ur == 'R':\n",
    "        #    buffer_size = buffer_size_rural\n",
    "        buffer_size = buffer_size_urban\n",
    "\n",
    "        f_i = ee.Feature(ee.Geometry.Point([survey_df_i['longitude'].iloc[0], \n",
    "                                            survey_df_i['latitude'].iloc[0]]), \n",
    "                         {'uid': survey_df_i['uid'].iloc[0],\n",
    "                          'year': str(survey_df_i['year'].iloc[0])})\n",
    "        \n",
    "        f_i = f_i.buffer(buffer_size)\n",
    "\n",
    "        survey_fc_list.append(f_i)\n",
    "        \n",
    "    survey_fc = ee.FeatureCollection(survey_fc_list)\n",
    "    \n",
    "    return survey_fc\n",
    "\n",
    "def extract_sat(survey_df, buffer_size_urban, buffer_size_rural, year, satellite, survey_name, file_name):\n",
    "    '''\n",
    "    Extract satellite imagery to locations \n",
    "    \n",
    "    Inputs:\n",
    "        survey_df: pandas dataframe of survey locations. Function assumes \n",
    "                   the dataframe contains (1) latitude, (2) longitude and\n",
    "                   (3) uid variables. Assumes coordinates in WGS84.\n",
    "    Returns:\n",
    "        (feature collection)\n",
    "    '''\n",
    "        \n",
    "    year_start_sp5 = \"2018-01-01\"\n",
    "    year_end_sp5 = '2020-12-31'\n",
    "    \n",
    "    # Prep worldpop -----------------------------------------------\n",
    "    if satellite == 'worldpop':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 100 \n",
    "        \n",
    "        # Year\n",
    "        year_use = year\n",
    "        \n",
    "        year_plus = year_use\n",
    "        year_minus = year_use\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('WorldPop/GP/100m/pop')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .median()\n",
    "        \n",
    "        # After the reducer computers the sum, it names the value \"sum\", not population\n",
    "        BANDS = ['sum']\n",
    "        \n",
    "    # Prep worldpop_2020 ---------------------------------------------\n",
    "    if satellite == 'worldpop2020':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 100 \n",
    "        \n",
    "        # Year\n",
    "        year_use = '2020'\n",
    "        \n",
    "        year_plus = year_use\n",
    "        year_minus = year_use\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('WorldPop/GP/100m/pop')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .median()\n",
    "        \n",
    "        # After the reducer computers the sum, it names the value \"sum\", not population\n",
    "        BANDS = ['sum']\n",
    "            \n",
    "    # Sentinel-5P OFFL AER AI: Offline UV Aerosol Index  -------------------\n",
    "    if satellite == 'uv_aer':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 1000 # ok to upscale\n",
    "        \n",
    "        # Starts in 2018; take all years\n",
    "        image = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_AER_AI\")\\\n",
    "            .filterDate(year_start_sp5, year_end_sp5)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['absorbing_aerosol_index']\n",
    "        \n",
    "    # Sentinel-5P OFFL CO: Offline Carbon Monoxide  -------------------\n",
    "    if satellite == 'CO':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 1000 # ok to upscale\n",
    "        \n",
    "        # Starts in 2018; take all years\n",
    "        image = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_CO\")\\\n",
    "            .filterDate(year_start_sp5, year_end_sp5)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['CO_column_number_density', 'H2O_column_number_density']\n",
    "        \n",
    "    # Sentinel-5P OFFL HCHO: Offline Formaldehyde  -------------------\n",
    "    if satellite == 'HCHO':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 1000 # ok to upscale\n",
    "        \n",
    "        # Starts in 2018; take all years\n",
    "        image = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_HCHO\")\\\n",
    "            .filterDate(year_start_sp5, year_end_sp5)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['tropospheric_HCHO_column_number_density', 'tropospheric_HCHO_column_number_density_amf']\n",
    "        \n",
    "    # Sentinel-5P Nitrogen Dioxide  -----------------------------\n",
    "    if satellite == 'NO2':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 1000 # ok to upscale\n",
    "        \n",
    "        # Starts in 2018; take all years\n",
    "        image = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_NO2\")\\\n",
    "            .filterDate(year_start_sp5, year_end_sp5)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['NO2_column_number_density', 'tropospheric_NO2_column_number_density',\\\n",
    "                 'stratospheric_NO2_column_number_density', 'NO2_slant_column_number_density']\n",
    "        \n",
    "    # Sentinel-5P OFFL O3: Offline Ozone  -------------------\n",
    "    if satellite == 'ozone':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 1000 # ok to upscale\n",
    "        \n",
    "        # Starts in 2018; take all years\n",
    "        image = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_O3\")\\\n",
    "            .filterDate(year_start_sp5, year_end_sp5)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['O3_column_number_density', 'O3_effective_temperature']\n",
    "        \n",
    "    # Sentinel-5P OFFL SO2: Offline Sulphur Dioxide  -------------------\n",
    "    if satellite == 'SO2':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 1000 # ok to upscale\n",
    "        \n",
    "        # Starts in 2018; take all years\n",
    "        image = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_SO2\")\\\n",
    "            .filterDate(year_start_sp5, year_end_sp5)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['SO2_column_number_density', 'SO2_column_number_density_amf', 'SO2_slant_column_number_density']\n",
    "        \n",
    "    # Sentinel-5P OFFL CH4: Offline Methane  -------------------\n",
    "    if satellite == 'CH4':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 1000 # ok to upscale\n",
    "        \n",
    "        # Starts in 2018; take all years\n",
    "        image = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_CH4\")\\\n",
    "            .filterDate(year_start_sp5, year_end_sp5)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['CH4_column_volume_mixing_ratio_dry_air']\n",
    "        \n",
    "    # CSP gHM: Global Human Modification ---------------------------------\n",
    "    if satellite == 'GlobalHumanModification':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 1000 # ok to upscale\n",
    "                \n",
    "        image = ee.ImageCollection(\"CSP/HM/GlobalHumanModification\")\\\n",
    "            .median()\n",
    "        \n",
    "        # Original name is \"gHM\", but because only one value, it takes the\n",
    "        # name of the reducer; we use mean\n",
    "        BANDS = ['mean']\n",
    "        \n",
    "    # WorldClim BIO Variables V1 ---------------------------------\n",
    "    if satellite == 'worldclim_bio':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 1000 # ok to upscale\n",
    "                \n",
    "        image = ee.Image('WORLDCLIM/V1/BIO')\n",
    "        \n",
    "        BANDS = ['bio01', 'bio02', 'bio03', 'bio04', 'bio05', 'bio06', 'bio07', 'bio08', 'bio09', 'bio10',\\\n",
    "                 'bio11', 'bio12', 'bio13', 'bio14', 'bio15', 'bio16', 'bio17', 'bio18', 'bio19']\n",
    "        \n",
    "    # Elevation - SRTM ------------------------------------------\n",
    "    if satellite == 'elevation':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 1000 # ok to upscale\n",
    "                \n",
    "        image = ee.Image('USGS/SRTMGL1_003') # CGIAR/SRTM90_V4\n",
    "        \n",
    "        # elevation?\n",
    "        BANDS = ['mean']\n",
    "        \n",
    "    # Elevation - SRTM ------------------------------------------\n",
    "    if satellite == 'slope':\n",
    "        # https://developers.google.com/earth-engine/datasets/catalog/CGIAR_SRTM90_V4#description\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 500 # ok to upscale\n",
    "                \n",
    "        image_raw = ee.Image('USGS/SRTMGL1_003') # CGIAR/SRTM90_V4\n",
    "        image_elev = image_raw.select('elevation')\n",
    "        image = ee.Terrain.slope(image_elev)\n",
    "                \n",
    "        # mean?\n",
    "        BANDS = ['mean']\n",
    "        \n",
    "    # Prep l5 ---------------------------------------------------\n",
    "    if satellite == 'l5':\n",
    "        \n",
    "        SCALE = 100 # ok to upscale\n",
    "        #SCALE = 2000\n",
    "        \n",
    "        ### Year\n",
    "        # (1) landsat 5 starts in March 1984; if year is less than\n",
    "        #     1985, use 1985 as year (to ensure have year before and after)\n",
    "        # (2) landsat 5 ends in May 2012; if year is greater than\n",
    "        #     2011, use 2011 as year\n",
    "        if year < 1985:\n",
    "            year_use = 1985\n",
    "        elif year > 2011:\n",
    "            year_use = 2011\n",
    "        else:\n",
    "            year_use = year\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        #image = ee.ImageCollection('LANDSAT/LT05/C02/T1_L2')\\\n",
    "        #    .filterDate(year_minus_str, year_plus_str)\\\n",
    "        #    #.map(cloud_mask_landsatSR)\\\n",
    "        #    .median()\\\n",
    "        #    .multiply(0.0001)\n",
    "        \n",
    "        image = ee.ImageCollection('LANDSAT/LT05/C02/T1_L2')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .median()\\\n",
    "            .multiply(0.0001)\n",
    "\n",
    "        # https://www.linkedin.com/pulse/ndvi-ndbi-ndwi-calculation-using-landsat-7-8-tek-bahadur-kshetri\n",
    "        ndvi = image.normalizedDifference(['B4', 'B3']).rename('NDVI');\n",
    "        ndbi = image.normalizedDifference(['B5', 'B4']).rename('NDBI');\n",
    "        image = image.addBands(ndvi)\n",
    "        image = image.addBands(ndbi)\n",
    "        \n",
    "        bu = image.select('NDBI').subtract(image.select('NDVI')).rename('BU')\n",
    "        image = image.addBands(bu)\n",
    "        \n",
    "        BANDS = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'NDVI', 'NDBI', 'BU']\n",
    "        \n",
    "    # Prep l7 ---------------------------------------------------\n",
    "    if (satellite == 'l7') | (satellite == 'l7_sdspace'):\n",
    "        \n",
    "        SCALE = 100 # ok to upscale\n",
    "        #SCALE = 2000\n",
    "        \n",
    "        # Year\n",
    "        # landsat 7 starts in May 1999; if year is less than\n",
    "        # 2000, use 2000 as year (to ensure have year before and after)\n",
    "        if year < 2000:\n",
    "            year_use = 2000\n",
    "        else:\n",
    "            year_use = year\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('LANDSAT/LE07/C01/T1_SR')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .map(cloud_mask_landsatSR)\\\n",
    "            .median()\\\n",
    "            .multiply(0.0001)\n",
    "\n",
    "        # https://www.linkedin.com/pulse/ndvi-ndbi-ndwi-calculation-using-landsat-7-8-tek-bahadur-kshetri\n",
    "        ndvi = image.normalizedDifference(['B4', 'B3']).rename('NDVI');\n",
    "        ndbi = image.normalizedDifference(['B5', 'B4']).rename('NDBI');\n",
    "        image = image.addBands(ndvi)\n",
    "        image = image.addBands(ndbi)\n",
    "        \n",
    "        bu = image.select('NDBI').subtract(image.select('NDVI')).rename('BU')\n",
    "        image = image.addBands(bu)\n",
    "        \n",
    "        BANDS = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'NDVI', 'NDBI', 'BU']\n",
    "        \n",
    "    # Prep l7 ---------------------------------------------------\n",
    "    if satellite == 'l7_sdtime':\n",
    "        \n",
    "        SCALE = 100 # ok to upscale\n",
    "        #SCALE = 2000\n",
    "        \n",
    "        # Year\n",
    "        # landsat 7 starts in May 1999; if year is less than\n",
    "        # 2000, use 2000 as year (to ensure have year before and after)\n",
    "        if year < 2000:\n",
    "            year_use = 2000\n",
    "        else:\n",
    "            year_use = year\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('LANDSAT/LE07/C01/T1_SR')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .map(cloud_mask_landsatSR)\\\n",
    "            .reduce(ee.Reducer.stdDev())\n",
    "        \n",
    "        BANDS = ['B1_stdDev', 'B2_stdDev', 'B3_stdDev', 'B4_stdDev', 'B5_stdDev', 'B6_stdDev', 'B7_stdDev']\n",
    "                \n",
    "    # Prep l8 ---------------------------------------------------\n",
    "    if (satellite == 'l8') | (satellite == 'l8_sdspace'):\n",
    "        \n",
    "        SCALE = 100 # ok to upscale\n",
    "        #SCALE = 2000\n",
    "        \n",
    "        # Year\n",
    "        # landsat 8 starts in April 2013; if year is less than\n",
    "        # 2014, use 2014 as year (to ensure have year before and after)\n",
    "        if year < 2014:\n",
    "            year_use = 2014\n",
    "        else:\n",
    "            year_use = year\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .map(cloud_mask_landsatSR)\\\n",
    "            .median()\\\n",
    "            .multiply(0.0001)\n",
    "\n",
    "        # https://www.linkedin.com/pulse/ndvi-ndbi-ndwi-calculation-using-landsat-7-8-tek-bahadur-kshetri\n",
    "        ndvi = image.normalizedDifference(['B5', 'B4']).rename('NDVI');\n",
    "        ndbi = image.normalizedDifference(['B6', 'B5']).rename('NDBI');\n",
    "        image = image.addBands(ndvi)\n",
    "        image = image.addBands(ndbi)\n",
    "        \n",
    "        bu = image.select('NDBI').subtract(image.select('NDVI')).rename('BU')\n",
    "        image = image.addBands(bu)\n",
    "        \n",
    "        BANDS = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B10', 'B11', 'NDVI', 'NDBI', 'BU']\n",
    "        \n",
    "    # Prep l8 ---------------------------------------------------\n",
    "    if satellite == 'l8_sdtime':\n",
    "        \n",
    "        SCALE = 100 # ok to upscale\n",
    "        #SCALE = 2000\n",
    "        \n",
    "        # Year\n",
    "        # landsat 8 starts in April 2013; if year is less than\n",
    "        # 2014, use 2014 as year (to ensure have year before and after)\n",
    "        if year < 2014:\n",
    "            year_use = 2014\n",
    "        else:\n",
    "            year_use = year\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .map(cloud_mask_landsatSR)\\\n",
    "            .reduce(ee.Reducer.stdDev())\n",
    "        \n",
    "        BANDS = ['B1_stdDev', 'B2_stdDev', 'B3_stdDev', 'B4_stdDev', 'B5_stdDev', 'B6_stdDev', 'B7_stdDev', 'B10_stdDev', 'B11_stdDev']\n",
    "                \n",
    "    # Prep s2 ---------------------------------------------------\n",
    "    if satellite == 's2':\n",
    "        \n",
    "        SCALE = 100 # ok to upscale\n",
    "        \n",
    "        # Year\n",
    "        # sentinel starts in March 2017; juse use 2018\n",
    "        year_use = 2018\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-12-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('COPERNICUS/S2_SR')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .map(cloud_mask_sentinel2)\\\n",
    "            .median()\\\n",
    "            .multiply(0.0001)\n",
    "        \n",
    "        ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI');\n",
    "        image = image.addBands(ndvi)\n",
    "        \n",
    "        BANDS = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12', 'AOT', 'NDVI']\n",
    "\n",
    "        image = image.select(BANDS) \n",
    "        \n",
    "    # Prep drought ---------------------------------------------------\n",
    "    if satellite == 'gridmet_drought':\n",
    "        \n",
    "        SCALE = 5000 \n",
    "\n",
    "        year_minus_str = str(year) + '-01-01'\n",
    "        year_plus_str = str(year) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection(\"GRIDMET/DROUGHT\")\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['pdsi', 'z', 'eddi1y', 'eddi2y', 'eddi5y']\n",
    "        \n",
    "    # Prep AOD ------------------------------------------------------\n",
    "    if satellite == 'aod':\n",
    "        \n",
    "        SCALE = 1000 \n",
    "\n",
    "        year_minus_str = str(year) + '-01-01'\n",
    "        year_plus_str = str(year) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection(\"MODIS/006/MCD19A2_GRANULES\")\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['Optical_Depth_047', 'Optical_Depth_055']\n",
    "    \n",
    "    # Prep drought ---------------------------------------------------\n",
    "    if satellite == 'ecmwf_weather':\n",
    "        \n",
    "        SCALE = 1000 \n",
    "\n",
    "        year_minus_str = str(year) + '-01-01'\n",
    "        year_plus_str = str(year) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection(\"ECMWF/ERA5/DAILY\")\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .mean()\n",
    "        \n",
    "        BANDS = ['mean_2m_air_temperature', \n",
    "                 'minimum_2m_air_temperature', \n",
    "                 'maximum_2m_air_temperature',\n",
    "                 'total_precipitation']\n",
    "        \n",
    "    # Prep drought ---------------------------------------------------\n",
    "    if satellite == 'ecmwf_weather_q1':\n",
    "        \n",
    "        SCALE = 1000 \n",
    "\n",
    "        year_minus_str = str(year) + '-01-01'\n",
    "        year_plus_str = str(year) + '-03-31'\n",
    "        \n",
    "        image = ee.ImageCollection(\"ECMWF/ERA5/DAILY\")\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .mean()\n",
    "        \n",
    "        BANDS = ['mean_2m_air_temperature', \n",
    "                 'minimum_2m_air_temperature', \n",
    "                 'maximum_2m_air_temperature',\n",
    "                 'total_precipitation']\n",
    "        \n",
    "    # Prep drought ---------------------------------------------------\n",
    "    if satellite == 'ecmwf_weather_q2':\n",
    "        \n",
    "        SCALE = 1000 \n",
    "\n",
    "        year_minus_str = str(year) + '-04-01'\n",
    "        year_plus_str = str(year) + '-06-30'\n",
    "        \n",
    "        image = ee.ImageCollection(\"ECMWF/ERA5/DAILY\")\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .mean()\n",
    "        \n",
    "        BANDS = ['mean_2m_air_temperature', \n",
    "                 'minimum_2m_air_temperature', \n",
    "                 'maximum_2m_air_temperature',\n",
    "                 'total_precipitation']\n",
    "        \n",
    "    # Prep drought ---------------------------------------------------\n",
    "    if satellite == 'ecmwf_weather_q3':\n",
    "        \n",
    "        SCALE = 1000 \n",
    "\n",
    "        year_minus_str = str(year) + '-07-01'\n",
    "        year_plus_str = str(year) + '-09-30'\n",
    "        \n",
    "        image = ee.ImageCollection(\"ECMWF/ERA5/DAILY\")\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .mean()\n",
    "        \n",
    "        BANDS = ['mean_2m_air_temperature', \n",
    "                 'minimum_2m_air_temperature', \n",
    "                 'maximum_2m_air_temperature',\n",
    "                 'total_precipitation']\n",
    "        \n",
    "    # Prep drought ---------------------------------------------------\n",
    "    if satellite == 'ecmwf_weather_q4':\n",
    "        \n",
    "        SCALE = 1000 \n",
    "\n",
    "        year_minus_str = str(year) + '-10-01'\n",
    "        year_plus_str = str(year) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection(\"ECMWF/ERA5/DAILY\")\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .mean()\n",
    "        \n",
    "        BANDS = ['mean_2m_air_temperature', \n",
    "                 'minimum_2m_air_temperature', \n",
    "                 'maximum_2m_air_temperature',\n",
    "                 'total_precipitation']\n",
    "    \n",
    "    # Prep viirs ---------------------------------------------------\n",
    "    if (satellite == 'viirs') | (satellite == 'viirs_sdspace'):\n",
    "        \n",
    "        SCALE = 500 \n",
    "        \n",
    "        # Year\n",
    "        # VIIRS starts in April 2012; if year is less than\n",
    "        # 2013, use 2013 as year (to ensure have year before and after)\n",
    "        if year < 2013:\n",
    "            year_use = 2013\n",
    "        else:\n",
    "            year_use = year\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('NOAA/VIIRS/DNB/MONTHLY_V1/VCMCFG')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['avg_rad']\n",
    "        \n",
    "    # Prep viirs ---------------------------------------------------\n",
    "    # https://gis.stackexchange.com/questions/344626/gee-pixel-based-sd-over-time-series-sentinel-2-ndvi\n",
    "    if satellite == 'viirs_sdtime':\n",
    "        \n",
    "        SCALE = 500 \n",
    "        \n",
    "        # Year\n",
    "        # VIIRS starts in April 2012; if year is less than\n",
    "        # 2013, use 2013 as year (to ensure have year before and after)\n",
    "        if year < 2013:\n",
    "            year_use = 2013\n",
    "        else:\n",
    "            year_use = year\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('NOAA/VIIRS/DNB/MONTHLY_V1/VCMCFG')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .reduce(ee.Reducer.stdDev())\n",
    "        \n",
    "        BANDS = ['avg_rad_stdDev']\n",
    "        \n",
    "    # Prep viirs181920 ---------------------------------------------------\n",
    "    if satellite == 'viirs181920':\n",
    "        \n",
    "        SCALE = 500 \n",
    "        \n",
    "        # Year\n",
    "        # VIIRS starts in April 2012; if year is less than\n",
    "        # 2013, use 2013 as year (to ensure have year before and after)\n",
    "        year_use = 2019\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('NOAA/VIIRS/DNB/MONTHLY_V1/VCMCFG')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['avg_rad']\n",
    "        \n",
    "    # Prep DMSP ---------------------------------------------------\n",
    "    if satellite == 'dmsp':\n",
    "        \n",
    "        SCALE = 1000 \n",
    "        \n",
    "        # Year\n",
    "        # DMSP-OLS starts in 2013; if year is more than\n",
    "        # 2012, use 2012 as year (to ensure have year before and after)\n",
    "        if year > 2012:\n",
    "            year_use = 2012\n",
    "        else:\n",
    "            year_use = year\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('NOAA/DMSP-OLS/NIGHTTIME_LIGHTS')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['stable_lights', 'avg_lights_x_pct']\n",
    "    \n",
    "    # Prep Survey ---------------------------------------------------\n",
    "    survey_fc = survey_to_fc_buffer(survey_df, buffer_size_urban, buffer_size_rural)\n",
    "        \n",
    "    # Extract Values ---------------------------------------------------\n",
    "    if (satellite == 'worldpop') | (satellite == 'worldpop2020'):\n",
    "        vals = image.reduceRegions(collection = survey_fc,\n",
    "                                   reducer = ee.Reducer.sum(),\n",
    "                                   scale = SCALE,\n",
    "                                   tileScale = 8)\n",
    "    elif (satellite == 'viirs_sdspace') | (satellite == 'l7_sdspace') | (satellite == 'l8_sdspace'):\n",
    "        vals = image.reduceRegions(collection = survey_fc,\n",
    "                           reducer = ee.Reducer.stdDev(),\n",
    "                           scale = SCALE,\n",
    "                           tileScale = 8)        \n",
    "    else:\n",
    "        vals = image.reduceRegions(collection = survey_fc,\n",
    "                                   reducer = ee.Reducer.mean(),\n",
    "                                   scale = SCALE,\n",
    "                                   tileScale = 8)\n",
    "\n",
    "    # OLD =============\n",
    "    # Survey dataset that only contains the uid variable\n",
    "    #survey_df = survey_df[['uid']]\n",
    "            \n",
    "    #for band_i in BANDS:\n",
    "    #    survey_df[satellite + '_' + band_i] = vals.aggregate_array(band_i).getInfo()\n",
    "        \n",
    "    # NEW =============\n",
    "    #df_out = fc2df(vals)\n",
    "    #print(df_out)\n",
    "    #df_out = pd.DataFrame()\n",
    "    \n",
    "    bands_to_export = BANDS.copy()\n",
    "    bands_to_export.append('uid')\n",
    "    bands_to_export.append('year')\n",
    "    #print(bands_to_export)\n",
    "    \n",
    "    task = ee.batch.Export.table.toDrive(collection=vals, \n",
    "                                         folder='satellite_data_from_gee_' + survey_name.lower(), \n",
    "                                         description=file_name, \n",
    "                                         fileFormat='CSV',\n",
    "                                         selectors = bands_to_export)\n",
    "    # selectors=props\n",
    "    task.start()\n",
    "    #ee.batch.data.startProcessing(mytask.id, mytask.config)\n",
    "    \n",
    "    if False:\n",
    "        time_elapsed = 0\n",
    "        while task.active():\n",
    "            if((time_elapsed % 60) == 0):\n",
    "                print('Polling for task (id: {}).'.format(task.id))\n",
    "            time.sleep(5)\n",
    "            time_elapsed = time_elapsed + 5\n",
    "        \n",
    "    return task\n",
    "\n",
    "def extract_satellite_in_chunks(survey_df, buffer_size_urban, buffer_size_rural, satellite, file_name, year, survey_name):\n",
    "    \n",
    "    vals_df_list = []\n",
    "    \n",
    "    for chunk_i in list(np.unique(survey_df.chunk_id)):\n",
    "\n",
    "        survey_df_i = survey_df[survey_df['chunk_id'] == chunk_i]\n",
    "        vals_i_df = extract_sat(survey_df_i, buffer_size_urban, buffer_size_rural, satellite, file_name, year, chunk_i, survey_name)\n",
    "\n",
    "        vals_df_list.append(vals_i_df)\n",
    "    \n",
    "    return vals_df_list\n",
    "\n",
    "def extract_satellite_by_year(survey_df, buffer_size_urban, buffer_size_rural, satellite, file_name, survey_name):\n",
    "    \n",
    "    vals_df_list = []\n",
    "    \n",
    "    for year_i in list(np.unique(survey_df.year)):\n",
    "\n",
    "        survey_df_i = survey_df[survey_df['year'] == year_i]\n",
    "        vals_i_df = extract_satellite_in_chunks(survey_df_i, buffer_size_urban, buffer_size_rural, satellite, file_name, year_i, survey_name)\n",
    "\n",
    "        vals_df_list.append(vals_i_df)\n",
    "    \n",
    "    return vals_df_list\n",
    "\n",
    "def chunk_ids(total_length, chunk_size):\n",
    "    n_numbers = np.ceil(total_length / chunk_size)\n",
    "    n_numbers = int(n_numbers)\n",
    "    \n",
    "    chunk_ids = list(range(0,n_numbers)) * chunk_size\n",
    "    chunk_ids.sort()\n",
    "    chunk_ids = chunk_ids[:total_length]\n",
    "    \n",
    "    return chunk_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load/Prep Survey Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df = pd.read_csv(os.path.join(cf.DROPBOX_DIRECTORY, 'Data', SURVEY_NAME, 'FinalData', 'Individual Datasets', 'survey_socioeconomic.csv'))\n",
    "survey_df = survey_df[['uid', 'year', 'latitude', 'longitude']] # urban_rural\n",
    "survey_df = survey_df.sort_values('year')\n",
    "#survey_df = survey_df.head(1000)\n",
    "#survey_df = survey_df[survey_df.uid != 'IA201400180012']\n",
    "\n",
    "survey_years = list(survey_df.year.unique())\n",
    "\n",
    "#CHUNK_SIZE = 1000\n",
    "#survey_df['chunk_id'] = chunk_ids(survey_df.shape[0], CHUNK_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  uid  year   latitude  longitude\n",
      "67364  PE200000000264  2000 -14.865667 -74.547850\n",
      "65669  NM200000000013  2000 -17.718744  24.487464\n",
      "65670  NM200000000014  2000 -17.939638  24.638673\n",
      "65671  NM200000000015  2000 -17.858389  24.790742\n",
      "65672  NM200000000016  2000 -17.600396  25.005822\n",
      "(82424, 4)\n"
     ]
    }
   ],
   "source": [
    "print(survey_df.head())\n",
    "print(survey_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If re-extract, delete existing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REEXTRACT_IF_FILE_EXISTS:\n",
    "    print(\"Deleting existing files from Google Drive\")\n",
    "\n",
    "    ## Path with files\n",
    "    OUT_PATH = os.path.join(cf.GOOGLEDRIVE_DIRECTORY, \n",
    "                            'Data', \n",
    "                             SURVEY_NAME, \n",
    "                             'FinalData', \n",
    "                             'Individual Datasets',\n",
    "                             'satellite_data_from_gee_' + SURVEY_NAME.lower())\n",
    "\n",
    "    ## Grab csv files\n",
    "    files_to_rm = [x for x in os.listdir(OUT_PATH) if '.csv' in x]\n",
    "\n",
    "    ## Delete files\n",
    "    for file_i in files_to_rm:\n",
    "\n",
    "        path_i = os.path.join(OUT_PATH, file_i)\n",
    "        os.remove(path_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of files already extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3913"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Path with files\n",
    "OUT_PATH = os.path.join(cf.GOOGLEDRIVE_DIRECTORY, \n",
    "                        'Data', \n",
    "                         SURVEY_NAME, \n",
    "                         'FinalData', \n",
    "                         'Individual Datasets',\n",
    "                         'satellite_data_from_gee_' + SURVEY_NAME.lower())\n",
    "\n",
    "## Grab csv files\n",
    "files_extracted = [x for x in os.listdir(OUT_PATH) if '.csv' in x]\n",
    "\n",
    "len(files_extracted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l7_sdspace\n",
      "viirs_sdtime_2500\n",
      "viirs_sdspace_2500\n"
     ]
    }
   ],
   "source": [
    "to_extract = ['elevation', \n",
    "              'slope',\n",
    "              'viirs_750',\n",
    "              'viirs_1120',\n",
    "              'viirs_1250',\n",
    "              'viirs_1500',\n",
    "              'viirs_2000',\n",
    "              'viirs_2500',\n",
    "              'viirs_5000',\n",
    "              'viirs181920_750',\n",
    "              'viirs181920_1120',\n",
    "              'viirs181920_1250',\n",
    "              'viirs181920_1500',\n",
    "              'viirs181920_2000',\n",
    "              'viirs181920_2500',\n",
    "              'viirs181920_5000',\n",
    "              'viirs_sdtime_2500',\n",
    "              'viirs_sdspace_2500',\n",
    "              'GlobalHumanModification',\n",
    "              'worldpop_750',\n",
    "              'worldpop_1500',\n",
    "              'worldpop_2000',\n",
    "              'worldpop_2500',\n",
    "              'worldpop_5000',\n",
    "              'worldpop_10000',\n",
    "              'worldpop2020_750',\n",
    "              'worldpop2020_1500',\n",
    "              'worldpop2020_2000',\n",
    "              'worldpop2020_2500',\n",
    "              'worldpop2020_5000',\n",
    "              'worldpop2020_10000',\n",
    "              'l8',\n",
    "              'l7',\n",
    "              'l8_sdtime',\n",
    "              'l7_sdtime',\n",
    "              'l8_sdspace',\n",
    "              'l7_sdspace',\n",
    "              'aod',\n",
    "              'ecmwf_weather',\n",
    "              'ecmwf_weather_q1',\n",
    "              'ecmwf_weather_q2',\n",
    "              'ecmwf_weather_q3',\n",
    "              'ecmwf_weather_q4',\n",
    "              'NO2', 'uv_aer', 'CO', 'HCHO', 'ozone', 'SO2', 'CH4',]\n",
    "\n",
    "tasks_all = []\n",
    "\n",
    "# Loop over satellites ------------------------------\n",
    "for name in to_extract:\n",
    "    print(name)\n",
    "        \n",
    "    sat = name\n",
    "    \n",
    "    if name in ['NO2', 'uv_aer', 'CO', 'HCHO', 'ozone', 'SO2', 'CH4', 'l8', 'l7', 'l5', 'l7_sdtime', 'l8_sdtime', 'l7_sdspace', 'l8_sdspace', 'aod', 'GlobalHumanModification', 'elevation', 'slope']:\n",
    "        if SURVEY_NAME == \"DHS\":\n",
    "            buffer_u = 2500\n",
    "            buffer_r = 2500\n",
    "            \n",
    "        if SURVEY_NAME == \"PAK_POINTS\":\n",
    "            buffer_u = 1500\n",
    "            buffer_r = 1500\n",
    "            \n",
    "        if SURVEY_NAME == \"PAK_CITY_POINTS\":\n",
    "            buffer_u = 750\n",
    "            buffer_r = 750\n",
    "    \n",
    "    #if name in ['elevation', 'slope']:\n",
    "    #    if SURVEY_NAME == \"DHS\":\n",
    "    #        buffer_u = 5000\n",
    "    #        buffer_r = 5000\n",
    "    #        \n",
    "    #    if SURVEY_NAME == \"PAK_POINTS\":\n",
    "    #        buffer_u = 1500\n",
    "    #        buffer_r = 1500\n",
    "                \n",
    "    if name in ['ecmwf_weather',\n",
    "                'ecmwf_weather_q1', 'ecmwf_weather_q2', 'ecmwf_weather_q3', 'ecmwf_weather_q4']:\n",
    "        # 27km radius\n",
    "        buffer_u = 10000\n",
    "        buffer_r = 10000\n",
    "        \n",
    "    if name == 'viirs181920_750':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 750\n",
    "        buffer_r = 750\n",
    "        \n",
    "    if name == 'viirs181920_1120':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 1120\n",
    "        buffer_r = 1120\n",
    "                \n",
    "    if name == 'viirs181920_1250':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 1250\n",
    "        buffer_r = 1250\n",
    "        \n",
    "    if name == 'viirs181920_1500':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 1500\n",
    "        buffer_r = 1500\n",
    "        \n",
    "    if name == 'viirs181920_2000':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 2000\n",
    "        buffer_r = 2000\n",
    "        \n",
    "    if name == 'viirs181920_2500':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 2500\n",
    "        buffer_r = 2500\n",
    "        \n",
    "    if name == 'viirs181920_5000':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 5000\n",
    "        buffer_r = 5000\n",
    "\n",
    "    if name == 'viirs_750':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 750\n",
    "        buffer_r = 750\n",
    "            \n",
    "    if name == 'viirs_1120':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 1120\n",
    "        buffer_r = 1120\n",
    "            \n",
    "    if name == 'viirs_1250':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 1250\n",
    "        buffer_r = 1250\n",
    "        \n",
    "    if name == 'viirs_1500':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 1500\n",
    "        buffer_r = 1500\n",
    "            \n",
    "    if name == 'viirs_2000':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 2000\n",
    "        buffer_r = 2000\n",
    "        \n",
    "    if name == 'viirs_2500':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 2500\n",
    "        buffer_r = 2500\n",
    "        \n",
    "    if name == 'viirs_5000':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 5000\n",
    "        buffer_r = 5000\n",
    "        \n",
    "    if name == 'viirs_sdtime_2500':\n",
    "        sat = 'viirs_sdtime'\n",
    "        buffer_u = 2500\n",
    "        buffer_r = 2500\n",
    "        \n",
    "    if name == 'viirs_sdspace_2500':\n",
    "        sat = 'viirs_sdspace'\n",
    "        buffer_u = 2500\n",
    "        buffer_r = 2500\n",
    "        \n",
    "    if name == 'worldpop_750':\n",
    "        sat = 'worldpop'\n",
    "        buffer_u = 750\n",
    "        buffer_r = 750\n",
    "        \n",
    "    if name == 'worldpop_1500':\n",
    "        sat = 'worldpop'\n",
    "        buffer_u = 1500\n",
    "        buffer_r = 1500\n",
    "        \n",
    "    if name == 'worldpop_2000':\n",
    "        sat = 'worldpop'\n",
    "        buffer_u = 2000\n",
    "        buffer_r = 2000\n",
    "        \n",
    "    if name == 'worldpop_2500':\n",
    "        sat = 'worldpop'\n",
    "        buffer_u = 2500\n",
    "        buffer_r = 2500\n",
    "        \n",
    "    if name == 'worldpop_5000':\n",
    "        sat = 'worldpop'\n",
    "        buffer_u = 5000\n",
    "        buffer_r = 5000\n",
    "        \n",
    "    if name == 'worldpop_10000':\n",
    "        sat = 'worldpop'\n",
    "        buffer_u = 10000\n",
    "        buffer_r = 10000\n",
    "       \n",
    "    if name == 'worldpop2020_750':\n",
    "        sat = 'worldpop2020'\n",
    "        buffer_u = 750\n",
    "        buffer_r = 750\n",
    "    \n",
    "    if name == 'worldpop2020_1500':\n",
    "        sat = 'worldpop2020'\n",
    "        buffer_u = 1500\n",
    "        buffer_r = 1500\n",
    "        \n",
    "    if name == 'worldpop2020_2000':\n",
    "        sat = 'worldpop2020'\n",
    "        buffer_u = 2000\n",
    "        buffer_r = 2000\n",
    "        \n",
    "    if name == 'worldpop2020_2500':\n",
    "        sat = 'worldpop2020'\n",
    "        buffer_u = 2500\n",
    "        buffer_r = 2500\n",
    "        \n",
    "    if name == 'worldpop2020_5000':\n",
    "        sat = 'worldpop2020'\n",
    "        buffer_u = 5000\n",
    "        buffer_r = 5000\n",
    "        \n",
    "    if name == 'worldpop2020_10000':\n",
    "        sat = 'worldpop2020'\n",
    "        buffer_u = 10000\n",
    "        buffer_r = 10000\n",
    "        \n",
    "    # Define Chunk Size ---------------------------------\n",
    "    CHUNK_SIZE = 5000\n",
    "    \n",
    "    if sat in ['NO2', 'uv_aer', 'CO', 'HCHO', 'ozone', 'SO2', 'CH4']:\n",
    "        CHUNK_SIZE = 500\n",
    "        \n",
    "    survey_df['chunk_id'] = chunk_ids(survey_df.shape[0], CHUNK_SIZE)\n",
    "                        \n",
    "    # Loop over satellites ------------------------------\n",
    "    \n",
    "    # LOOP OVER YEARS AND CHUNKS TO EXTRACT DATA \n",
    "    for year_i in survey_df['year'].unique():\n",
    "        \n",
    "        survey_df_year = survey_df[survey_df['year'] == year_i]\n",
    "\n",
    "        # LOOP OVER CHUNKS\n",
    "        for chunk_id_i in survey_df_year['chunk_id'].unique():\n",
    "            survey_df_year_i = survey_df_year[survey_df_year['chunk_id'] == chunk_id_i]\n",
    "\n",
    "            file_name_i = 'gee_' + name + '_ubuff' + str(buffer_u) + '_rbuff' + str(buffer_r) + '_' + str(year_i) + '_' + str(chunk_id_i)    \n",
    "            file_name_i_csv = file_name_i + '.csv'\n",
    "\n",
    "            # ONLY EXTRACT DATA IF NOT ALREADY EXTRACTED\n",
    "            if file_name_i_csv not in files_extracted:\n",
    "                                \n",
    "                task_i = extract_sat(survey_df = survey_df_year_i, \n",
    "                                     buffer_size_urban = buffer_u, \n",
    "                                     buffer_size_rural = buffer_r, \n",
    "                                     year = year_i,\n",
    "                                     satellite = sat, \n",
    "                                     survey_name = SURVEY_NAME,\n",
    "                                     file_name = file_name_i)\n",
    "\n",
    "                tasks_all.append(task_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_all[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Failed Tasks\n",
    "for task_i in tasks_all:\n",
    "            \n",
    "    task_i_status = task_i.status()\n",
    "    if task_i_status['state'] == 'FAILED':\n",
    "        print(task_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ready Tasks\n",
    "for task_i in tasks_all:\n",
    "            \n",
    "    task_i_status = task_i.status()\n",
    "    if task_i_status['state'] == 'READY':\n",
    "        print(task_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Task BHS4NXHDJOGTPZ75IMM5XR26 EXPORT_FEATURES: gee_elevation_ubuff2500_rbuff2500_2002_0 (UNSUBMITTED)>\n",
      "<Task HCEV5DKDEJ3X3LC7YYW42AX7 EXPORT_FEATURES: gee_elevation_ubuff2500_rbuff2500_2003_0 (UNSUBMITTED)>\n",
      "<Task IFTGCSK7GTCW5HQPQC66U57Z EXPORT_FEATURES: gee_elevation_ubuff2500_rbuff2500_2005_0 (UNSUBMITTED)>\n",
      "<Task BOEPWFFXWKLOM7O357OV34YT EXPORT_FEATURES: gee_elevation_ubuff2500_rbuff2500_2006_0 (UNSUBMITTED)>\n",
      "<Task 4SCOVMOX7OSZCCXDJD5PNMB3 EXPORT_FEATURES: gee_elevation_ubuff2500_rbuff2500_2008_0 (UNSUBMITTED)>\n",
      "<Task DLYQMHA6RVVQM2J24YV4BFHJ EXPORT_FEATURES: gee_elevation_ubuff2500_rbuff2500_2009_0 (UNSUBMITTED)>\n",
      "<Task SLXNKMFCIG2CMGF2L3UQPGFH EXPORT_FEATURES: gee_elevation_ubuff2500_rbuff2500_2010_0 (UNSUBMITTED)>\n",
      "<Task WVIWMQVWN2LUSAFRYUDVGQOD EXPORT_FEATURES: gee_elevation_ubuff2500_rbuff2500_2010_1 (UNSUBMITTED)>\n",
      "<Task RT6G5HVDJMQEMSAZFUXKNDIM EXPORT_FEATURES: gee_elevation_ubuff2500_rbuff2500_2011_1 (UNSUBMITTED)>\n",
      "<Task XWMYFEJ654P23GQTM5SKC75M EXPORT_FEATURES: gee_elevation_ubuff2500_rbuff2500_2011_2 (UNSUBMITTED)>\n",
      "<Task NXEAOQXVIRDZRPHA4FQD6HDV EXPORT_FEATURES: gee_elevation_ubuff2500_rbuff2500_2012_2 (UNSUBMITTED)>\n",
      "<Task W5KQFYKB2RRDYEI7HTD3X3WC EXPORT_FEATURES: gee_elevation_ubuff2500_rbuff2500_2013_2 (UNSUBMITTED)>\n",
      "<Task CM4P5LWT3MSMCDDGH67RH2ND EXPORT_FEATURES: gee_elevation_ubuff2500_rbuff2500_2014_2 (UNSUBMITTED)>\n",
      "<Task HNCTHAT6GKMOBKBGYOPXUS77 EXPORT_FEATURES: gee_elevation_ubuff2500_rbuff2500_2014_3 (UNSUBMITTED)>\n",
      "<Task CAV6Q7LOOIALGGI5ON6P66ZC EXPORT_FEATURES: gee_elevation_ubuff2500_rbuff2500_2015_3 (UNSUBMITTED)>\n",
      "<Task LVHPQTAS3VM3GRWKAOGEUEFN EXPORT_FEATURES: gee_elevation_ubuff2500_rbuff2500_2015_4 (UNSUBMITTED)>\n",
      "<Task OU7LI5WMR7KALGMJUARZEBMY EXPORT_FEATURES: gee_elevation_ubuff2500_rbuff2500_2015_5 (UNSUBMITTED)>\n",
      "<Task GMDKSQD2L7QKLAUJUOVTC64H EXPORT_FEATURES: gee_elevation_ubuff2500_rbuff2500_2015_6 (UNSUBMITTED)>\n",
      "<Task AXRWIAS7Q2DEXRQ7JH5KS5UQ EXPORT_FEATURES: gee_elevation_ubuff2500_rbuff2500_2015_7 (UNSUBMITTED)>\n",
      "<Task QXM7C37P56NEHA46CHMTLAEJ EXPORT_FEATURES: gee_elevation_ubuff2500_rbuff2500_2015_8 (UNSUBMITTED)>\n",
      "<Task VWVLHXACVVX2RNCQ6CCUREWN EXPORT_FEATURES: gee_elevation_ubuff2500_rbuff2500_2015_9 (UNSUBMITTED)>\n",
      "<Task LNNPKQZLICKHXJ3XKQW7NUQC EXPORT_FEATURES: gee_elevation_ubuff2500_rbuff2500_2016_9 (UNSUBMITTED)>\n",
      "<Task YRCQFPOJJQMBVVGJHDPSU6X3 EXPORT_FEATURES: gee_elevation_ubuff2500_rbuff2500_2016_10 (UNSUBMITTED)>\n",
      "<Task VPOWUGOO43I7LHONPENAVKAS EXPORT_FEATURES: gee_elevation_ubuff2500_rbuff2500_2017_10 (UNSUBMITTED)>\n",
      "<Task TQZ3YLWCTVAUOFWB4KZCXGK7 EXPORT_FEATURES: gee_elevation_ubuff2500_rbuff2500_2017_11 (UNSUBMITTED)>\n",
      "<Task YPUZITYERNQ2ARYPIVMBMY6W EXPORT_FEATURES: gee_elevation_ubuff2500_rbuff2500_2018_11 (UNSUBMITTED)>\n",
      "<Task J5YHUTWI2SVSXST7ERYXXXM5 EXPORT_FEATURES: gee_elevation_ubuff2500_rbuff2500_2018_12 (UNSUBMITTED)>\n",
      "<Task E4KJYS4VTGQ7WETIPFY534SG EXPORT_FEATURES: gee_elevation_ubuff2500_rbuff2500_2019_12 (UNSUBMITTED)>\n",
      "<Task 6RWOZCTEMXI7VM7GKH37Z4AI EXPORT_FEATURES: gee_slope_ubuff2500_rbuff2500_2002_0 (UNSUBMITTED)>\n",
      "<Task GKIZJ2B3TY3RJVHFCP35AE3Y EXPORT_FEATURES: gee_slope_ubuff2500_rbuff2500_2003_0 (UNSUBMITTED)>\n",
      "<Task 2P5WKC247WQ7PKUL7HDNAM5P EXPORT_FEATURES: gee_slope_ubuff2500_rbuff2500_2005_0 (UNSUBMITTED)>\n",
      "<Task JSZ4RXPQRYFHOVELO6MLPJHE EXPORT_FEATURES: gee_slope_ubuff2500_rbuff2500_2006_0 (UNSUBMITTED)>\n",
      "<Task 67O4NWFLYMREFUXLO2PFPSEL EXPORT_FEATURES: gee_slope_ubuff2500_rbuff2500_2008_0 (UNSUBMITTED)>\n",
      "<Task FPMS2NK5PSMJRT4XJ7UCKISL EXPORT_FEATURES: gee_slope_ubuff2500_rbuff2500_2009_0 (UNSUBMITTED)>\n",
      "<Task FISPNIX3XJKYJ3TU4ODF5X67 EXPORT_FEATURES: gee_slope_ubuff2500_rbuff2500_2010_0 (UNSUBMITTED)>\n",
      "<Task VEKYYZI4DBR4BEVUVVVTZ2P7 EXPORT_FEATURES: gee_slope_ubuff2500_rbuff2500_2010_1 (UNSUBMITTED)>\n",
      "<Task ZIC6OQYJMSDD7ZLOUZJ4ONME EXPORT_FEATURES: gee_slope_ubuff2500_rbuff2500_2011_1 (UNSUBMITTED)>\n",
      "<Task 5QDLABX377LHGUKWRZMCQILH EXPORT_FEATURES: gee_slope_ubuff2500_rbuff2500_2011_2 (UNSUBMITTED)>\n",
      "<Task YULUB7OTYRPDFB2H4OSSRY66 EXPORT_FEATURES: gee_slope_ubuff2500_rbuff2500_2012_2 (UNSUBMITTED)>\n",
      "<Task FYMO3QHXCM3YIJAAVEK3BKJG EXPORT_FEATURES: gee_slope_ubuff2500_rbuff2500_2013_2 (UNSUBMITTED)>\n",
      "<Task YGUV2FQYJYK4XJZ34RA4X7XL EXPORT_FEATURES: gee_slope_ubuff2500_rbuff2500_2014_2 (UNSUBMITTED)>\n",
      "<Task 4IT2RFPY4Q3Q65PFVZJH4LRR EXPORT_FEATURES: gee_slope_ubuff2500_rbuff2500_2014_3 (UNSUBMITTED)>\n",
      "<Task Y7OOB2ZCN5I3TWELRMUJBU4O EXPORT_FEATURES: gee_slope_ubuff2500_rbuff2500_2015_3 (UNSUBMITTED)>\n",
      "<Task OFREPDWHH3OWOLZSUT7Z22CF EXPORT_FEATURES: gee_slope_ubuff2500_rbuff2500_2015_4 (UNSUBMITTED)>\n",
      "<Task UCWBG3U7AQ6UCJJUOPFKH2KZ EXPORT_FEATURES: gee_slope_ubuff2500_rbuff2500_2015_5 (UNSUBMITTED)>\n",
      "<Task RVFVQ3PEDTO2QIUCX64RCCIS EXPORT_FEATURES: gee_slope_ubuff2500_rbuff2500_2015_6 (UNSUBMITTED)>\n",
      "<Task GZLSY3CU3B3DVYSJNCM677V3 EXPORT_FEATURES: gee_slope_ubuff2500_rbuff2500_2015_7 (UNSUBMITTED)>\n",
      "<Task YMRF3MUI5OFT3PJ6X6Y63SFS EXPORT_FEATURES: gee_slope_ubuff2500_rbuff2500_2015_8 (UNSUBMITTED)>\n",
      "<Task A7IK6K5PH3EM4GLPLKDBBLLC EXPORT_FEATURES: gee_slope_ubuff2500_rbuff2500_2015_9 (UNSUBMITTED)>\n",
      "<Task 7ADTVJZSNXRYDLGKPJMSO46T EXPORT_FEATURES: gee_slope_ubuff2500_rbuff2500_2016_9 (UNSUBMITTED)>\n",
      "<Task 7L6AV3XPF6BPRTWXSJMEC4WQ EXPORT_FEATURES: gee_slope_ubuff2500_rbuff2500_2016_10 (UNSUBMITTED)>\n",
      "<Task WIOLYVQGBLNUKETTDHU4L5RW EXPORT_FEATURES: gee_slope_ubuff2500_rbuff2500_2017_10 (UNSUBMITTED)>\n",
      "<Task 7BJ4UYLCFFEN3FRDMKTPHXOI EXPORT_FEATURES: gee_slope_ubuff2500_rbuff2500_2017_11 (UNSUBMITTED)>\n",
      "<Task 674RZNAENNOL4HAJAYM6UHVT EXPORT_FEATURES: gee_slope_ubuff2500_rbuff2500_2018_11 (UNSUBMITTED)>\n",
      "<Task SZ3P6JDJWB2OJP6VN7O4AW5W EXPORT_FEATURES: gee_slope_ubuff2500_rbuff2500_2018_12 (UNSUBMITTED)>\n",
      "<Task Z3R6SMWSYZ3A2GR6SSGV2E75 EXPORT_FEATURES: gee_slope_ubuff2500_rbuff2500_2019_12 (UNSUBMITTED)>\n"
     ]
    }
   ],
   "source": [
    "## Completed Tasks\n",
    "for task_i in tasks_all:\n",
    "            \n",
    "    task_i_status = task_i.status()\n",
    "    if task_i_status['state'] == 'COMPLETED':\n",
    "        print(task_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING\n",
      "RUNNING\n",
      "READY\n"
     ]
    }
   ],
   "source": [
    "## View State\n",
    "for task_i in tasks_all:            \n",
    "    print(task_i.status()['state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Task BIDUIEZSHCTV3KGKRWGY26WS EXPORT_FEATURES: gee_ecmwf_weather_q4_ubuff10000_rbuff10000_2020_0 (UNSUBMITTED)>,\n",
       " <Task NLSCPPZMQ4UAVG2J2YVQX3OC EXPORT_FEATURES: gee_ecmwf_weather_q4_ubuff10000_rbuff10000_2020_1 (UNSUBMITTED)>,\n",
       " <Task 7URYT2DPD4734BIVQGKWHFAA EXPORT_FEATURES: gee_ecmwf_weather_q4_ubuff10000_rbuff10000_2020_2 (UNSUBMITTED)>,\n",
       " <Task 4F5CWNA6BROJVWRNRFOOHOHF EXPORT_FEATURES: gee_ecmwf_weather_q4_ubuff10000_rbuff10000_2020_3 (UNSUBMITTED)>]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cancel Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    for task_i in tasks_all:\n",
    "        task_i.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_extract = ['NO2', 'uv_aer', 'CO', 'HCHO', 'ozone', 'SO2', 'CH4',\n",
    "              'elevation', \n",
    "              'slope',\n",
    "              'viirs_1120',\n",
    "              'viirs_1250',\n",
    "              'viirs_2000',\n",
    "              'viirs_2500',\n",
    "              'viirs_5000',\n",
    "              'viirs181920_1120',\n",
    "              'viirs181920_1250',\n",
    "              'viirs181920_2000',\n",
    "              'viirs181920_2500',\n",
    "              'viirs181920_5000',\n",
    "              'GlobalHumanModification',\n",
    "              'worldpop_2000',\n",
    "              'worldpop_5000',\n",
    "              'worldpop_10000',\n",
    "              'worldpop2020_2000',\n",
    "              'worldpop2020_5000',\n",
    "              'worldpop2020_10000',\n",
    "              'l8',\n",
    "              'aod',\n",
    "              'ecmwf_weather',\n",
    "              'ecmwf_weather_q1',\n",
    "              'ecmwf_weather_q2',\n",
    "              'ecmwf_weather_q3',\n",
    "              'ecmwf_weather_q4']\n",
    "\n",
    "tasks_all = []\n",
    "for name in to_extract:\n",
    "    print(name)\n",
    "        \n",
    "    sat = name\n",
    "        \n",
    "    if name in ['NO2', 'uv_aer', 'CO', 'HCHO', 'ozone', 'SO2', 'CH4', 'l8', 'aod']:\n",
    "        buffer_u = 2500\n",
    "        buffer_r = 2500\n",
    "    \n",
    "    if name in ['elevation', 'slope']:\n",
    "        buffer_u = 5000\n",
    "        buffer_r = 5000\n",
    "        \n",
    "    if name in ['GlobalHumanModification', \n",
    "               'ecmwf_weather',\n",
    "               'ecmwf_weather_q1', 'ecmwf_weather_q2', 'ecmwf_weather_q3', 'ecmwf_weather_q4']:\n",
    "        buffer_u = 10000\n",
    "        buffer_r = 10000\n",
    "        \n",
    "    if name == 'viirs181920_1120':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 1120\n",
    "        buffer_r = 1120\n",
    "                \n",
    "    if name == 'viirs181920_1250':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 1250\n",
    "        buffer_r = 1250\n",
    "        \n",
    "    if name == 'viirs181920_2000':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 2000\n",
    "        buffer_r = 2000\n",
    "        \n",
    "    if name == 'viirs181920_2500':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 2500\n",
    "        buffer_r = 2500\n",
    "        \n",
    "    if name == 'viirs181920_5000':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 5000\n",
    "        buffer_r = 5000\n",
    "            \n",
    "    if name == 'viirs_1120':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 1120\n",
    "        buffer_r = 1120\n",
    "            \n",
    "    if name == 'viirs_1250':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 1250\n",
    "        buffer_r = 1250\n",
    "            \n",
    "    if name == 'viirs_2000':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 2000\n",
    "        buffer_r = 2000\n",
    "        \n",
    "    if name == 'viirs_2500':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 2500\n",
    "        buffer_r = 2500\n",
    "        \n",
    "    if name == 'viirs_5000':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 5000\n",
    "        buffer_r = 5000\n",
    "        \n",
    "    if name == 'worldpop_2000':\n",
    "        sat = 'worldpop'\n",
    "        buffer_u = 2000\n",
    "        buffer_r = 2000\n",
    "        \n",
    "    if name == 'worldpop_5000':\n",
    "        sat = 'worldpop'\n",
    "        buffer_u = 5000\n",
    "        buffer_r = 5000\n",
    "        \n",
    "    if name == 'worldpop_10000':\n",
    "        sat = 'worldpop'\n",
    "        buffer_u = 10000\n",
    "        buffer_r = 10000\n",
    "        \n",
    "    if name == 'worldpop2020_2000':\n",
    "        sat = 'worldpop2020'\n",
    "        buffer_u = 2000\n",
    "        buffer_r = 2000\n",
    "        \n",
    "    if name == 'worldpop2020_5000':\n",
    "        sat = 'worldpop2020'\n",
    "        buffer_u = 5000\n",
    "        buffer_r = 5000\n",
    "        \n",
    "    if name == 'worldpop2020_10000':\n",
    "        sat = 'worldpop2020'\n",
    "        buffer_u = 10000\n",
    "        buffer_r = 10000\n",
    "                \n",
    "    file_root = name + '_ubuff' + str(buffer_u) + '_rbuff' + str(buffer_r)\n",
    "        \n",
    "    # Check if should extract data\n",
    "    if (file_root not in files_already_extracted) | REEXTRACT_IF_FILE_EXISTS:\n",
    "        print(file_root)\n",
    "        \n",
    "        tasks_i = extract_satellite_by_year(survey_df, buffer_u, buffer_r, sat, name, SURVEY_NAME)\n",
    "        tasks_all.append(tasks_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
