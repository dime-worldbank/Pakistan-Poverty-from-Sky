{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Satellite Imagery to Survey Locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "__IMPORTANT:__ \n",
    "1. Set `DROPBOX_DIRECTORY` and `GOOGLEDRIVE_DIRECTORY` to point to the `Big Data Poverty Estimation` folders on yout computer.\n",
    "2. The script needs to be separately run when `SURVEY_NAME` equals `DHS`, `DHS_nga_policy_experiment`, and `LSMS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROPBOX_DIRECTORY     = '/Users/robmarty/Dropbox/World Bank/IEs/Big Data Poverty Estimation'\n",
    "GOOGLEDRIVE_DIRECTORY = '/Users/robmarty/Library/CloudStorage/GoogleDrive-robmarty3@gmail.com/My Drive/Big Data Poverty Estimation'\n",
    "\n",
    "SURVEY_NAME = 'DHS'\n",
    "\n",
    "# If file exists, whether to reextract or skip\n",
    "REEXTRACT_IF_FILE_EXISTS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "# ee.Authenticate()\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import geetools\n",
    "from geetools import ui, cloud_mask\n",
    "import os, datetime\n",
    "import glob√•\n",
    "import config as cf\n",
    "import pandas as pd\n",
    "import time\n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "\n",
    "cloud_mask_landsatSR = cloud_mask.landsatSR()\n",
    "cloud_mask_sentinel2 = cloud_mask.sentinel2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gis.stackexchange.com/questions/257727/iterate-over-imagecollection-returning-pandas-dataframe-using-earth-engine-pyt\n",
    "def fc2df(fc):\n",
    "    # Convert a FeatureCollection into a pandas DataFrame\n",
    "    # Features is a list of dict with the output\n",
    "    features = fc.getInfo()['features']\n",
    "\n",
    "    dictarr = []\n",
    "\n",
    "    for f in features:\n",
    "        # Store all attributes in a dict\n",
    "        attr = f['properties']\n",
    "        # and treat geometry separately\n",
    "        attr['geometry'] = f['geometry']  # GeoJSON Feature!\n",
    "        # attr['geometrytype'] = f['geometry']['type']\n",
    "        dictarr.append(attr)\n",
    "\n",
    "    df = gpd.GeoDataFrame(dictarr)\n",
    "    # Convert GeoJSON features to shape\n",
    "    df = df.drop(columns=['geometry'])\n",
    "    return df\n",
    "\n",
    "def survey_to_fc_buffer(survey_df, buffer_size_urban, buffer_size_rural):\n",
    "    '''\n",
    "    Convert pandas dataframe of survey locations to a feature collection. \n",
    "    \n",
    "    Inputs:\n",
    "        survey_df: pandas dataframe of survey locations. Function assumes \n",
    "                   the dataframe contains (1) latitude, (2) longitude and\n",
    "                   (3) uid variables. Assumes coordinates in WGS84.\n",
    "    Returns:\n",
    "        (feature collection)\n",
    "    '''\n",
    "    \n",
    "    POLLUTION_SCALE = 10000\n",
    "    \n",
    "    survey_fc_list = []\n",
    "    \n",
    "    n_rows = survey_df.shape[0]\n",
    "    for i in range(0, n_rows):\n",
    "        survey_df_i = survey_df.iloc[[i]]\n",
    "        \n",
    "        #ur = survey_df_i['urban_rural'].iloc[0]\n",
    "        #if ur == 'U':\n",
    "        #    buffer_size = buffer_size_urban\n",
    "        #elif ur == 'R':\n",
    "        #    buffer_size = buffer_size_rural\n",
    "        buffer_size = buffer_size_urban\n",
    "\n",
    "        f_i = ee.Feature(ee.Geometry.Point([survey_df_i['longitude'].iloc[0], \n",
    "                                            survey_df_i['latitude'].iloc[0]]), \n",
    "                         {'uid': survey_df_i['uid'].iloc[0],\n",
    "                          'year': str(survey_df_i['year'].iloc[0])})\n",
    "        \n",
    "        f_i = f_i.buffer(buffer_size)\n",
    "\n",
    "        survey_fc_list.append(f_i)\n",
    "        \n",
    "    survey_fc = ee.FeatureCollection(survey_fc_list)\n",
    "    \n",
    "    return survey_fc\n",
    "\n",
    "def extract_sat(survey_df, buffer_size_urban, buffer_size_rural, year, satellite, survey_name, file_name):\n",
    "    '''\n",
    "    Extract satellite imagery to locations \n",
    "    \n",
    "    Inputs:\n",
    "        survey_df: pandas dataframe of survey locations. Function assumes \n",
    "                   the dataframe contains (1) latitude, (2) longitude and\n",
    "                   (3) uid variables. Assumes coordinates in WGS84.\n",
    "    Returns:\n",
    "        (feature collection)\n",
    "    '''\n",
    "    \n",
    "    POLLUTION_SCALE = 10000 # HERE!\n",
    "        \n",
    "    year_start_sp5 = \"2018-01-01\"\n",
    "    year_end_sp5 = '2020-12-31'\n",
    "    \n",
    "    # Prep worldpop -----------------------------------------------\n",
    "    if satellite == 'worldpop':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 100 \n",
    "        \n",
    "        # Year\n",
    "        year_use = min([2020, year])\n",
    "                \n",
    "        year_plus = year_use\n",
    "        year_minus = year_use\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('WorldPop/GP/100m/pop')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .median()\n",
    "        \n",
    "        # After the reducer computers the sum, it names the value \"sum\", not population\n",
    "        BANDS = ['sum']\n",
    "        \n",
    "    # Prep worldpop_2020 ---------------------------------------------\n",
    "    if satellite == 'worldpop2020':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 100 \n",
    "        \n",
    "        # Year\n",
    "        year_use = '2020'\n",
    "                \n",
    "        year_plus = year_use\n",
    "        year_minus = year_use\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('WorldPop/GP/100m/pop')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .median()\n",
    "        \n",
    "        # After the reducer computers the sum, it names the value \"sum\", not population\n",
    "        BANDS = ['sum']\n",
    "            \n",
    "    # Sentinel-5P OFFL AER AI: Offline UV Aerosol Index  -------------------\n",
    "    if satellite == 'uv_aer':\n",
    "        \n",
    "        # Scale\n",
    "        #SCALE = 1113.2 # takes too long\n",
    "        #SCALE = 10000\n",
    "        SCALE = POLLUTION_SCALE\n",
    "        \n",
    "        # Starts in 2018; take all years\n",
    "        image = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_AER_AI\")\\\n",
    "            .filterDate(year_start_sp5, year_end_sp5)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['absorbing_aerosol_index']\n",
    "        \n",
    "    # Sentinel-5P OFFL CO: Offline Carbon Monoxide  -------------------\n",
    "    if satellite == 'CO':\n",
    "        \n",
    "        # Scale\n",
    "        #SCALE = 1113.2 # takes too long\n",
    "        #SCALE = 10000\n",
    "        SCALE = POLLUTION_SCALE\n",
    "        \n",
    "        # Starts in 2018; take all years\n",
    "        image = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_CO\")\\\n",
    "            .filterDate(year_start_sp5, year_end_sp5)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['CO_column_number_density', 'H2O_column_number_density']\n",
    "        \n",
    "    # Sentinel-5P OFFL HCHO: Offline Formaldehyde  -------------------\n",
    "    if satellite == 'HCHO':\n",
    "        \n",
    "        # Scale\n",
    "        #SCALE = 1113.2 # takes too long\n",
    "        #SCALE = 10000\n",
    "        SCALE = POLLUTION_SCALE\n",
    "        \n",
    "        # Starts in 2018; take all years\n",
    "        image = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_HCHO\")\\\n",
    "            .filterDate(year_start_sp5, year_end_sp5)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['tropospheric_HCHO_column_number_density', 'tropospheric_HCHO_column_number_density_amf']\n",
    "        \n",
    "    # Sentinel-5P Nitrogen Dioxide  -----------------------------\n",
    "    if satellite == 'NO2':\n",
    "        \n",
    "        # Scale\n",
    "        #SCALE = 1113.2 # takes too long\n",
    "        #SCALE = 10000\n",
    "        SCALE = POLLUTION_SCALE\n",
    "        \n",
    "        # Starts in 2018; take all years\n",
    "        image = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_NO2\")\\\n",
    "            .filterDate(year_start_sp5, year_end_sp5)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['NO2_column_number_density', 'tropospheric_NO2_column_number_density',\\\n",
    "                 'stratospheric_NO2_column_number_density', 'NO2_slant_column_number_density']\n",
    "        \n",
    "    # Sentinel-5P OFFL O3: Offline Ozone  -------------------\n",
    "    if satellite == 'ozone':\n",
    "        \n",
    "        # Scale\n",
    "        #SCALE = 1113.2 # takes too long\n",
    "        #SCALE = 10000\n",
    "        SCALE = POLLUTION_SCALE\n",
    "        \n",
    "        # Starts in 2018; take all years\n",
    "        image = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_O3\")\\\n",
    "            .filterDate(year_start_sp5, year_end_sp5)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['O3_column_number_density', 'O3_effective_temperature']\n",
    "        \n",
    "    # Sentinel-5P OFFL SO2: Offline Sulphur Dioxide  -------------------\n",
    "    if satellite == 'SO2':\n",
    "        \n",
    "        # Scale\n",
    "        #SCALE = 1113.2 # takes too long\n",
    "        #SCALE = 10000\n",
    "        SCALE = POLLUTION_SCALE\n",
    "        \n",
    "        # Starts in 2018; take all years\n",
    "        image = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_SO2\")\\\n",
    "            .filterDate(year_start_sp5, year_end_sp5)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['SO2_column_number_density', 'SO2_column_number_density_amf', 'SO2_slant_column_number_density']\n",
    "        \n",
    "    # Sentinel-5P OFFL CH4: Offline Methane  -------------------\n",
    "    if satellite == 'CH4':\n",
    "        \n",
    "        # Scale\n",
    "        #SCALE = 1113.2 # takes too long\n",
    "        #SCALE = 10000\n",
    "        SCALE = POLLUTION_SCALE\n",
    "        \n",
    "        # Starts in 2018; take all years\n",
    "        image = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_CH4\")\\\n",
    "            .filterDate(year_start_sp5, year_end_sp5)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['CH4_column_volume_mixing_ratio_dry_air']\n",
    "        \n",
    "    # CSP gHM: Global Human Modification ---------------------------------\n",
    "    if satellite == 'GlobalHumanModification':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 1000 # ok to upscale\n",
    "                \n",
    "        image = ee.ImageCollection(\"CSP/HM/GlobalHumanModification\")\\\n",
    "            .median()\n",
    "        \n",
    "        # Original name is \"gHM\", but because only one value, it takes the\n",
    "        # name of the reducer; we use mean\n",
    "        BANDS = ['mean']\n",
    "        \n",
    "    # WorldClim BIO Variables V1 ---------------------------------\n",
    "    if satellite == 'worldclim_bio':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 1000 # ok to upscale\n",
    "                \n",
    "        image = ee.Image('WORLDCLIM/V1/BIO')\n",
    "        \n",
    "        BANDS = ['bio01', 'bio02', 'bio03', 'bio04', 'bio05', 'bio06', 'bio07', 'bio08', 'bio09', 'bio10',\\\n",
    "                 'bio11', 'bio12', 'bio13', 'bio14', 'bio15', 'bio16', 'bio17', 'bio18', 'bio19']\n",
    "        \n",
    "    # Elevation - SRTM ------------------------------------------\n",
    "    if satellite == 'elevation':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 1000 # ok to upscale\n",
    "                \n",
    "        image = ee.Image('USGS/SRTMGL1_003') # CGIAR/SRTM90_V4\n",
    "        \n",
    "        # elevation?\n",
    "        BANDS = ['mean']\n",
    "        \n",
    "    # Elevation - SRTM ------------------------------------------\n",
    "    if satellite == 'slope':\n",
    "        # https://developers.google.com/earth-engine/datasets/catalog/CGIAR_SRTM90_V4#description\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 500 # ok to upscale\n",
    "                \n",
    "        image_raw = ee.Image('USGS/SRTMGL1_003') # CGIAR/SRTM90_V4\n",
    "        image_elev = image_raw.select('elevation')\n",
    "        image = ee.Terrain.slope(image_elev)\n",
    "                \n",
    "        # mean?\n",
    "        BANDS = ['mean']\n",
    "        \n",
    "    # Prep l5 ---------------------------------------------------\n",
    "    if satellite == 'l5':\n",
    "        \n",
    "        SCALE = 100 # ok to upscale\n",
    "        #SCALE = 2000\n",
    "        \n",
    "        ### Year\n",
    "        # (1) landsat 5 starts in March 1984; if year is less than\n",
    "        #     1985, use 1985 as year (to ensure have year before and after)\n",
    "        # (2) landsat 5 ends in May 2012; if year is greater than\n",
    "        #     2011, use 2011 as year\n",
    "        if year < 1985:\n",
    "            year_use = 1985\n",
    "        elif year > 2011:\n",
    "            year_use = 2011\n",
    "        else:\n",
    "            year_use = year\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        #image = ee.ImageCollection('LANDSAT/LT05/C02/T1_L2')\\\n",
    "        #    .filterDate(year_minus_str, year_plus_str)\\\n",
    "        #    #.map(cloud_mask_landsatSR)\\\n",
    "        #    .median()\\\n",
    "        #    .multiply(0.0001)\n",
    "        \n",
    "        image = ee.ImageCollection('LANDSAT/LT05/C02/T1_L2')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .median()\\\n",
    "            .multiply(0.0001)\n",
    "\n",
    "        # https://www.linkedin.com/pulse/ndvi-ndbi-ndwi-calculation-using-landsat-7-8-tek-bahadur-kshetri\n",
    "        ndvi = image.normalizedDifference(['B4', 'B3']).rename('NDVI');\n",
    "        ndbi = image.normalizedDifference(['B5', 'B4']).rename('NDBI');\n",
    "        image = image.addBands(ndvi)\n",
    "        image = image.addBands(ndbi)\n",
    "        \n",
    "        bu = image.select('NDBI').subtract(image.select('NDVI')).rename('BU')\n",
    "        image = image.addBands(bu)\n",
    "        \n",
    "        BANDS = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'NDVI', 'NDBI', 'BU']\n",
    "        \n",
    "    # Prep l7 ---------------------------------------------------\n",
    "    if (satellite == 'l7') | (satellite == 'l7_sdspace'):\n",
    "        \n",
    "        SCALE = 100 # ok to upscale\n",
    "        #SCALE = 2000\n",
    "        \n",
    "        # Year\n",
    "        # landsat 7 starts in May 1999; if year is less than\n",
    "        # 2000, use 2000 as year (to ensure have year before and after)\n",
    "        if year < 2000:\n",
    "            year_use = 2000\n",
    "        else:\n",
    "            year_use = year\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('LANDSAT/LE07/C01/T1_SR')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .map(cloud_mask_landsatSR)\\\n",
    "            .median()\\\n",
    "            .multiply(0.0001)\n",
    "\n",
    "        # https://www.linkedin.com/pulse/ndvi-ndbi-ndwi-calculation-using-landsat-7-8-tek-bahadur-kshetri\n",
    "        ndvi = image.normalizedDifference(['B4', 'B3']).rename('NDVI');\n",
    "        ndbi = image.normalizedDifference(['B5', 'B4']).rename('NDBI');\n",
    "        image = image.addBands(ndvi)\n",
    "        image = image.addBands(ndbi)\n",
    "        \n",
    "        bu = image.select('NDBI').subtract(image.select('NDVI')).rename('BU')\n",
    "        image = image.addBands(bu)\n",
    "        \n",
    "        BANDS = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'NDVI', 'NDBI', 'BU']\n",
    "        \n",
    "    # Prep l7 ---------------------------------------------------\n",
    "    if satellite == 'l7_sdtime':\n",
    "        \n",
    "        SCALE = 100 # ok to upscale\n",
    "        #SCALE = 2000\n",
    "        \n",
    "        # Year\n",
    "        # landsat 7 starts in May 1999; if year is less than\n",
    "        # 2000, use 2000 as year (to ensure have year before and after)\n",
    "        if year < 2000:\n",
    "            year_use = 2000\n",
    "        else:\n",
    "            year_use = year\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('LANDSAT/LE07/C01/T1_SR')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .map(cloud_mask_landsatSR)\\\n",
    "            .reduce(ee.Reducer.stdDev())\n",
    "        \n",
    "        BANDS = ['B1_stdDev', 'B2_stdDev', 'B3_stdDev', 'B4_stdDev', 'B5_stdDev', 'B6_stdDev', 'B7_stdDev']\n",
    "                \n",
    "    # Prep l8 ---------------------------------------------------\n",
    "    if (satellite == 'l8') | (satellite == 'l8_sdspace'):\n",
    "        \n",
    "        SCALE = 100 # ok to upscale\n",
    "        #SCALE = 2000\n",
    "        \n",
    "        # Year\n",
    "        # landsat 8 starts in April 2013; if year is less than\n",
    "        # 2014, use 2014 as year (to ensure have year before and after)\n",
    "        if year < 2014:\n",
    "            year_use = 2014\n",
    "        else:\n",
    "            year_use = year\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .map(cloud_mask_landsatSR)\\\n",
    "            .median()\\\n",
    "            .multiply(0.0001)\n",
    "\n",
    "        # https://www.linkedin.com/pulse/ndvi-ndbi-ndwi-calculation-using-landsat-7-8-tek-bahadur-kshetri\n",
    "        ndvi = image.normalizedDifference(['B5', 'B4']).rename('NDVI');\n",
    "        ndbi = image.normalizedDifference(['B6', 'B5']).rename('NDBI');\n",
    "        image = image.addBands(ndvi)\n",
    "        image = image.addBands(ndbi)\n",
    "        \n",
    "        bu = image.select('NDBI').subtract(image.select('NDVI')).rename('BU')\n",
    "        image = image.addBands(bu)\n",
    "        \n",
    "        BANDS = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B10', 'B11', 'NDVI', 'NDBI', 'BU']\n",
    "        \n",
    "    # Prep l8 ---------------------------------------------------\n",
    "    if satellite == 'l8_sdtime':\n",
    "        \n",
    "        SCALE = 100 # ok to upscale\n",
    "        #SCALE = 2000\n",
    "        \n",
    "        # Year\n",
    "        # landsat 8 starts in April 2013; if year is less than\n",
    "        # 2014, use 2014 as year (to ensure have year before and after)\n",
    "        if year < 2014:\n",
    "            year_use = 2014\n",
    "        else:\n",
    "            year_use = year\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .map(cloud_mask_landsatSR)\\\n",
    "            .reduce(ee.Reducer.stdDev())\n",
    "        \n",
    "        BANDS = ['B1_stdDev', 'B2_stdDev', 'B3_stdDev', 'B4_stdDev', 'B5_stdDev', 'B6_stdDev', 'B7_stdDev', 'B10_stdDev', 'B11_stdDev']\n",
    "                \n",
    "    # Prep s2 ---------------------------------------------------\n",
    "    if satellite == 's2':\n",
    "        \n",
    "        SCALE = 100 # ok to upscale\n",
    "        \n",
    "        # Year\n",
    "        # sentinel starts in March 2017; juse use 2018\n",
    "        year_use = 2018\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-12-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('COPERNICUS/S2_SR')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .map(cloud_mask_sentinel2)\\\n",
    "            .median()\\\n",
    "            .multiply(0.0001)\n",
    "        \n",
    "        ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI');\n",
    "        image = image.addBands(ndvi)\n",
    "        \n",
    "        BANDS = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12', 'AOT', 'NDVI']\n",
    "\n",
    "        image = image.select(BANDS) \n",
    "      \n",
    "    # Prep SAR Median - HH/HV DESC ---------------------------------------------------\n",
    "    # https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S1_GRD\n",
    "    if 's1_sar' in satellite:\n",
    "        \n",
    "        SCALE = 10 # ok to upscale\n",
    "    \n",
    "        ## H/V\n",
    "        if 'hh' in satellite:\n",
    "            HV_VAR = 'HH'\n",
    "            \n",
    "        if 'hv' in satellite:\n",
    "            HV_VAR = 'HV'\n",
    "            \n",
    "        if 'vv' in satellite:\n",
    "            HV_VAR = 'VV'\n",
    "            \n",
    "        if 'vh' in satellite:\n",
    "            HV_VAR = 'VH'\n",
    "            \n",
    "        if 'vdiv' in satellite:\n",
    "            HV_VAR = 'VV_DIV_VH'\n",
    "            \n",
    "        ## A/D\n",
    "        if 'desc' in satellite:\n",
    "            AD_VAR = 'DESCENDING'\n",
    "            \n",
    "        if 'asc' in satellite:\n",
    "            AD_VAR = 'ASCENDING'\n",
    "            \n",
    "        ## Year\n",
    "        year_use = 2018\n",
    "            \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        ## Image\n",
    "        if HV_VAR == 'VV_DIV_VH':\n",
    "            \n",
    "            image_vv = ee.ImageCollection('COPERNICUS/S1_GRD')\\\n",
    "                .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV'))\\\n",
    "                .filter(ee.Filter.eq('instrumentMode', 'IW'))\\\n",
    "                .filter(ee.Filter.eq('orbitProperties_pass', AD_VAR))\\\n",
    "                .select('VV')\\\n",
    "                .filterDate(year_minus_str, year_plus_str)\\\n",
    "                .mean()\n",
    "\n",
    "            image_vh = ee.ImageCollection('COPERNICUS/S1_GRD')\\\n",
    "                .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VH'))\\\n",
    "                .filter(ee.Filter.eq('instrumentMode', 'IW'))\\\n",
    "                .filter(ee.Filter.eq('orbitProperties_pass', AD_VAR))\\\n",
    "                .select('VH')\\\n",
    "                .filterDate(year_minus_str, year_plus_str)\\\n",
    "                .mean()\n",
    "\n",
    "            image = image_vv.divide(image_vh)\n",
    "        \n",
    "        else: \n",
    "            image = ee.ImageCollection('COPERNICUS/S1_GRD')\\\n",
    "                .filter(ee.Filter.listContains('transmitterReceiverPolarisation', HV_VAR))\\\n",
    "                .filter(ee.Filter.eq('instrumentMode', 'IW'))\\\n",
    "                .filter(ee.Filter.eq('orbitProperties_pass', AD_VAR))\\\n",
    "                .select(HV_VAR)\\\n",
    "                .filterDate(year_minus_str, year_plus_str)\\\n",
    "                .mean()\n",
    "\n",
    "        ## Mean / Std Dev\n",
    "        #if 'mean' in satellite:\n",
    "        #    image = image.mean()\n",
    "            \n",
    "        #if 'stddev' in satellite:\n",
    "        #    image = image.reduce(ee.Reducer.stdDev())\n",
    "            \n",
    "        BANDS = ['mean']\n",
    "                \n",
    "\n",
    "    # Prep drought ---------------------------------------------------\n",
    "    if satellite == 'gridmet_drought':\n",
    "        \n",
    "        SCALE = 5000 \n",
    "\n",
    "        year_minus_str = str(year) + '-01-01'\n",
    "        year_plus_str = str(year) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection(\"GRIDMET/DROUGHT\")\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['pdsi', 'z', 'eddi1y', 'eddi2y', 'eddi5y']\n",
    "        \n",
    "    # Prep AOD ------------------------------------------------------\n",
    "    if satellite == 'aod':\n",
    "        \n",
    "        SCALE = 1000 \n",
    "        \n",
    "        year_use = max([2001, year])\n",
    "\n",
    "        year_minus_str = str(year_use) + '-01-01'\n",
    "        year_plus_str = str(year_use) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection(\"MODIS/006/MCD19A2_GRANULES\")\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['Optical_Depth_047', 'Optical_Depth_055']\n",
    "    \n",
    "    # Prep drought ---------------------------------------------------\n",
    "    if satellite == 'ecmwf_weather':\n",
    "        \n",
    "        SCALE = 1000 \n",
    "\n",
    "        # Data available until July 2020\n",
    "        year_use = min([2019, year])\n",
    "        \n",
    "        year_minus_str = str(year_use) + '-01-01'\n",
    "        year_plus_str = str(year_use) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection(\"ECMWF/ERA5/DAILY\")\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .mean()\n",
    "        \n",
    "        BANDS = ['mean_2m_air_temperature', \n",
    "                 'minimum_2m_air_temperature', \n",
    "                 'maximum_2m_air_temperature',\n",
    "                 'total_precipitation']\n",
    "        \n",
    "    # Prep drought ---------------------------------------------------\n",
    "    if satellite == 'ecmwf_weather_q1':\n",
    "        \n",
    "        SCALE = 1000 \n",
    "        \n",
    "        # Data available until July 2020\n",
    "        year_use = min([2019, year])\n",
    "\n",
    "        year_minus_str = str(year_use) + '-01-01'\n",
    "        year_plus_str = str(year_use) + '-03-31'\n",
    "        \n",
    "        image = ee.ImageCollection(\"ECMWF/ERA5/DAILY\")\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .mean()\n",
    "        \n",
    "        BANDS = ['mean_2m_air_temperature', \n",
    "                 'minimum_2m_air_temperature', \n",
    "                 'maximum_2m_air_temperature',\n",
    "                 'total_precipitation']\n",
    "        \n",
    "    # Prep drought ---------------------------------------------------\n",
    "    if satellite == 'ecmwf_weather_q2':\n",
    "        \n",
    "        SCALE = 1000 \n",
    "        \n",
    "        # Data available until July 2020\n",
    "        year_use = min([2019, year])\n",
    "\n",
    "        year_minus_str = str(year_use) + '-04-01'\n",
    "        year_plus_str = str(year_use) + '-06-30'\n",
    "        \n",
    "        image = ee.ImageCollection(\"ECMWF/ERA5/DAILY\")\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .mean()\n",
    "        \n",
    "        BANDS = ['mean_2m_air_temperature', \n",
    "                 'minimum_2m_air_temperature', \n",
    "                 'maximum_2m_air_temperature',\n",
    "                 'total_precipitation']\n",
    "        \n",
    "    # Prep drought ---------------------------------------------------\n",
    "    if satellite == 'ecmwf_weather_q3':\n",
    "        \n",
    "        SCALE = 1000 \n",
    "        \n",
    "        # Data available until July 2020\n",
    "        year_use = min([2019, year])\n",
    "\n",
    "        year_minus_str = str(year_use) + '-07-01'\n",
    "        year_plus_str = str(year_use) + '-09-30'\n",
    "        \n",
    "        image = ee.ImageCollection(\"ECMWF/ERA5/DAILY\")\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .mean()\n",
    "        \n",
    "        BANDS = ['mean_2m_air_temperature', \n",
    "                 'minimum_2m_air_temperature', \n",
    "                 'maximum_2m_air_temperature',\n",
    "                 'total_precipitation']\n",
    "        \n",
    "    # Prep drought ---------------------------------------------------\n",
    "    if satellite == 'ecmwf_weather_q4':\n",
    "        \n",
    "        SCALE = 1000 \n",
    "        \n",
    "        # Data available until July 2020\n",
    "        year_use = min([2019, year])\n",
    "\n",
    "        year_minus_str = str(year_use) + '-10-01'\n",
    "        year_plus_str = str(year_use) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection(\"ECMWF/ERA5/DAILY\")\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .mean()\n",
    "        \n",
    "        BANDS = ['mean_2m_air_temperature', \n",
    "                 'minimum_2m_air_temperature', \n",
    "                 'maximum_2m_air_temperature',\n",
    "                 'total_precipitation']\n",
    "    \n",
    "    # Prep viirs ---------------------------------------------------\n",
    "    if (satellite == 'viirs') | (satellite == 'viirs_sdspace'):\n",
    "        \n",
    "        SCALE = 500 \n",
    "        \n",
    "        # Year\n",
    "        # VIIRS starts in April 2012; if year is less than\n",
    "        # 2013, use 2013 as year (to ensure have year before and after)\n",
    "        if year < 2013:\n",
    "            year_use = 2013\n",
    "        else:\n",
    "            year_use = year\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('NOAA/VIIRS/DNB/MONTHLY_V1/VCMCFG')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['avg_rad']\n",
    "        \n",
    "    # Prep viirs ---------------------------------------------------\n",
    "    # https://gis.stackexchange.com/questions/344626/gee-pixel-based-sd-over-time-series-sentinel-2-ndvi\n",
    "    if satellite == 'viirs_sdtime':\n",
    "        \n",
    "        SCALE = 500 \n",
    "        \n",
    "        # Year\n",
    "        # VIIRS starts in April 2012; if year is less than\n",
    "        # 2013, use 2013 as year (to ensure have year before and after)\n",
    "        if year < 2013:\n",
    "            year_use = 2013\n",
    "        else:\n",
    "            year_use = year\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('NOAA/VIIRS/DNB/MONTHLY_V1/VCMCFG')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .reduce(ee.Reducer.stdDev())\n",
    "        \n",
    "        BANDS = ['avg_rad_stdDev']\n",
    "        \n",
    "    # Prep viirs181920 ---------------------------------------------------\n",
    "    if satellite == 'viirs181920':\n",
    "        \n",
    "        SCALE = 500 \n",
    "        \n",
    "        # Year\n",
    "        # VIIRS starts in April 2012; if year is less than\n",
    "        # 2013, use 2013 as year (to ensure have year before and after)\n",
    "        year_use = 2019\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('NOAA/VIIRS/DNB/MONTHLY_V1/VCMCFG')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['avg_rad']\n",
    "        \n",
    "    # Prep DMSP ---------------------------------------------------\n",
    "    if satellite == 'dmsp':\n",
    "        \n",
    "        SCALE = 1000 \n",
    "        \n",
    "        # Year\n",
    "        # DMSP-OLS starts in 2013; if year is more than\n",
    "        # 2012, use 2012 as year (to ensure have year before and after)\n",
    "        if year > 2012:\n",
    "            year_use = 2012\n",
    "        else:\n",
    "            year_use = year\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('NOAA/DMSP-OLS/NIGHTTIME_LIGHTS')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['stable_lights', 'avg_lights_x_pct']\n",
    "    \n",
    "    # Prep Survey ---------------------------------------------------\n",
    "    survey_fc = survey_to_fc_buffer(survey_df, buffer_size_urban, buffer_size_rural)\n",
    "        \n",
    "    # Extract Values ---------------------------------------------------\n",
    "    if (satellite == 'worldpop') | (satellite == 'worldpop2020'):\n",
    "        vals = image.reduceRegions(collection = survey_fc,\n",
    "                                   reducer = ee.Reducer.sum(),\n",
    "                                   scale = SCALE,\n",
    "                                   tileScale = 1)\n",
    "    elif (satellite == 'viirs_sdspace') | (satellite == 'l7_sdspace') | (satellite == 'l8_sdspace') | ( ('s1_sar' in satellite) & ('stddev' in satellite) ):\n",
    "        vals = image.reduceRegions(collection = survey_fc,\n",
    "                           reducer = ee.Reducer.stdDev(),\n",
    "                           scale = SCALE,\n",
    "                           tileScale = 8)   \n",
    "    elif (satellite == 'NO2') | (satellite == 'uv_aer') | (satellite == 'CO') | (satellite == 'HCHO') | (satellite == 'ozone') | (satellite == 'SO2') | (satellite == 'CH4'):\n",
    "        \n",
    "        #vals = survey_fc.map(lambda feature: ee.Feature(None, image.reduceRegion(\n",
    "        #    reducer=ee.Reducer.mean(),\n",
    "        #    geometry=feature.geometry(),\n",
    "        #    scale=SCALE,\n",
    "        #    bestEffort = True\n",
    "        #)))\n",
    "        \n",
    "        vals = image.reduceRegions(collection = survey_fc,\n",
    "                                   reducer = ee.Reducer.mean(),\n",
    "                                   scale = SCALE) \n",
    "        \n",
    "    else:\n",
    "        \n",
    "        vals = image.reduceRegions(collection = survey_fc,\n",
    "                                   reducer = ee.Reducer.mean(),\n",
    "                                   scale = SCALE,\n",
    "                                   tileScale = 8) \n",
    "        \n",
    "\n",
    "        \n",
    "    # OLD =============\n",
    "    # Survey dataset that only contains the uid variable\n",
    "    #survey_df = survey_df[['uid']]\n",
    "            \n",
    "    #for band_i in BANDS:\n",
    "    #    survey_df[satellite + '_' + band_i] = vals.aggregate_array(band_i).getInfo()\n",
    "        \n",
    "    # NEW =============\n",
    "    #df_out = fc2df(vals)\n",
    "    #print(df_out)\n",
    "    #df_out = pd.DataFrame()\n",
    "    \n",
    "    bands_to_export = BANDS.copy()\n",
    "    bands_to_export.append('uid')\n",
    "    bands_to_export.append('year')\n",
    "    \n",
    "    task = ee.batch.Export.table.toDrive(collection=vals, \n",
    "                                         folder='satellite_data_from_gee_' + survey_name.lower(), \n",
    "                                         description=file_name, \n",
    "                                         fileFormat='CSV',\n",
    "                                         selectors = bands_to_export)\n",
    "    \n",
    "    task.start()\n",
    "    \n",
    "    if False:\n",
    "        time_elapsed = 0\n",
    "        while task.active():\n",
    "            if((time_elapsed % 60) == 0):\n",
    "                print('Polling for task (id: {}).'.format(task.id))\n",
    "            time.sleep(5)\n",
    "            time_elapsed = time_elapsed + 5\n",
    "        \n",
    "    return task\n",
    "\n",
    "def extract_satellite_in_chunks(survey_df, buffer_size_urban, buffer_size_rural, satellite, file_name, year, survey_name):\n",
    "    \n",
    "    vals_df_list = []\n",
    "    \n",
    "    for chunk_i in list(np.unique(survey_df.chunk_id)):\n",
    "\n",
    "        survey_df_i = survey_df[survey_df['chunk_id'] == chunk_i]\n",
    "        vals_i_df = extract_sat(survey_df_i, buffer_size_urban, buffer_size_rural, satellite, file_name, year, chunk_i, survey_name)\n",
    "\n",
    "        vals_df_list.append(vals_i_df)\n",
    "    \n",
    "    return vals_df_list\n",
    "\n",
    "def extract_satellite_by_year(survey_df, buffer_size_urban, buffer_size_rural, satellite, file_name, survey_name):\n",
    "    \n",
    "    vals_df_list = []\n",
    "    \n",
    "    for year_i in list(np.unique(survey_df.year)):\n",
    "\n",
    "        survey_df_i = survey_df[survey_df['year'] == year_i]\n",
    "        vals_i_df = extract_satellite_in_chunks(survey_df_i, buffer_size_urban, buffer_size_rural, satellite, file_name, year_i, survey_name)\n",
    "\n",
    "        vals_df_list.append(vals_i_df)\n",
    "    \n",
    "    return vals_df_list\n",
    "\n",
    "def chunk_ids(total_length, chunk_size):\n",
    "    n_numbers = np.ceil(total_length / chunk_size)\n",
    "    n_numbers = int(n_numbers)\n",
    "    \n",
    "    chunk_ids = list(range(0,n_numbers)) * chunk_size\n",
    "    chunk_ids.sort()\n",
    "    chunk_ids = chunk_ids[:total_length]\n",
    "    \n",
    "    return chunk_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load/Prep Survey Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df = pd.read_csv(os.path.join(DROPBOX_DIRECTORY, 'Data', SURVEY_NAME, 'FinalData', 'Individual Datasets', 'survey_socioeconomic.csv'))\n",
    "survey_df = survey_df[['uid', 'year', 'latitude', 'longitude', 'most_recent_survey']] # urban_rural\n",
    "survey_df = survey_df.sort_values('year')\n",
    "\n",
    "survey_years = list(survey_df.year.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          uid  year   latitude  longitude  most_recent_survey\n",
      "0      id_103  2018   6.238425   1.632913                True\n",
      "2147  id_2183  2018   9.999258  -5.578881                True\n",
      "2148  id_1222  2018  10.014145  -5.903957                True\n",
      "2149  id_1223  2018  10.017717  -6.669962                True\n",
      "2150  id_1224  2018  10.021108  -7.199163                True\n",
      "(3939, 5)\n"
     ]
    }
   ],
   "source": [
    "print(survey_df.head())\n",
    "print(survey_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If re-extract, delete existing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting existing files from Google Drive\n"
     ]
    }
   ],
   "source": [
    "if REEXTRACT_IF_FILE_EXISTS:\n",
    "    print(\"Deleting existing files from Google Drive\")\n",
    "\n",
    "    ## Path with files\n",
    "    OUT_PATH = os.path.join(GOOGLEDRIVE_DIRECTORY, \n",
    "                            'Data', \n",
    "                             SURVEY_NAME, \n",
    "                             'FinalData', \n",
    "                             'Individual Datasets',\n",
    "                             'satellite_data_from_gee_' + SURVEY_NAME.lower())\n",
    "\n",
    "    ## Grab csv files\n",
    "    files_to_rm = [x for x in os.listdir(OUT_PATH) if '.csv' in x]\n",
    "\n",
    "    ## Delete files\n",
    "    for file_i in files_to_rm:\n",
    "\n",
    "        path_i = os.path.join(OUT_PATH, file_i)\n",
    "        os.remove(path_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of files already extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Path with files\n",
    "OUT_PATH = os.path.join(GOOGLEDRIVE_DIRECTORY, \n",
    "                        'Data', \n",
    "                         SURVEY_NAME, \n",
    "                         'FinalData', \n",
    "                         'Individual Datasets',\n",
    "                         'satellite_data_from_gee_' + SURVEY_NAME.lower())\n",
    "\n",
    "## Grab csv files\n",
    "files_extracted = [x for x in os.listdir(OUT_PATH) if '.csv' in x]\n",
    "\n",
    "len(files_extracted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"file\": check if file exists\n",
    "# \"data\": check processed data\n",
    "\n",
    "how_check_processed = 'file' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elevation\n",
      "slope\n",
      "viirs_1120\n",
      "viirs_2500\n",
      "viirs_3360\n",
      "viirs181920_1120\n",
      "viirs181920_3360\n",
      "viirs_sdtime_2500\n",
      "viirs_sdspace_2500\n",
      "GlobalHumanModification\n",
      "worldpop_1000\n",
      "worldpop_2000\n",
      "worldpop_5000\n",
      "worldpop_10000\n",
      "worldpop2020_1000\n",
      "worldpop2020_2000\n",
      "worldpop2020_5000\n",
      "worldpop2020_10000\n",
      "l8\n",
      "l7\n",
      "l8_sdtime\n",
      "l7_sdtime\n",
      "l8_sdspace\n",
      "l7_sdspace\n",
      "aod\n",
      "ecmwf_weather\n",
      "ecmwf_weather_q1\n",
      "ecmwf_weather_q2\n",
      "ecmwf_weather_q3\n",
      "ecmwf_weather_q4\n",
      "s1_sar_vv_desc_mean\n",
      "s1_sar_vv_desc_stddev\n",
      "s1_sar_vv_asc_mean\n",
      "s1_sar_vv_asc_stddev\n",
      "s1_sar_vh_desc_mean\n",
      "s1_sar_vh_desc_stddev\n",
      "s1_sar_vh_asc_mean\n",
      "s1_sar_vh_asc_stddev\n",
      "s1_sar_vdiv_desc_mean\n",
      "s1_sar_vdiv_desc_stddev\n",
      "s1_sar_vdiv_asc_mean\n",
      "s1_sar_vdiv_asc_stddev\n"
     ]
    }
   ],
   "source": [
    "to_extract = ['elevation', \n",
    "              'slope',\n",
    "              'viirs_1120',\n",
    "              'viirs_2500',\n",
    "              'viirs_3360',\n",
    "              'viirs181920_1120',\n",
    "              'viirs181920_3360',\n",
    "              'viirs_sdtime_2500',\n",
    "              'viirs_sdspace_2500',\n",
    "              'GlobalHumanModification',\n",
    "              'worldpop_1000',\n",
    "              'worldpop_2000',\n",
    "              'worldpop_5000',\n",
    "              'worldpop_10000',\n",
    "              'worldpop2020_1000',\n",
    "              'worldpop2020_2000',\n",
    "              'worldpop2020_5000',\n",
    "              'worldpop2020_10000',\n",
    "              'l8',\n",
    "              'l7',\n",
    "              'l8_sdtime',\n",
    "              'l7_sdtime',\n",
    "              'l8_sdspace',\n",
    "              'l7_sdspace',\n",
    "              'aod',\n",
    "              'ecmwf_weather',\n",
    "              'ecmwf_weather_q1',\n",
    "              'ecmwf_weather_q2',\n",
    "              'ecmwf_weather_q3',\n",
    "              'ecmwf_weather_q4',\n",
    "             's1_sar_vv_desc_mean',\n",
    "             's1_sar_vv_desc_stddev',\n",
    "             's1_sar_vv_asc_mean',\n",
    "             's1_sar_vv_asc_stddev',\n",
    "             's1_sar_vh_desc_mean',\n",
    "             's1_sar_vh_desc_stddev',\n",
    "             's1_sar_vh_asc_mean',\n",
    "             's1_sar_vh_asc_stddev',\n",
    "             's1_sar_vdiv_desc_mean',\n",
    "             's1_sar_vdiv_desc_stddev',\n",
    "             's1_sar_vdiv_asc_mean',\n",
    "             's1_sar_vdiv_asc_stddev']\n",
    "\n",
    "tasks_all = []\n",
    "\n",
    "# Loop over satellites ------------------------------\n",
    "for name in to_extract:\n",
    "    print(name)\n",
    "        \n",
    "    sat = name\n",
    "    \n",
    "    if name in ['NO2', 'uv_aer', 'CO', 'HCHO', 'ozone', 'SO2', 'CH4', 'l8', 'l7', 'l5', 'l7_sdtime', 'l8_sdtime', 'l7_sdspace', 'l8_sdspace', 'aod', 'GlobalHumanModification', 'elevation', 'slope']:\n",
    "        if SURVEY_NAME == \"DHS\":\n",
    "            buffer_u = 2500\n",
    "            buffer_r = 2500\n",
    "            \n",
    "        if SURVEY_NAME == \"DHS_policy_experiment\":\n",
    "            buffer_u = 2500\n",
    "            buffer_r = 2500\n",
    "            \n",
    "        if SURVEY_NAME == \"DHS_nga_policy_experiment\":\n",
    "            buffer_u = 2500\n",
    "            buffer_r = 2500\n",
    "            \n",
    "        if SURVEY_NAME == \"LSMS\":\n",
    "            buffer_u = 2500\n",
    "            buffer_r = 2500\n",
    "            \n",
    "        if SURVEY_NAME == \"PAK_POINTS\":\n",
    "            buffer_u = 1500\n",
    "            buffer_r = 1500\n",
    "            \n",
    "        if SURVEY_NAME == \"PAK_CITY_POINTS\":\n",
    "            buffer_u = 750\n",
    "            buffer_r = 750\n",
    "            \n",
    "        if SURVEY_NAME == \"LAGOS_POINTS\":\n",
    "            buffer_u = 1000\n",
    "            buffer_r = 1000\n",
    "            \n",
    "    if 's1_sar' in name:\n",
    "        buffer_u = 2500\n",
    "        buffer_r = 2500\n",
    "                    \n",
    "    if name in ['ecmwf_weather',\n",
    "                'ecmwf_weather_q1', 'ecmwf_weather_q2', 'ecmwf_weather_q3', 'ecmwf_weather_q4']:\n",
    "        # 27km radius\n",
    "        buffer_u = 10000\n",
    "        buffer_r = 10000\n",
    "        \n",
    "    if name == 'viirs181920_750':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 750\n",
    "        buffer_r = 750\n",
    "        \n",
    "    if name == 'viirs181920_1120':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 1120\n",
    "        buffer_r = 1120\n",
    "                \n",
    "    if name == 'viirs181920_1250':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 1250\n",
    "        buffer_r = 1250\n",
    "        \n",
    "    if name == 'viirs181920_1500':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 1500\n",
    "        buffer_r = 1500\n",
    "        \n",
    "    if name == 'viirs181920_2000':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 2000\n",
    "        buffer_r = 2000\n",
    "        \n",
    "    if name == 'viirs181920_2500':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 2500\n",
    "        buffer_r = 2500\n",
    "        \n",
    "    if name == 'viirs181920_3360':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 3360\n",
    "        buffer_r = 3360\n",
    "        \n",
    "    if name == 'viirs181920_5000':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 5000\n",
    "        buffer_r = 5000\n",
    "\n",
    "    if name == 'viirs_750':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 750\n",
    "        buffer_r = 750\n",
    "            \n",
    "    if name == 'viirs_1120':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 1120\n",
    "        buffer_r = 1120\n",
    "            \n",
    "    if name == 'viirs_1250':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 1250\n",
    "        buffer_r = 1250\n",
    "        \n",
    "    if name == 'viirs_1500':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 1500\n",
    "        buffer_r = 1500\n",
    "            \n",
    "    if name == 'viirs_2000':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 2000\n",
    "        buffer_r = 2000\n",
    "        \n",
    "    if name == 'viirs_2500':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 2500\n",
    "        buffer_r = 2500\n",
    "        \n",
    "    if name == 'viirs_3360':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 3360\n",
    "        buffer_r = 3360\n",
    "        \n",
    "    if name == 'viirs_5000':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 5000\n",
    "        buffer_r = 5000\n",
    "        \n",
    "    if name == 'viirs_sdtime_2500':\n",
    "        sat = 'viirs_sdtime'\n",
    "        buffer_u = 2500\n",
    "        buffer_r = 2500\n",
    "        \n",
    "    if name == 'viirs_sdspace_2500':\n",
    "        sat = 'viirs_sdspace'\n",
    "        buffer_u = 2500\n",
    "        buffer_r = 2500\n",
    "        \n",
    "    if name == 'worldpop_750':\n",
    "        sat = 'worldpop'\n",
    "        buffer_u = 750\n",
    "        buffer_r = 750\n",
    "        \n",
    "    if name == 'worldpop_1000':\n",
    "        sat = 'worldpop'\n",
    "        buffer_u = 1000\n",
    "        buffer_r = 1000\n",
    "    \n",
    "    if name == 'worldpop_1500':\n",
    "        sat = 'worldpop'\n",
    "        buffer_u = 1500\n",
    "        buffer_r = 1500\n",
    "        \n",
    "    if name == 'worldpop_2000':\n",
    "        sat = 'worldpop'\n",
    "        buffer_u = 2000\n",
    "        buffer_r = 2000\n",
    "        \n",
    "    if name == 'worldpop_2500':\n",
    "        sat = 'worldpop'\n",
    "        buffer_u = 2500\n",
    "        buffer_r = 2500\n",
    "        \n",
    "    if name == 'worldpop_5000':\n",
    "        sat = 'worldpop'\n",
    "        buffer_u = 5000\n",
    "        buffer_r = 5000\n",
    "        \n",
    "    if name == 'worldpop_10000':\n",
    "        sat = 'worldpop'\n",
    "        buffer_u = 10000\n",
    "        buffer_r = 10000\n",
    "       \n",
    "    if name == 'worldpop2020_750':\n",
    "        sat = 'worldpop2020'\n",
    "        buffer_u = 750\n",
    "        buffer_r = 750\n",
    "    \n",
    "    if name == 'worldpop2020_1000':\n",
    "        sat = 'worldpop2020'\n",
    "        buffer_u = 1000\n",
    "        buffer_r = 1000\n",
    "    \n",
    "    if name == 'worldpop2020_1500':\n",
    "        sat = 'worldpop2020'\n",
    "        buffer_u = 1500\n",
    "        buffer_r = 1500\n",
    "        \n",
    "    if name == 'worldpop2020_2000':\n",
    "        sat = 'worldpop2020'\n",
    "        buffer_u = 2000\n",
    "        buffer_r = 2000\n",
    "        \n",
    "    if name == 'worldpop2020_2500':\n",
    "        sat = 'worldpop2020'\n",
    "        buffer_u = 2500\n",
    "        buffer_r = 2500\n",
    "        \n",
    "    if name == 'worldpop2020_5000':\n",
    "        sat = 'worldpop2020'\n",
    "        buffer_u = 5000\n",
    "        buffer_r = 5000\n",
    "        \n",
    "    if name == 'worldpop2020_10000':\n",
    "        sat = 'worldpop2020'\n",
    "        buffer_u = 10000\n",
    "        buffer_r = 10000\n",
    "        \n",
    "    survey_df_use = survey_df.copy()\n",
    "    \n",
    "    # Define Chunk Size ---------------------------------\n",
    "    CHUNK_SIZE = 5000\n",
    "    \n",
    "    if sat in ['NO2', 'uv_aer', 'CO', 'HCHO', 'ozone', 'SO2', 'CH4']:\n",
    "        CHUNK_SIZE = 1\n",
    "        \n",
    "    if sat in ['NO2', 'uv_aer', 'CO', 'HCHO', 'ozone', 'SO2', 'CH4']:\n",
    "        survey_df_use = survey_df_use[survey_df_use.most_recent_survey == True]\n",
    "        \n",
    "    ## Check to see if file exists    \n",
    "    if (how_check_processed == 'file'):\n",
    "        \n",
    "        survey_df_use['chunk_id'] = chunk_ids(survey_df_use.shape[0], CHUNK_SIZE)\n",
    "\n",
    "        # Loop over satellites ------------------------------\n",
    "\n",
    "        # LOOP OVER YEARS AND CHUNKS TO EXTRACT DATA \n",
    "        for year_i in survey_df_use['year'].unique():\n",
    "\n",
    "            survey_df_year = survey_df_use[survey_df_use['year'] == year_i]\n",
    "\n",
    "            # LOOP OVER CHUNKS\n",
    "            for chunk_id_i in survey_df_year['chunk_id'].unique():\n",
    "                survey_df_year_i = survey_df_year[survey_df_year['chunk_id'] == chunk_id_i]\n",
    "\n",
    "                # TODO: CHANGE BACK - changes to \"gee_small_\" for pollution stuff due to issues.\n",
    "                file_name_i = 'gee_' + name + '_ubuff' + str(buffer_u) + '_rbuff' + str(buffer_r) + '_' + str(year_i) + '_' + str(chunk_id_i)    \n",
    "                file_name_i_csv = file_name_i + '.csv'\n",
    "\n",
    "                # ONLY EXTRACT DATA IF NOT ALREADY EXTRACTED\n",
    "\n",
    "                ## Check against file name\n",
    "                if (file_name_i_csv not in files_extracted):\n",
    "\n",
    "                    \n",
    "                    task_i = extract_sat(survey_df = survey_df_year_i, \n",
    "                                         buffer_size_urban = buffer_u, \n",
    "                                         buffer_size_rural = buffer_r, \n",
    "                                         year = year_i,\n",
    "                                         satellite = sat, \n",
    "                                         survey_name = SURVEY_NAME,\n",
    "                                         file_name = file_name_i)\n",
    "\n",
    "                    tasks_all.append(task_i)\n",
    "                    \n",
    "    if (how_check_processed == 'data'):\n",
    "        \n",
    "        survey_df_use_copy = survey_df_use.copy()\n",
    "\n",
    "        ## Root name\n",
    "        fname_root = 'gee_' + sat + '_ubuff' + str(buffer_u) + '_rbuff' + str(buffer_r)\n",
    "\n",
    "        ## Make dataframe\n",
    "        all_filenames = [i for i in glob.glob('*.{}'.format('.csv'))]\n",
    "        sat_files     = glob.glob(OUT_PATH + '/' + fname_root + '*')\n",
    "        processed_df  = pd.concat([pd.read_csv(f) for f in sat_files])\n",
    "\n",
    "        ## Merge and subset to not processed\n",
    "        processed_df = processed_df[['uid', 'year']]\n",
    "        processed_df['already_scraped'] = 1\n",
    "\n",
    "        survey_df_use_copy = survey_df_use_copy.merge(processed_df, on=['uid', 'year'], how='left')\n",
    "\n",
    "        survey_df_ntprcsd = survey_df_use_copy[survey_df_use_copy.already_scraped.isnull()]\n",
    "        \n",
    "        print(survey_df_ntprcsd.shape[0])\n",
    "\n",
    "        if (survey_df_ntprcsd.shape[0] > 0):\n",
    "        \n",
    "            ## Add chunks\n",
    "            survey_df_ntprcsd['chunk_id'] = chunk_ids(survey_df_ntprcsd.shape[0], CHUNK_SIZE)\n",
    "\n",
    "            # Loop over satellites ------------------------------\n",
    "\n",
    "            # LOOP OVER YEARS AND CHUNKS TO EXTRACT DATA \n",
    "            for year_i in survey_df_ntprcsd['year'].unique():\n",
    "\n",
    "                survey_df_year = survey_df_ntprcsd[survey_df_ntprcsd['year'] == year_i]\n",
    "\n",
    "                # LOOP OVER CHUNKS\n",
    "                for chunk_id_i in survey_df_year['chunk_id'].unique():\n",
    "                    survey_df_year_i = survey_df_year[survey_df_year['chunk_id'] == chunk_id_i]\n",
    "\n",
    "                    # TODO: CHANGE BACK - changes to \"gee_small_\" for pollution stuff due to issues.\n",
    "                    now = datetime.now()\n",
    "                    dt_string = now.strftime(\"%d%m%Y%H%M%S\")\n",
    "\n",
    "                    file_name_i = 'gee_' + name + '_ubuff' + str(buffer_u) + '_rbuff' + str(buffer_r) + '_' + str(year_i) + '_' + str(chunk_id_i) + '_' + str(dt_string)    \n",
    "                    file_name_i_csv = file_name_i + '.csv'\n",
    "\n",
    "                    # Extract data\n",
    "                    \n",
    "                    task_i = extract_sat(survey_df = survey_df_year_i, \n",
    "                                         buffer_size_urban = buffer_u, \n",
    "                                         buffer_size_rural = buffer_r, \n",
    "                                         year = year_i,\n",
    "                                         satellite = sat, \n",
    "                                         survey_name = SURVEY_NAME,\n",
    "                                         file_name = file_name_i)\n",
    "\n",
    "                    tasks_all.append(task_i)\n",
    "                    \n",
    "                    time.sleep(1.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
