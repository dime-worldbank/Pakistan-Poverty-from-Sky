{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>To authorize access needed by Earth Engine, open the following\n",
       "        URL in a web browser and follow the instructions:</p>\n",
       "        <p><a href=https://accounts.google.com/o/oauth2/auth?client_id=517222506229-vsmmajv00ul0bs7p89v5m89qs8eb9359.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fearthengine+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code&code_challenge=cy4vvtmwcAwgXsTIPzJcF45V7isTw26zDxHIC8LRtHs&code_challenge_method=S256>https://accounts.google.com/o/oauth2/auth?client_id=517222506229-vsmmajv00ul0bs7p89v5m89qs8eb9359.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fearthengine+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code&code_challenge=cy4vvtmwcAwgXsTIPzJcF45V7isTw26zDxHIC8LRtHs&code_challenge_method=S256</a></p>\n",
       "        <p>The authorization workflow will generate a code, which you\n",
       "        should paste in the box below</p>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter verification code:  4/1AX4XfWjQfWjyrmBJBPhrItD5E1VX2tXmp5A4eZwVHUYG4I4j-dLWg8dsp70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully saved authorization token.\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "ee.Authenticate()\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import geetools\n",
    "from geetools import ui, cloud_mask\n",
    "import os, datetime\n",
    "import config as cf\n",
    "import pandas as pd\n",
    "import eeconvert\n",
    "import time\n",
    "import geopandas as gpd\n",
    "\n",
    "cloud_mask_landsatSR = cloud_mask.landsatSR()\n",
    "cloud_mask_sentinel2 = cloud_mask.sentinel2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "SURVEY_NAME = 'DHS'\n",
    "REEXTRACT_IF_FILE_EXISTS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gis.stackexchange.com/questions/257727/iterate-over-imagecollection-returning-pandas-dataframe-using-earth-engine-pyt\n",
    "def fc2df(fc):\n",
    "    # Convert a FeatureCollection into a pandas DataFrame\n",
    "    # Features is a list of dict with the output\n",
    "    features = fc.getInfo()['features']\n",
    "\n",
    "    dictarr = []\n",
    "\n",
    "    for f in features:\n",
    "        # Store all attributes in a dict\n",
    "        attr = f['properties']\n",
    "        # and treat geometry separately\n",
    "        attr['geometry'] = f['geometry']  # GeoJSON Feature!\n",
    "        # attr['geometrytype'] = f['geometry']['type']\n",
    "        dictarr.append(attr)\n",
    "\n",
    "    df = gpd.GeoDataFrame(dictarr)\n",
    "    # Convert GeoJSON features to shape\n",
    "    df = df.drop(columns=['geometry'])\n",
    "    return df\n",
    "\n",
    "def survey_to_fc_buffer(survey_df, buffer_size_urban, buffer_size_rural):\n",
    "    '''\n",
    "    Convert pandas dataframe of survey locations to a feature collection. \n",
    "    \n",
    "    Inputs:\n",
    "        survey_df: pandas dataframe of survey locations. Function assumes \n",
    "                   the dataframe contains (1) latitude, (2) longitude and\n",
    "                   (3) uid variables. Assumes coordinates in WGS84.\n",
    "    Returns:\n",
    "        (feature collection)\n",
    "    '''\n",
    "    \n",
    "    survey_fc_list = []\n",
    "    \n",
    "    n_rows = survey_df.shape[0]\n",
    "    for i in range(0, n_rows):\n",
    "        survey_df_i = survey_df.iloc[[i]]\n",
    "        \n",
    "        ur = survey_df_i['urban_rural'].iloc[0]\n",
    "        if ur == 'U':\n",
    "            buffer_size = buffer_size_urban\n",
    "        elif ur == 'R':\n",
    "            buffer_size = buffer_size_rural\n",
    "\n",
    "        f_i = ee.Feature(ee.Geometry.Point([survey_df_i['longitude'].iloc[0], \n",
    "                                            survey_df_i['latitude'].iloc[0]]), \n",
    "                         {'uid': survey_df_i['uid'].iloc[0],\n",
    "                          'year': str(survey_df_i['year'].iloc[0])})\n",
    "        \n",
    "        f_i = f_i.buffer(buffer_size)\n",
    "\n",
    "        survey_fc_list.append(f_i)\n",
    "        \n",
    "    survey_fc = ee.FeatureCollection(survey_fc_list)\n",
    "    \n",
    "    return survey_fc\n",
    "\n",
    "def extract_sat(survey_df, buffer_size_urban, buffer_size_rural, satellite, file_name, year, chunk, survey_name):\n",
    "    '''\n",
    "    Extract satellite imagery to locations \n",
    "    \n",
    "    Inputs:\n",
    "        survey_df: pandas dataframe of survey locations. Function assumes \n",
    "                   the dataframe contains (1) latitude, (2) longitude and\n",
    "                   (3) uid variables. Assumes coordinates in WGS84.\n",
    "    Returns:\n",
    "        (feature collection)\n",
    "    '''\n",
    "    \n",
    "    #print(survey_df.uid)\n",
    "    \n",
    "    year_start_sp5 = \"2018-01-01\"\n",
    "    year_end_sp5 = '2020-12-31'\n",
    "    \n",
    "    # Prep worldpop -----------------------------------------------\n",
    "    if satellite == 'worldpop':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 100 \n",
    "        \n",
    "        # Year\n",
    "        year_use = year\n",
    "        \n",
    "        year_plus = year_use\n",
    "        year_minus = year_use\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('WorldPop/GP/100m/pop')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .median()\n",
    "        \n",
    "        # After the reducer computers the sum, it names the value \"sum\", not population\n",
    "        BANDS = ['sum']\n",
    "        \n",
    "    # Prep worldpop_2020 ---------------------------------------------\n",
    "    if satellite == 'worldpop2020':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 100 \n",
    "        \n",
    "        # Year\n",
    "        year_use = '2020'\n",
    "        \n",
    "        year_plus = year_use\n",
    "        year_minus = year_use\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('WorldPop/GP/100m/pop')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .median()\n",
    "        \n",
    "        # After the reducer computers the sum, it names the value \"sum\", not population\n",
    "        BANDS = ['sum']\n",
    "    \n",
    "    # Prep l7 ---------------------------------------------------\n",
    "    if satellite == 'l7':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 100 # ok to upscale\n",
    "        \n",
    "        # Year\n",
    "        year_use = year\n",
    "        \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('LANDSAT/LC07/C01/T1_SR')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .map(cloud_mask_landsatSR)\\\n",
    "            .median()\\\n",
    "            .multiply(0.0001)\n",
    "\n",
    "        ndvi = image.normalizedDifference(['B4', 'B3']).rename('NDVI');\n",
    "        image = image.addBands(ndvi)\n",
    "        \n",
    "        BANDS = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7']\n",
    "        \n",
    "    # Sentinel-5P OFFL AER AI: Offline UV Aerosol Index  -------------------\n",
    "    if satellite == 'uv_aer':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 1000 # ok to upscale\n",
    "        \n",
    "        # Starts in 2018; take all years\n",
    "        image = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_AER_AI\")\\\n",
    "            .filterDate(year_start_sp5, year_end_sp5)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['absorbing_aerosol_index']\n",
    "        \n",
    "    # Sentinel-5P OFFL CO: Offline Carbon Monoxide  -------------------\n",
    "    if satellite == 'CO':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 1000 # ok to upscale\n",
    "        \n",
    "        # Starts in 2018; take all years\n",
    "        image = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_CO\")\\\n",
    "            .filterDate(year_start_sp5, year_end_sp5)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['CO_column_number_density', 'H2O_column_number_density']\n",
    "        \n",
    "    # Sentinel-5P OFFL HCHO: Offline Formaldehyde  -------------------\n",
    "    if satellite == 'HCHO':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 1000 # ok to upscale\n",
    "        \n",
    "        # Starts in 2018; take all years\n",
    "        image = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_HCHO\")\\\n",
    "            .filterDate(year_start_sp5, year_end_sp5)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['tropospheric_HCHO_column_number_density', 'tropospheric_HCHO_column_number_density_amf']\n",
    "        \n",
    "    # Sentinel-5P Nitrogen Dioxide  -----------------------------\n",
    "    if satellite == 'NO2':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 1000 # ok to upscale\n",
    "        \n",
    "        # Starts in 2018; take all years\n",
    "        image = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_NO2\")\\\n",
    "            .filterDate(year_start_sp5, year_end_sp5)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['NO2_column_number_density', 'tropospheric_NO2_column_number_density',\\\n",
    "                 'stratospheric_NO2_column_number_density', 'NO2_slant_column_number_density']\n",
    "        \n",
    "    # Sentinel-5P OFFL O3: Offline Ozone  -------------------\n",
    "    if satellite == 'ozone':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 1000 # ok to upscale\n",
    "        \n",
    "        # Starts in 2018; take all years\n",
    "        image = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_O3\")\\\n",
    "            .filterDate(year_start_sp5, year_end_sp5)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['O3_column_number_density', 'O3_effective_temperature']\n",
    "        \n",
    "    # Sentinel-5P OFFL SO2: Offline Sulphur Dioxide  -------------------\n",
    "    if satellite == 'SO2':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 1000 # ok to upscale\n",
    "        \n",
    "        # Starts in 2018; take all years\n",
    "        image = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_SO2\")\\\n",
    "            .filterDate(year_start_sp5, year_end_sp5)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['SO2_column_number_density', 'SO2_column_number_density_amf', 'SO2_slant_column_number_density']\n",
    "        \n",
    "    # Sentinel-5P OFFL CH4: Offline Methane  -------------------\n",
    "    if satellite == 'CH4':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 1000 # ok to upscale\n",
    "        \n",
    "        # Starts in 2018; take all years\n",
    "        image = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_CH4\")\\\n",
    "            .filterDate(year_start_sp5, year_end_sp5)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['CH4_column_volume_mixing_ratio_dry_air']\n",
    "        \n",
    "    # CSP gHM: Global Human Modification ---------------------------------\n",
    "    if satellite == 'GlobalHumanModification':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 1000 # ok to upscale\n",
    "                \n",
    "        image = ee.ImageCollection(\"CSP/HM/GlobalHumanModification\")\\\n",
    "            .median()\n",
    "        \n",
    "        # Original name is \"gHM\", but because only one value, it takes the\n",
    "        # name of the reducer; we use mean\n",
    "        BANDS = ['mean']\n",
    "        \n",
    "    # WorldClim BIO Variables V1 ---------------------------------\n",
    "    if satellite == 'worldclim_bio':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 1000 # ok to upscale\n",
    "                \n",
    "        image = ee.Image('WORLDCLIM/V1/BIO')\n",
    "        \n",
    "        BANDS = ['bio01', 'bio02', 'bio03', 'bio04', 'bio05', 'bio06', 'bio07', 'bio08', 'bio09', 'bio10',\\\n",
    "                 'bio11', 'bio12', 'bio13', 'bio14', 'bio15', 'bio16', 'bio17', 'bio18', 'bio19']\n",
    "        \n",
    "    # Elevation - SRTM ------------------------------------------\n",
    "    if satellite == 'elevation':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 1000 # ok to upscale\n",
    "                \n",
    "        image = ee.Image('USGS/SRTMGL1_003') # CGIAR/SRTM90_V4\n",
    "        \n",
    "        # elevation?\n",
    "        BANDS = ['mean']\n",
    "        \n",
    "    # Elevation - SRTM ------------------------------------------\n",
    "    if satellite == 'slope':\n",
    "        # https://developers.google.com/earth-engine/datasets/catalog/CGIAR_SRTM90_V4#description\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 500 # ok to upscale\n",
    "                \n",
    "        image_raw = ee.Image('USGS/SRTMGL1_003') # CGIAR/SRTM90_V4\n",
    "        image_elev = image_raw.select('elevation')\n",
    "        image = ee.Terrain.slope(image_elev)\n",
    "                \n",
    "        # mean?\n",
    "        BANDS = ['mean']\n",
    "        \n",
    "    # Prep l8 ---------------------------------------------------\n",
    "    if satellite == 'l8':\n",
    "        \n",
    "        SCALE = 100 # ok to upscale\n",
    "        #SCALE = 2000\n",
    "        \n",
    "        # Year\n",
    "        # landsat 8 starts in April 2013; if year is less than\n",
    "        # 2014, use 2014 as year (to ensure have year before and after)\n",
    "        if year < 2014:\n",
    "            year_use = 2014\n",
    "        else:\n",
    "            year_use = year\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .map(cloud_mask_landsatSR)\\\n",
    "            .median()\\\n",
    "            .multiply(0.0001)\n",
    "\n",
    "        # https://www.linkedin.com/pulse/ndvi-ndbi-ndwi-calculation-using-landsat-7-8-tek-bahadur-kshetri\n",
    "        ndvi = image.normalizedDifference(['B5', 'B4']).rename('NDVI');\n",
    "        ndbi = image.normalizedDifference(['B6', 'B5']).rename('NDBI');\n",
    "        image = image.addBands(ndvi)\n",
    "        image = image.addBands(ndbi)\n",
    "        \n",
    "        bu = image.select('NDBI').subtract(image.select('NDVI')).rename('BU')\n",
    "        image = image.addBands(bu)\n",
    "        \n",
    "        BANDS = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B10', 'B11', 'NDVI', 'NDBI', 'BU']\n",
    "        #BANDS = ['NDVI']\n",
    "        \n",
    "    # Prep s2 ---------------------------------------------------\n",
    "    if satellite == 's2':\n",
    "        \n",
    "        SCALE = 100 # ok to upscale\n",
    "        \n",
    "        # Year\n",
    "        # sentinel starts in March 2017; juse use 2018\n",
    "        year_use = 2018\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-12-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('COPERNICUS/S2_SR')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .map(cloud_mask_sentinel2)\\\n",
    "            .median()\\\n",
    "            .multiply(0.0001)\n",
    "        \n",
    "        ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI');\n",
    "        image = image.addBands(ndvi)\n",
    "        \n",
    "        BANDS = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12', 'AOT', 'NDVI']\n",
    "\n",
    "        image = image.select(BANDS) \n",
    "        \n",
    "    # Prep drought ---------------------------------------------------\n",
    "    if satellite == 'gridmet_drought':\n",
    "        \n",
    "        SCALE = 5000 \n",
    "\n",
    "        year_minus_str = str(year) + '-01-01'\n",
    "        year_plus_str = str(year) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection(\"GRIDMET/DROUGHT\")\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['pdsi', 'z', 'eddi1y', 'eddi2y', 'eddi5y']\n",
    "    \n",
    "    \n",
    "    # Prep drought ---------------------------------------------------\n",
    "    if satellite == 'ecmwf_weather':\n",
    "        \n",
    "        SCALE = 1000 \n",
    "\n",
    "        year_minus_str = str(year) + '-01-01'\n",
    "        year_plus_str = str(year) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection(\"ECMWF/ERA5/DAILY\")\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .mean()\n",
    "        \n",
    "        BANDS = ['mean_2m_air_temperature', \n",
    "                 'minimum_2m_air_temperature', \n",
    "                 'maximum_2m_air_temperature',\n",
    "                 'total_precipitation']\n",
    "        \n",
    "    # Prep drought ---------------------------------------------------\n",
    "    if satellite == 'ecmwf_weather_q1':\n",
    "        \n",
    "        SCALE = 1000 \n",
    "\n",
    "        year_minus_str = str(year) + '-01-01'\n",
    "        year_plus_str = str(year) + '-03-31'\n",
    "        \n",
    "        image = ee.ImageCollection(\"ECMWF/ERA5/DAILY\")\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .mean()\n",
    "        \n",
    "        BANDS = ['mean_2m_air_temperature', \n",
    "                 'minimum_2m_air_temperature', \n",
    "                 'maximum_2m_air_temperature',\n",
    "                 'total_precipitation']\n",
    "        \n",
    "    # Prep drought ---------------------------------------------------\n",
    "    if satellite == 'ecmwf_weather_q2':\n",
    "        \n",
    "        SCALE = 1000 \n",
    "\n",
    "        year_minus_str = str(year) + '-04-01'\n",
    "        year_plus_str = str(year) + '-06-30'\n",
    "        \n",
    "        image = ee.ImageCollection(\"ECMWF/ERA5/DAILY\")\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .mean()\n",
    "        \n",
    "        BANDS = ['mean_2m_air_temperature', \n",
    "                 'minimum_2m_air_temperature', \n",
    "                 'maximum_2m_air_temperature',\n",
    "                 'total_precipitation']\n",
    "        \n",
    "    # Prep drought ---------------------------------------------------\n",
    "    if satellite == 'ecmwf_weather_q3':\n",
    "        \n",
    "        SCALE = 1000 \n",
    "\n",
    "        year_minus_str = str(year) + '-07-01'\n",
    "        year_plus_str = str(year) + '-09-30'\n",
    "        \n",
    "        image = ee.ImageCollection(\"ECMWF/ERA5/DAILY\")\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .mean()\n",
    "        \n",
    "        BANDS = ['mean_2m_air_temperature', \n",
    "                 'minimum_2m_air_temperature', \n",
    "                 'maximum_2m_air_temperature',\n",
    "                 'total_precipitation']\n",
    "        \n",
    "    # Prep drought ---------------------------------------------------\n",
    "    if satellite == 'ecmwf_weather_q4':\n",
    "        \n",
    "        SCALE = 1000 \n",
    "\n",
    "        year_minus_str = str(year) + '-10-01'\n",
    "        year_plus_str = str(year) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection(\"ECMWF/ERA5/DAILY\")\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .mean()\n",
    "        \n",
    "        BANDS = ['mean_2m_air_temperature', \n",
    "                 'minimum_2m_air_temperature', \n",
    "                 'maximum_2m_air_temperature',\n",
    "                 'total_precipitation']\n",
    "    \n",
    "    # Prep viirs ---------------------------------------------------\n",
    "    if satellite == 'viirs':\n",
    "        \n",
    "        SCALE = 500 \n",
    "        \n",
    "        # Year\n",
    "        # VIIRS starts in April 2012; if year is less than\n",
    "        # 2013, use 2013 as year (to ensure have year before and after)\n",
    "        if year < 2013:\n",
    "            year_use = 2013\n",
    "        else:\n",
    "            year_use = year\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('NOAA/VIIRS/DNB/MONTHLY_V1/VCMCFG')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['avg_rad']\n",
    "        \n",
    "    # Prep viirs181920 ---------------------------------------------------\n",
    "    if satellite == 'viirs181920':\n",
    "        \n",
    "        SCALE = 500 \n",
    "        \n",
    "        # Year\n",
    "        # VIIRS starts in April 2012; if year is less than\n",
    "        # 2013, use 2013 as year (to ensure have year before and after)\n",
    "        year_use = 2019\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('NOAA/VIIRS/DNB/MONTHLY_V1/VCMCFG')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['avg_rad']\n",
    "        \n",
    "    # Prep DMSP ---------------------------------------------------\n",
    "    if satellite == 'dmsp':\n",
    "        \n",
    "        SCALE = 1000 \n",
    "        \n",
    "        # Year\n",
    "        # DMSP-OLS starts in 2013; if year is more than\n",
    "        # 2012, use 2012 as year (to ensure have year before and after)\n",
    "        if year > 2012:\n",
    "            year_use = 2012\n",
    "        else:\n",
    "            year_use = year\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('NOAA/DMSP-OLS/NIGHTTIME_LIGHTS')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['stable_lights', 'avg_lights_x_pct']\n",
    "    \n",
    "    # Prep Survey ---------------------------------------------------\n",
    "    survey_fc = survey_to_fc_buffer(survey_df, buffer_size_urban, buffer_size_rural)\n",
    "    \n",
    "    # Extract Values ---------------------------------------------------\n",
    "    if (satellite == 'worldpop') | (satellite == 'worldpop2020'):\n",
    "        vals = image.reduceRegions(collection = survey_fc,\n",
    "                                   reducer = ee.Reducer.sum(),\n",
    "                                   scale = SCALE,\n",
    "                                   tileScale = 8)\n",
    "    else:\n",
    "        vals = image.reduceRegions(collection = survey_fc,\n",
    "                                   reducer = ee.Reducer.mean(),\n",
    "                                   scale = SCALE,\n",
    "                                   tileScale = 8)\n",
    "\n",
    "    # OLD =============\n",
    "    # Survey dataset that only contains the uid variable\n",
    "    #survey_df = survey_df[['uid']]\n",
    "            \n",
    "    #for band_i in BANDS:\n",
    "    #    survey_df[satellite + '_' + band_i] = vals.aggregate_array(band_i).getInfo()\n",
    "        \n",
    "    # NEW =============\n",
    "    #df_out = fc2df(vals)\n",
    "    #print(df_out)\n",
    "    #df_out = pd.DataFrame()\n",
    "    \n",
    "    bands_to_export = BANDS.copy()\n",
    "    bands_to_export.append('uid')\n",
    "    bands_to_export.append('year')\n",
    "    #print(bands_to_export)\n",
    "    \n",
    "    task = ee.batch.Export.table.toDrive(collection=vals, \n",
    "                                         folder='satellite_data_from_gee_' + survey_name.lower(), \n",
    "                                         description=file_name + \"_ubuff\" + str(buffer_size_urban) + '_rbuff' + str(buffer_size_rural) + \"_\" + str(year) + '_' + str(chunk), \n",
    "                                         fileFormat='CSV',\n",
    "                                         selectors = bands_to_export)\n",
    "    # selectors=props\n",
    "    task.start()\n",
    "    #ee.batch.data.startProcessing(mytask.id, mytask.config)\n",
    "    \n",
    "    if False:\n",
    "        time_elapsed = 0\n",
    "        while task.active():\n",
    "            if((time_elapsed % 60) == 0):\n",
    "                print('Polling for task (id: {}).'.format(task.id))\n",
    "            time.sleep(5)\n",
    "            time_elapsed = time_elapsed + 5\n",
    "        \n",
    "    return task\n",
    "\n",
    "def extract_satellite_in_chunks(survey_df, buffer_size_urban, buffer_size_rural, satellite, file_name, year, survey_name):\n",
    "    \n",
    "    vals_df_list = []\n",
    "    \n",
    "    for chunk_i in list(np.unique(survey_df.chunk_id)):\n",
    "        #print(chunk_i)\n",
    "        #time.sleep(5)\n",
    "\n",
    "        survey_df_i = survey_df[survey_df['chunk_id'] == chunk_i]\n",
    "        #print(survey_df_i.shape)\n",
    "        vals_i_df = extract_sat(survey_df_i, buffer_size_urban, buffer_size_rural, satellite, file_name, year, chunk_i, survey_name)\n",
    "\n",
    "        vals_df_list.append(vals_i_df)\n",
    "\n",
    "    #vals_df = pd.concat(vals_df_list)\n",
    "    \n",
    "    return vals_df_list\n",
    "\n",
    "def extract_satellite_by_year(survey_df, buffer_size_urban, buffer_size_rural, satellite, file_name, survey_name):\n",
    "    \n",
    "    vals_df_list = []\n",
    "    \n",
    "    for year_i in list(np.unique(survey_df.year)):\n",
    "        #print(year_i)\n",
    "        #time.sleep(5)\n",
    "\n",
    "        survey_df_i = survey_df[survey_df['year'] == year_i]\n",
    "        vals_i_df = extract_satellite_in_chunks(survey_df_i, buffer_size_urban, buffer_size_rural, satellite, file_name, year_i, survey_name)\n",
    "\n",
    "        vals_df_list.append(vals_i_df)\n",
    "\n",
    "    #vals_df = pd.concat(vals_df_list)\n",
    "    \n",
    "    return vals_df_list\n",
    "\n",
    "def chunk_ids(total_length, chunk_size):\n",
    "    n_numbers = np.ceil(total_length / chunk_size)\n",
    "    n_numbers = int(n_numbers)\n",
    "    \n",
    "    chunk_ids = list(range(0,n_numbers)) * chunk_size\n",
    "    chunk_ids.sort()\n",
    "    chunk_ids = chunk_ids[:total_length]\n",
    "    \n",
    "    return chunk_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load/Prep Survey Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robmarty/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3044: DtypeWarning: Columns (46,50,53,56,57,58,60) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "survey_df = pd.read_csv(os.path.join(cf.DROPBOX_DIRECTORY, 'Data', SURVEY_NAME, 'FinalData', 'Individual Datasets', 'survey_socioeconomic.csv'))\n",
    "survey_df = survey_df[['uid', 'year', 'urban_rural', 'latitude', 'longitude']]\n",
    "survey_df = survey_df.sort_values('year')\n",
    "#survey_df = survey_df[survey_df.uid != 'IA201400180012']\n",
    "\n",
    "CHUNK_SIZE = 5000\n",
    "survey_years = list(survey_df.year.unique())\n",
    "survey_df['chunk_id'] = chunk_ids(survey_df.shape[0], CHUNK_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  uid  year urban_rural  latitude   longitude  chunk_id\n",
      "46728  ID200372023024  2002           R -0.817255  122.693646         0\n",
      "46637  ID200314031501  2002           U -0.528189  103.394810         0\n",
      "46636  ID200314031026  2002           R -0.125667  103.314689         0\n",
      "46635  ID200314031018  2002           R -0.645283  102.873035         0\n",
      "46634  ID200314011002  2002           R -0.840921  102.749269         0\n",
      "(63374, 6)\n"
     ]
    }
   ],
   "source": [
    "print(survey_df.head())\n",
    "print(survey_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If re-extract, delete existing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting existing files from Google Drive\n"
     ]
    }
   ],
   "source": [
    "if REEXTRACT_IF_FILE_EXISTS:\n",
    "    print(\"Deleting existing files from Google Drive\")\n",
    "\n",
    "    ## Path with files\n",
    "    OUT_PATH = os.path.join(cf.GOOGLEDRIVE_DIRECTORY, \n",
    "                            'Data', \n",
    "                             SURVEY_NAME, \n",
    "                             'FinalData', \n",
    "                             'Individual Datasets',\n",
    "                             'satellite_data_from_gee_' + SURVEY_NAME.lower())\n",
    "\n",
    "    ## Grab csv files\n",
    "    files_to_rm = [x for x in os.listdir(OUT_PATH) if '.csv' in x]\n",
    "\n",
    "    ## Delete files\n",
    "    for file_i in files_to_rm:\n",
    "\n",
    "        path_i = os.path.join(OUT_PATH, file_i)\n",
    "        os.remove(path_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of files already extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['viirs_5000_ubuff5000_rbuff5000',\n",
       " 'worldpop_5000_ubuff5000_rbuff5000',\n",
       " 'viirs_2000_ubuff2000_rbuff2000',\n",
       " 'worldpop_2000_ubuff2000_rbuff2000',\n",
       " 'viirs_ubuff5000_rbuff5000',\n",
       " 'worldpop2020_5000_ubuff5000_rbuff5000',\n",
       " '.DS_Store',\n",
       " 'worldpop2020_2000_ubuff2000_rbuff2000',\n",
       " 'viirs_ubuff2000_rbuff2000',\n",
       " 'ecmwf_weather_ubuff10000_rbuff10000',\n",
       " 'ecmwf_weather_q2_ubuff10000_rbuff10000',\n",
       " 'worldpop_ubuff10000_rbuff10000',\n",
       " 'NO2_ubuff2500_rbuff2500',\n",
       " 'uv_aer_ubuff2500_rbuff2500',\n",
       " 'worldpop_10000_ubuff10000_rbuff10000',\n",
       " 'HCHO_ubuff2500_rbuff2500',\n",
       " 'l8_ubuff2500_rbuff2500',\n",
       " 'CH4_ubuff2500_rbuff2500',\n",
       " 'ecmwf_weather_q1_ubuff10000_rbuff10000',\n",
       " 'elevation_ubuff5000_rbuff5000',\n",
       " 'SO2_ubuff2500_rbuff2500',\n",
       " 'slope_ubuff5000_rbuff5000',\n",
       " 'viirs_ubuff2500_rbuff2500',\n",
       " 'GlobalHumanModification_ubuff10000_rbuff10000',\n",
       " 'ecmwf_weather_q3_ubuff10000_rbuff10000',\n",
       " '_archive',\n",
       " 'ozone_ubuff2500_rbuff2500',\n",
       " 'CO_ubuff2500_rbuff2500',\n",
       " 'viirs_1250_ubuff1250_rbuff1250',\n",
       " 'viirs_2500_ubuff2500_rbuff2500',\n",
       " 'worldpop2020_10000_ubuff10000_rbuff10000',\n",
       " 'ecmwf_weather_q4_ubuff10000_rbuff10000']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaned files\n",
    "DB_DATA_PATH = os.path.join(cf.DROPBOX_DIRECTORY, 'Data', \n",
    "                            SURVEY_NAME, 'FinalData', \n",
    "                            'Individual Datasets', 'satellite_data_from_gee')\n",
    "\n",
    "\n",
    "files_already_extracted = [x.replace('.Rds', '') for x in os.listdir(DB_DATA_PATH)]\n",
    "files_already_extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "viirs181920_1250\n",
      "viirs181920_1250_ubuff1250_rbuff1250\n",
      "viirs181920_2000\n",
      "viirs181920_2000_ubuff2000_rbuff2000\n",
      "viirs181920_2500\n",
      "viirs181920_2500_ubuff2500_rbuff2500\n",
      "viirs181920_5000\n",
      "viirs181920_5000_ubuff5000_rbuff5000\n",
      "worldpop2020_2000\n",
      "worldpop2020_2000_ubuff2000_rbuff2000\n",
      "worldpop2020_5000\n",
      "worldpop2020_5000_ubuff5000_rbuff5000\n",
      "worldpop2020_10000\n",
      "worldpop2020_10000_ubuff10000_rbuff10000\n"
     ]
    }
   ],
   "source": [
    "to_extract = ['elevation', \n",
    "              'slope',\n",
    "              'viirs_1250',\n",
    "              'viirs_2000',\n",
    "              'viirs_2500',\n",
    "              'viirs_5000',\n",
    "              'viirs181920_1250',\n",
    "              'viirs181920_2000',\n",
    "              'viirs181920_2500',\n",
    "              'viirs181920_5000',\n",
    "              'GlobalHumanModification',\n",
    "              'worldpop_2000',\n",
    "              'worldpop_5000',\n",
    "              'worldpop_10000',\n",
    "              'worldpop2020_2000',\n",
    "              'worldpop2020_5000',\n",
    "              'worldpop2020_10000',\n",
    "              'l8',\n",
    "              'ecmwf_weather',\n",
    "              'ecmwf_weather_q1',\n",
    "              'ecmwf_weather_q2',\n",
    "              'ecmwf_weather_q3',\n",
    "              'ecmwf_weather_q4',\n",
    "              'NO2', 'uv_aer', 'CO', 'HCHO', 'ozone', 'SO2', 'CH4']\n",
    "\n",
    "to_extract = ['viirs181920_1250',\n",
    "              'viirs181920_2000',\n",
    "              'viirs181920_2500',\n",
    "              'viirs181920_5000',\n",
    "              'worldpop2020_2000',\n",
    "              'worldpop2020_5000',\n",
    "              'worldpop2020_10000']\n",
    "\n",
    "tasks_all = []\n",
    "for name in to_extract:\n",
    "    print(name)\n",
    "        \n",
    "    sat = name\n",
    "        \n",
    "    if name in ['NO2', 'uv_aer', 'CO', 'HCHO', 'ozone', 'SO2', 'CH4', 'l8']:\n",
    "        buffer_u = 2500\n",
    "        buffer_r = 2500\n",
    "    \n",
    "    if name in ['elevation', 'slope']:\n",
    "        buffer_u = 5000\n",
    "        buffer_r = 5000\n",
    "        \n",
    "    if name in ['GlobalHumanModification', \n",
    "               'ecmwf_weather',\n",
    "               'ecmwf_weather_q1', 'ecmwf_weather_q2', 'ecmwf_weather_q3', 'ecmwf_weather_q4']:\n",
    "        buffer_u = 10000\n",
    "        buffer_r = 10000\n",
    "                \n",
    "    if name == 'viirs181920_1250':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 1250\n",
    "        buffer_r = 1250\n",
    "        \n",
    "    if name == 'viirs181920_2000':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 2000\n",
    "        buffer_r = 2000\n",
    "        \n",
    "    if name == 'viirs181920_2500':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 2500\n",
    "        buffer_r = 2500\n",
    "        \n",
    "    if name == 'viirs181920_5000':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 5000\n",
    "        buffer_r = 5000\n",
    "            \n",
    "    if name == 'viirs_1250':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 1250\n",
    "        buffer_r = 1250\n",
    "            \n",
    "    if name == 'viirs_2000':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 2000\n",
    "        buffer_r = 2000\n",
    "        \n",
    "    if name == 'viirs_2500':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 2500\n",
    "        buffer_r = 2500\n",
    "        \n",
    "    if name == 'viirs_5000':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 5000\n",
    "        buffer_r = 5000\n",
    "        \n",
    "    if name == 'worldpop_2000':\n",
    "        sat = 'worldpop'\n",
    "        buffer_u = 2000\n",
    "        buffer_r = 2000\n",
    "        \n",
    "    if name == 'worldpop_5000':\n",
    "        sat = 'worldpop'\n",
    "        buffer_u = 5000\n",
    "        buffer_r = 5000\n",
    "        \n",
    "    if name == 'worldpop_10000':\n",
    "        sat = 'worldpop'\n",
    "        buffer_u = 10000\n",
    "        buffer_r = 10000\n",
    "        \n",
    "    if name == 'worldpop2020_2000':\n",
    "        sat = 'worldpop2020'\n",
    "        buffer_u = 2000\n",
    "        buffer_r = 2000\n",
    "        \n",
    "    if name == 'worldpop2020_5000':\n",
    "        sat = 'worldpop2020'\n",
    "        buffer_u = 5000\n",
    "        buffer_r = 5000\n",
    "        \n",
    "    if name == 'worldpop2020_10000':\n",
    "        sat = 'worldpop2020'\n",
    "        buffer_u = 10000\n",
    "        buffer_r = 10000\n",
    "                \n",
    "    file_root = name + '_ubuff' + str(buffer_u) + '_rbuff' + str(buffer_r)\n",
    "        \n",
    "    # Check if should extract data\n",
    "    if (file_root not in files_already_extracted) | REEXTRACT_IF_FILE_EXISTS:\n",
    "        print(file_root)\n",
    "        \n",
    "        tasks_i = extract_satellite_by_year(survey_df, buffer_u, buffer_r, sat, name, SURVEY_NAME)\n",
    "        tasks_all.append(tasks_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Failed Tasks\n",
    "for task_list in tasks_all:\n",
    "    for task_i in task_list:\n",
    "        \n",
    "        task_i_status = task_i[0].status()\n",
    "        if task_i_status['state'] == 'FAILED':\n",
    "            print(task_i[0].status())\n",
    "            print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tasks not started\n",
    "for task_list in tasks_all:\n",
    "    for task_i in task_list:\n",
    "        \n",
    "        task_i_status = task_i[0].status()\n",
    "        if task_i_status['state'] == 'READY':\n",
    "            print(task_i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Task CW3RRN3SATAYITVNY5NULUB6 EXPORT_FEATURES: viirs181920_1250_ubuff1250_rbuff1250_2002_0 (UNSUBMITTED)>\n",
      "<Task QKTMUCJVNWL66UZ7NTQMLAJI EXPORT_FEATURES: viirs181920_1250_ubuff1250_rbuff1250_2003_0 (UNSUBMITTED)>\n",
      "<Task 6CK2DQHVRK6HJO5URXKODKO3 EXPORT_FEATURES: viirs181920_1250_ubuff1250_rbuff1250_2005_0 (UNSUBMITTED)>\n",
      "<Task VURTTLZH7TV6UZ7XCABF5YQ7 EXPORT_FEATURES: viirs181920_1250_ubuff1250_rbuff1250_2006_0 (UNSUBMITTED)>\n",
      "<Task L2IVVYJZNN7VIJ65KR54UDYK EXPORT_FEATURES: viirs181920_1250_ubuff1250_rbuff1250_2008_0 (UNSUBMITTED)>\n",
      "<Task 7Y3U3NZHTLJDTOXRD2MP7X37 EXPORT_FEATURES: viirs181920_1250_ubuff1250_rbuff1250_2009_0 (UNSUBMITTED)>\n",
      "<Task B5QP4D7WUFAD7E5KV2HII5KJ EXPORT_FEATURES: viirs181920_1250_ubuff1250_rbuff1250_2010_0 (UNSUBMITTED)>\n",
      "<Task UY3XWHISNVK47HIE5E4PKKDI EXPORT_FEATURES: viirs181920_1250_ubuff1250_rbuff1250_2011_1 (UNSUBMITTED)>\n",
      "<Task SW5RR2RCTLFWIRJNWFCUYMVC EXPORT_FEATURES: viirs181920_1250_ubuff1250_rbuff1250_2012_2 (UNSUBMITTED)>\n",
      "<Task IWJNTKHDQIVB3CNXWPUJTUCR EXPORT_FEATURES: viirs181920_1250_ubuff1250_rbuff1250_2013_2 (UNSUBMITTED)>\n",
      "<Task GJJWVE4TFMG55VW7OM7VDGP4 EXPORT_FEATURES: viirs181920_1250_ubuff1250_rbuff1250_2014_2 (UNSUBMITTED)>\n",
      "<Task DACPP4IIRJ5L6BCRLMFC6PMY EXPORT_FEATURES: viirs181920_1250_ubuff1250_rbuff1250_2015_3 (UNSUBMITTED)>\n",
      "<Task JP2XPISYR5AQM2MXH44KAMV2 EXPORT_FEATURES: viirs181920_1250_ubuff1250_rbuff1250_2016_9 (UNSUBMITTED)>\n",
      "<Task LTW4NJ3PJNRL7LO5EZZHER57 EXPORT_FEATURES: viirs181920_1250_ubuff1250_rbuff1250_2017_10 (UNSUBMITTED)>\n",
      "<Task 6O3M7DSLPHN4DX67W7YPKVSW EXPORT_FEATURES: viirs181920_1250_ubuff1250_rbuff1250_2018_11 (UNSUBMITTED)>\n",
      "<Task KX5XAALCCF3TG4K44FPZ2O4B EXPORT_FEATURES: viirs181920_1250_ubuff1250_rbuff1250_2019_12 (UNSUBMITTED)>\n",
      "<Task X7ACBDWJAPCJYCUURC2EKCLT EXPORT_FEATURES: viirs181920_2000_ubuff2000_rbuff2000_2002_0 (UNSUBMITTED)>\n",
      "<Task BU3TXQUFD2QSG5BA5VMP3RK7 EXPORT_FEATURES: viirs181920_2000_ubuff2000_rbuff2000_2003_0 (UNSUBMITTED)>\n",
      "<Task USJ42BPSKKF7DSRKNIUS2H4T EXPORT_FEATURES: viirs181920_2000_ubuff2000_rbuff2000_2005_0 (UNSUBMITTED)>\n",
      "<Task XB6QORDD2ZS5AJSIZIKE444A EXPORT_FEATURES: viirs181920_2000_ubuff2000_rbuff2000_2006_0 (UNSUBMITTED)>\n",
      "<Task M24IDUEV34NMSEAAT273IXEW EXPORT_FEATURES: viirs181920_2000_ubuff2000_rbuff2000_2008_0 (UNSUBMITTED)>\n",
      "<Task B5CLB3BHXJMIDIURFTC3XGLW EXPORT_FEATURES: viirs181920_2000_ubuff2000_rbuff2000_2009_0 (UNSUBMITTED)>\n",
      "<Task 45ZMSPAJ3V6BYSIEGO2XWBAY EXPORT_FEATURES: viirs181920_2000_ubuff2000_rbuff2000_2010_0 (UNSUBMITTED)>\n",
      "<Task ACKAG5BVH6IBCH5NAVCR6DVV EXPORT_FEATURES: viirs181920_2000_ubuff2000_rbuff2000_2011_1 (UNSUBMITTED)>\n",
      "<Task TW2YGHZ4S5I2HAOVZ7G4EYTZ EXPORT_FEATURES: viirs181920_2000_ubuff2000_rbuff2000_2012_2 (UNSUBMITTED)>\n",
      "<Task TI56XX7DQZEDHN5WNRYXLSHO EXPORT_FEATURES: viirs181920_2000_ubuff2000_rbuff2000_2013_2 (UNSUBMITTED)>\n",
      "<Task 3AR474LMMO57ACNUI5DUHLXV EXPORT_FEATURES: viirs181920_2000_ubuff2000_rbuff2000_2014_2 (UNSUBMITTED)>\n",
      "<Task VS3IMEB2LJ3PQFDBLVTY5ZF5 EXPORT_FEATURES: viirs181920_2000_ubuff2000_rbuff2000_2015_3 (UNSUBMITTED)>\n",
      "<Task 5QB2M7SQ3SKKEGS7R4IOIMZ3 EXPORT_FEATURES: viirs181920_2000_ubuff2000_rbuff2000_2016_9 (UNSUBMITTED)>\n",
      "<Task JHY4ZGTZKV24NLG5HEGYJ4WM EXPORT_FEATURES: viirs181920_2000_ubuff2000_rbuff2000_2017_10 (UNSUBMITTED)>\n",
      "<Task WA7W4NLBE7NAPAIZF5SLLTSQ EXPORT_FEATURES: viirs181920_2000_ubuff2000_rbuff2000_2018_11 (UNSUBMITTED)>\n",
      "<Task SV65MUFENRSASREMEYDDZFZE EXPORT_FEATURES: viirs181920_2000_ubuff2000_rbuff2000_2019_12 (UNSUBMITTED)>\n",
      "<Task IBCBTNKDIBADFWRGNFOWXIY6 EXPORT_FEATURES: viirs181920_2500_ubuff2500_rbuff2500_2002_0 (UNSUBMITTED)>\n",
      "<Task UMXJWGLCYUZKFJ5FT6IQVXC3 EXPORT_FEATURES: viirs181920_2500_ubuff2500_rbuff2500_2003_0 (UNSUBMITTED)>\n",
      "<Task GJ67PEBA3X356X3JMUXIIR3A EXPORT_FEATURES: viirs181920_2500_ubuff2500_rbuff2500_2005_0 (UNSUBMITTED)>\n",
      "<Task QFXJVD7UZW4BFMKCLP33QCHC EXPORT_FEATURES: viirs181920_2500_ubuff2500_rbuff2500_2006_0 (UNSUBMITTED)>\n",
      "<Task RDIE6LFFZPNQAACMFFEK4WZE EXPORT_FEATURES: viirs181920_2500_ubuff2500_rbuff2500_2008_0 (UNSUBMITTED)>\n",
      "<Task MUZOTQ3SSBE7GXPUOSTDM2EQ EXPORT_FEATURES: viirs181920_2500_ubuff2500_rbuff2500_2009_0 (UNSUBMITTED)>\n",
      "<Task U67SNBNHTR7PI6BFJFTRZIGW EXPORT_FEATURES: viirs181920_2500_ubuff2500_rbuff2500_2010_0 (UNSUBMITTED)>\n",
      "<Task TLG5VIMZGWNYXWFJ6RV6OSJM EXPORT_FEATURES: viirs181920_2500_ubuff2500_rbuff2500_2011_1 (UNSUBMITTED)>\n",
      "<Task KRGPBY3ZO2257MTGL3U5KGQY EXPORT_FEATURES: viirs181920_2500_ubuff2500_rbuff2500_2012_2 (UNSUBMITTED)>\n",
      "<Task IUCPCI7WEHCLUEVXR2TULME7 EXPORT_FEATURES: viirs181920_2500_ubuff2500_rbuff2500_2013_2 (UNSUBMITTED)>\n",
      "<Task TI67SSDRRHULRW4FMFVYQT5L EXPORT_FEATURES: viirs181920_2500_ubuff2500_rbuff2500_2014_2 (UNSUBMITTED)>\n",
      "<Task WYEBDSWOHG2QR776CAKC7AIG EXPORT_FEATURES: viirs181920_2500_ubuff2500_rbuff2500_2015_3 (UNSUBMITTED)>\n",
      "<Task XPHXN6TPO2PI3NDAZEQQN4S7 EXPORT_FEATURES: viirs181920_2500_ubuff2500_rbuff2500_2016_9 (UNSUBMITTED)>\n",
      "<Task 6EABXMS6KWXHVGC54VV76EMG EXPORT_FEATURES: viirs181920_2500_ubuff2500_rbuff2500_2017_10 (UNSUBMITTED)>\n",
      "<Task XGXNXIBLKZSHNOYJ7TA4VUKP EXPORT_FEATURES: viirs181920_2500_ubuff2500_rbuff2500_2018_11 (UNSUBMITTED)>\n",
      "<Task LMBUIVROQBMIPKJUCCFRH5FA EXPORT_FEATURES: viirs181920_2500_ubuff2500_rbuff2500_2019_12 (UNSUBMITTED)>\n",
      "<Task DTRSMBNJ5Y3XJE4CH3MZ6XZ4 EXPORT_FEATURES: viirs181920_5000_ubuff5000_rbuff5000_2002_0 (UNSUBMITTED)>\n",
      "<Task CC65MVQFZCIZ42SFVIM6FQTQ EXPORT_FEATURES: viirs181920_5000_ubuff5000_rbuff5000_2003_0 (UNSUBMITTED)>\n",
      "<Task SRDPN76FJFNBBZ6SAYQWJ5SW EXPORT_FEATURES: viirs181920_5000_ubuff5000_rbuff5000_2005_0 (UNSUBMITTED)>\n",
      "<Task OAGGZEJFPVXHNWZBYKUNQYZT EXPORT_FEATURES: viirs181920_5000_ubuff5000_rbuff5000_2006_0 (UNSUBMITTED)>\n",
      "<Task C45LLUUQQT4RDJRX74FMKLJU EXPORT_FEATURES: viirs181920_5000_ubuff5000_rbuff5000_2008_0 (UNSUBMITTED)>\n",
      "<Task VT6DLMX2QP27NAHIJCVM25XE EXPORT_FEATURES: viirs181920_5000_ubuff5000_rbuff5000_2009_0 (UNSUBMITTED)>\n",
      "<Task R7QXLOKDZ5YTSFCHHYQW5JLI EXPORT_FEATURES: viirs181920_5000_ubuff5000_rbuff5000_2010_0 (UNSUBMITTED)>\n",
      "<Task S5DQJGL6FMPANP6Q732A5OTH EXPORT_FEATURES: viirs181920_5000_ubuff5000_rbuff5000_2011_1 (UNSUBMITTED)>\n",
      "<Task 4H3KTBXP45MIZLYWWFSFENIL EXPORT_FEATURES: viirs181920_5000_ubuff5000_rbuff5000_2012_2 (UNSUBMITTED)>\n",
      "<Task KQXG7AFU2LKYRPBOU763ZHQ7 EXPORT_FEATURES: viirs181920_5000_ubuff5000_rbuff5000_2013_2 (UNSUBMITTED)>\n",
      "<Task UD3SPUZGJSIJV5RBCP3TXUQ4 EXPORT_FEATURES: viirs181920_5000_ubuff5000_rbuff5000_2014_2 (UNSUBMITTED)>\n",
      "<Task LT4MT25YG3UM4EO3I6CUZY23 EXPORT_FEATURES: viirs181920_5000_ubuff5000_rbuff5000_2015_3 (UNSUBMITTED)>\n",
      "<Task M5DJ4QDJAYW33PJ7YKCGVVOF EXPORT_FEATURES: viirs181920_5000_ubuff5000_rbuff5000_2016_9 (UNSUBMITTED)>\n",
      "<Task VV2BU2KCMLPL2UDWI4GOQKCQ EXPORT_FEATURES: viirs181920_5000_ubuff5000_rbuff5000_2017_10 (UNSUBMITTED)>\n",
      "<Task OPZZASDNT32EJWOT3HOLVLOK EXPORT_FEATURES: viirs181920_5000_ubuff5000_rbuff5000_2018_11 (UNSUBMITTED)>\n",
      "<Task QFSKFYIDOSHK6WF2P3DI6OCS EXPORT_FEATURES: viirs181920_5000_ubuff5000_rbuff5000_2019_12 (UNSUBMITTED)>\n",
      "<Task DFRTDJG6O6YMDWIL2MBGJSIK EXPORT_FEATURES: worldpop2020_2000_ubuff2000_rbuff2000_2002_0 (UNSUBMITTED)>\n",
      "<Task 34S6MPFKPJFBQXUPIH3HOH73 EXPORT_FEATURES: worldpop2020_2000_ubuff2000_rbuff2000_2003_0 (UNSUBMITTED)>\n",
      "<Task N62YBEJTGXKSVJYUM34F4LT3 EXPORT_FEATURES: worldpop2020_2000_ubuff2000_rbuff2000_2005_0 (UNSUBMITTED)>\n",
      "<Task TDGCHKK3575QNKNTPAGUS44S EXPORT_FEATURES: worldpop2020_2000_ubuff2000_rbuff2000_2006_0 (UNSUBMITTED)>\n",
      "<Task O7FCRTFLAOM3D54CHB462O4V EXPORT_FEATURES: worldpop2020_2000_ubuff2000_rbuff2000_2008_0 (UNSUBMITTED)>\n",
      "<Task YARZONTC2BRYRFN7KMOP5GWE EXPORT_FEATURES: worldpop2020_2000_ubuff2000_rbuff2000_2009_0 (UNSUBMITTED)>\n",
      "<Task ESURNKRQ4QPDFOP3PLKJAKI5 EXPORT_FEATURES: worldpop2020_2000_ubuff2000_rbuff2000_2010_0 (UNSUBMITTED)>\n",
      "<Task NMKZU6FFWE66JNWTFCSGYBFB EXPORT_FEATURES: worldpop2020_2000_ubuff2000_rbuff2000_2011_1 (UNSUBMITTED)>\n",
      "<Task L7FZ33736SJKRTM7UFRQ5PJA EXPORT_FEATURES: worldpop2020_2000_ubuff2000_rbuff2000_2012_2 (UNSUBMITTED)>\n",
      "<Task XPJFXPRSSS7QRMSX3Q6PE4Z5 EXPORT_FEATURES: worldpop2020_2000_ubuff2000_rbuff2000_2013_2 (UNSUBMITTED)>\n",
      "<Task DYKFBJTYHBGCFJ3RMSQBWP6Z EXPORT_FEATURES: worldpop2020_2000_ubuff2000_rbuff2000_2014_2 (UNSUBMITTED)>\n",
      "<Task 6NWZWVHN6LXEIQ26FN7GTCTJ EXPORT_FEATURES: worldpop2020_2000_ubuff2000_rbuff2000_2015_3 (UNSUBMITTED)>\n",
      "<Task NUHZSA3ABNLWTB3QSK52QYLM EXPORT_FEATURES: worldpop2020_2000_ubuff2000_rbuff2000_2016_9 (UNSUBMITTED)>\n",
      "<Task BJVRFR2JESNFFV7WBS6VFW5L EXPORT_FEATURES: worldpop2020_2000_ubuff2000_rbuff2000_2017_10 (UNSUBMITTED)>\n",
      "<Task LOI7R6K5D7I5YGQKRKMWFX4L EXPORT_FEATURES: worldpop2020_2000_ubuff2000_rbuff2000_2018_11 (UNSUBMITTED)>\n",
      "<Task MIW2VXCADNHKOOFPPLWXFVD7 EXPORT_FEATURES: worldpop2020_2000_ubuff2000_rbuff2000_2019_12 (UNSUBMITTED)>\n",
      "<Task 5E7HS7V6QXQY7D7LYBUQXUKV EXPORT_FEATURES: worldpop2020_5000_ubuff5000_rbuff5000_2002_0 (UNSUBMITTED)>\n",
      "<Task GUOZFYHDWJUNBWWKC7QBM7I5 EXPORT_FEATURES: worldpop2020_5000_ubuff5000_rbuff5000_2003_0 (UNSUBMITTED)>\n",
      "<Task KC74TW2I6OLGVR22NRRZDLKU EXPORT_FEATURES: worldpop2020_5000_ubuff5000_rbuff5000_2005_0 (UNSUBMITTED)>\n",
      "<Task FLPRBCBIGEDGCV7UJAERWVFO EXPORT_FEATURES: worldpop2020_5000_ubuff5000_rbuff5000_2006_0 (UNSUBMITTED)>\n",
      "<Task GTQDBSNRNM6MUKK2EXORMD7X EXPORT_FEATURES: worldpop2020_5000_ubuff5000_rbuff5000_2008_0 (UNSUBMITTED)>\n",
      "<Task Z4OGDZAMOX47ZOUM3TJDS4FE EXPORT_FEATURES: worldpop2020_5000_ubuff5000_rbuff5000_2009_0 (UNSUBMITTED)>\n",
      "<Task HSX7WC3CPYH4OKLZDQPRUYX7 EXPORT_FEATURES: worldpop2020_5000_ubuff5000_rbuff5000_2010_0 (UNSUBMITTED)>\n",
      "<Task KXYATIMKSV6QYUQ7E7FP4ZBW EXPORT_FEATURES: worldpop2020_5000_ubuff5000_rbuff5000_2011_1 (UNSUBMITTED)>\n",
      "<Task GUTM4SL43L6BSEHMH27GHXPY EXPORT_FEATURES: worldpop2020_5000_ubuff5000_rbuff5000_2012_2 (UNSUBMITTED)>\n",
      "<Task WOAZKN7TODQMMJ4NVWV4M4VL EXPORT_FEATURES: worldpop2020_5000_ubuff5000_rbuff5000_2013_2 (UNSUBMITTED)>\n",
      "<Task FNKCRFVKOXS5MZ3R44OQESXL EXPORT_FEATURES: worldpop2020_5000_ubuff5000_rbuff5000_2014_2 (UNSUBMITTED)>\n",
      "<Task VLAC4GRQJLRVNPZBF6MTTHNA EXPORT_FEATURES: worldpop2020_5000_ubuff5000_rbuff5000_2015_3 (UNSUBMITTED)>\n",
      "<Task BHEBLAZWYMPFMVICPDV5X2MA EXPORT_FEATURES: worldpop2020_5000_ubuff5000_rbuff5000_2016_9 (UNSUBMITTED)>\n",
      "<Task OK3ED7BBQ3BNWAWQAEQXAVQ4 EXPORT_FEATURES: worldpop2020_5000_ubuff5000_rbuff5000_2017_10 (UNSUBMITTED)>\n",
      "<Task K2EHH2YXWFGFLVPHCES2YHO7 EXPORT_FEATURES: worldpop2020_5000_ubuff5000_rbuff5000_2018_11 (UNSUBMITTED)>\n",
      "<Task WEGCRJMOAMSUXTTO2DVPVMXY EXPORT_FEATURES: worldpop2020_5000_ubuff5000_rbuff5000_2019_12 (UNSUBMITTED)>\n",
      "<Task 4BFYW2DYFHKT56BITFN4JOE4 EXPORT_FEATURES: worldpop2020_10000_ubuff10000_rbuff10000_2002_0 (UNSUBMITTED)>\n",
      "<Task JR2FG5Z6L4ERJTSD6KKF6JFM EXPORT_FEATURES: worldpop2020_10000_ubuff10000_rbuff10000_2003_0 (UNSUBMITTED)>\n",
      "<Task 3DEUNUYMYJVBK57W66S6YHAP EXPORT_FEATURES: worldpop2020_10000_ubuff10000_rbuff10000_2005_0 (UNSUBMITTED)>\n",
      "<Task 5T7NXEK6SZR2SLMSELZ3KTA7 EXPORT_FEATURES: worldpop2020_10000_ubuff10000_rbuff10000_2006_0 (UNSUBMITTED)>\n",
      "<Task GN5T5QH2INK4BH5Q2H7UUVKS EXPORT_FEATURES: worldpop2020_10000_ubuff10000_rbuff10000_2008_0 (UNSUBMITTED)>\n",
      "<Task 3GSPSD2FHL4QU2QJBAJ46YAA EXPORT_FEATURES: worldpop2020_10000_ubuff10000_rbuff10000_2009_0 (UNSUBMITTED)>\n",
      "<Task XDS2HK2FMRG356XIIF2MIHYJ EXPORT_FEATURES: worldpop2020_10000_ubuff10000_rbuff10000_2010_0 (UNSUBMITTED)>\n",
      "<Task SCPCZME5MMY4PT2BHW3VWGRL EXPORT_FEATURES: worldpop2020_10000_ubuff10000_rbuff10000_2011_1 (UNSUBMITTED)>\n",
      "<Task GL7U7AUGUOMZLNAJGJHKLKLA EXPORT_FEATURES: worldpop2020_10000_ubuff10000_rbuff10000_2012_2 (UNSUBMITTED)>\n",
      "<Task FSNLFNUHTL5MTJX2CNHDJDDY EXPORT_FEATURES: worldpop2020_10000_ubuff10000_rbuff10000_2013_2 (UNSUBMITTED)>\n",
      "<Task E55FPSPBAYJTU62VXQJV6Y65 EXPORT_FEATURES: worldpop2020_10000_ubuff10000_rbuff10000_2014_2 (UNSUBMITTED)>\n"
     ]
    }
   ],
   "source": [
    "## Tasks completed\n",
    "for task_list in tasks_all:\n",
    "    for task_i in task_list:\n",
    "        \n",
    "        task_i_status = task_i[0].status()\n",
    "        if task_i_status['state'] == 'COMPLETED':\n",
    "            print(task_i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n",
      "COMPLETED\n"
     ]
    }
   ],
   "source": [
    "## See all tasks\n",
    "for task_list in tasks_all:\n",
    "    for task_i in task_list:\n",
    "        \n",
    "        print(task_i[0].status()['state'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cancel Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    for task_list in tasks_all:\n",
    "        for task_i in task_list:\n",
    "\n",
    "            task_i[0].cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
