{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>To authorize access needed by Earth Engine, open the following\n",
       "        URL in a web browser and follow the instructions:</p>\n",
       "        <p><a href=https://accounts.google.com/o/oauth2/auth?client_id=517222506229-vsmmajv00ul0bs7p89v5m89qs8eb9359.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fearthengine+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code&code_challenge=FakuX8MKwbeixjx1T6iAIdI-103gpp7tPFsWO4huYgc&code_challenge_method=S256>https://accounts.google.com/o/oauth2/auth?client_id=517222506229-vsmmajv00ul0bs7p89v5m89qs8eb9359.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fearthengine+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code&code_challenge=FakuX8MKwbeixjx1T6iAIdI-103gpp7tPFsWO4huYgc&code_challenge_method=S256</a></p>\n",
       "        <p>The authorization workflow will generate a code, which you\n",
       "        should paste in the box below</p>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter verification code:  4/1AX4XfWiCvaHvQCvGUB5QeLx5la_zwBh3EQjuvek9R8tGDFLBdJVx8oyRtlE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully saved authorization token.\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "ee.Authenticate()\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import geetools\n",
    "from geetools import ui, cloud_mask\n",
    "import os, datetime\n",
    "import config as cf\n",
    "import pandas as pd\n",
    "import eeconvert\n",
    "import time\n",
    "import geopandas as gpd\n",
    "\n",
    "cloud_mask_landsatSR = cloud_mask.landsatSR()\n",
    "cloud_mask_sentinel2 = cloud_mask.sentinel2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "SURVEY_NAME = 'OPM'\n",
    "REEXTRACT_IF_FILE_EXISTS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gis.stackexchange.com/questions/257727/iterate-over-imagecollection-returning-pandas-dataframe-using-earth-engine-pyt\n",
    "def fc2df(fc):\n",
    "    # Convert a FeatureCollection into a pandas DataFrame\n",
    "    # Features is a list of dict with the output\n",
    "    features = fc.getInfo()['features']\n",
    "\n",
    "    dictarr = []\n",
    "\n",
    "    for f in features:\n",
    "        # Store all attributes in a dict\n",
    "        attr = f['properties']\n",
    "        # and treat geometry separately\n",
    "        attr['geometry'] = f['geometry']  # GeoJSON Feature!\n",
    "        # attr['geometrytype'] = f['geometry']['type']\n",
    "        dictarr.append(attr)\n",
    "\n",
    "    df = gpd.GeoDataFrame(dictarr)\n",
    "    # Convert GeoJSON features to shape\n",
    "    df = df.drop(columns=['geometry'])\n",
    "    return df\n",
    "\n",
    "def survey_to_fc_buffer(survey_df, buffer_size_urban, buffer_size_rural):\n",
    "    '''\n",
    "    Convert pandas dataframe of survey locations to a feature collection. \n",
    "    \n",
    "    Inputs:\n",
    "        survey_df: pandas dataframe of survey locations. Function assumes \n",
    "                   the dataframe contains (1) latitude, (2) longitude and\n",
    "                   (3) uid variables. Assumes coordinates in WGS84.\n",
    "    Returns:\n",
    "        (feature collection)\n",
    "    '''\n",
    "    \n",
    "    survey_fc_list = []\n",
    "    \n",
    "    n_rows = survey_df.shape[0]\n",
    "    for i in range(0, n_rows):\n",
    "        survey_df_i = survey_df.iloc[[i]]\n",
    "        \n",
    "        ur = survey_df_i['urban_rural'].iloc[0]\n",
    "        if ur == 'U':\n",
    "            buffer_size = buffer_size_urban\n",
    "        elif ur == 'R':\n",
    "            buffer_size = buffer_size_rural\n",
    "\n",
    "        f_i = ee.Feature(ee.Geometry.Point([survey_df_i['longitude'].iloc[0], \n",
    "                                            survey_df_i['latitude'].iloc[0]]), \n",
    "                         {'uid': survey_df_i['uid'].iloc[0],\n",
    "                          'year': str(survey_df_i['year'].iloc[0])})\n",
    "        \n",
    "        f_i = f_i.buffer(buffer_size)\n",
    "\n",
    "        survey_fc_list.append(f_i)\n",
    "        \n",
    "    survey_fc = ee.FeatureCollection(survey_fc_list)\n",
    "    \n",
    "    return survey_fc\n",
    "\n",
    "def extract_sat(survey_df, buffer_size_urban, buffer_size_rural, satellite, year, chunk, survey_name):\n",
    "    '''\n",
    "    Extract satellite imagery to locations \n",
    "    \n",
    "    Inputs:\n",
    "        survey_df: pandas dataframe of survey locations. Function assumes \n",
    "                   the dataframe contains (1) latitude, (2) longitude and\n",
    "                   (3) uid variables. Assumes coordinates in WGS84.\n",
    "    Returns:\n",
    "        (feature collection)\n",
    "    '''\n",
    "    \n",
    "    #print(survey_df.uid)\n",
    "    \n",
    "    year_start_sp5 = \"2018-01-01\"\n",
    "    year_end_sp5 = '2020-12-31'\n",
    "    \n",
    "    # Prep l7 ---------------------------------------------------\n",
    "    if satellite == 'worldpop':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 100 \n",
    "        \n",
    "        # Year\n",
    "        year_use = year\n",
    "        \n",
    "        year_plus = year_use\n",
    "        year_minus = year_use\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('WorldPop/GP/100m/pop')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .median()\n",
    "        \n",
    "        # After the reducer computers the sum, it names the value \"sum\", not population\n",
    "        BANDS = ['sum']\n",
    "    \n",
    "    # Prep l7 ---------------------------------------------------\n",
    "    if satellite == 'l7':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 100 # ok to upscale\n",
    "        \n",
    "        # Year\n",
    "        year_use = year\n",
    "        \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('LANDSAT/LC07/C01/T1_SR')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .map(cloud_mask_landsatSR)\\\n",
    "            .median()\\\n",
    "            .multiply(0.0001)\n",
    "\n",
    "        ndvi = image.normalizedDifference(['B4', 'B3']).rename('NDVI');\n",
    "        image = image.addBands(ndvi)\n",
    "        \n",
    "        BANDS = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7']\n",
    "        \n",
    "    # Sentinel-5P OFFL AER AI: Offline UV Aerosol Index  -------------------\n",
    "    if satellite == 'uv_aer':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 1000 # ok to upscale\n",
    "        \n",
    "        # Starts in 2018; take all years\n",
    "        image = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_AER_AI\")\\\n",
    "            .filterDate(year_start_sp5, year_end_sp5)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['absorbing_aerosol_index']\n",
    "        \n",
    "    # Sentinel-5P OFFL CO: Offline Carbon Monoxide  -------------------\n",
    "    if satellite == 'CO':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 1000 # ok to upscale\n",
    "        \n",
    "        # Starts in 2018; take all years\n",
    "        image = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_CO\")\\\n",
    "            .filterDate(year_start_sp5, year_end_sp5)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['CO_column_number_density', 'H2O_column_number_density']\n",
    "        \n",
    "    # Sentinel-5P OFFL HCHO: Offline Formaldehyde  -------------------\n",
    "    if satellite == 'HCHO':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 1000 # ok to upscale\n",
    "        \n",
    "        # Starts in 2018; take all years\n",
    "        image = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_HCHO\")\\\n",
    "            .filterDate(year_start_sp5, year_end_sp5)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['tropospheric_HCHO_column_number_density', 'tropospheric_HCHO_column_number_density_amf']\n",
    "        \n",
    "    # Sentinel-5P Nitrogen Dioxide  -----------------------------\n",
    "    if satellite == 'NO2':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 1000 # ok to upscale\n",
    "        \n",
    "        # Starts in 2018; take all years\n",
    "        image = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_NO2\")\\\n",
    "            .filterDate(year_start_sp5, year_end_sp5)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['NO2_column_number_density', 'tropospheric_NO2_column_number_density',\\\n",
    "                 'stratospheric_NO2_column_number_density', 'NO2_slant_column_number_density']\n",
    "        \n",
    "    # Sentinel-5P OFFL O3: Offline Ozone  -------------------\n",
    "    if satellite == 'ozone':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 1000 # ok to upscale\n",
    "        \n",
    "        # Starts in 2018; take all years\n",
    "        image = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_O3\")\\\n",
    "            .filterDate(year_start_sp5, year_end_sp5)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['O3_column_number_density', 'O3_effective_temperature']\n",
    "        \n",
    "    # Sentinel-5P OFFL SO2: Offline Sulphur Dioxide  -------------------\n",
    "    if satellite == 'SO2':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 1000 # ok to upscale\n",
    "        \n",
    "        # Starts in 2018; take all years\n",
    "        image = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_SO2\")\\\n",
    "            .filterDate(year_start_sp5, year_end_sp5)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['SO2_column_number_density', 'SO2_column_number_density_amf', 'SO2_slant_column_number_density']\n",
    "        \n",
    "    # Sentinel-5P OFFL CH4: Offline Methane  -------------------\n",
    "    if satellite == 'CH4':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 1000 # ok to upscale\n",
    "        \n",
    "        # Starts in 2018; take all years\n",
    "        image = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_CH4\")\\\n",
    "            .filterDate(year_start_sp5, year_end_sp5)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['CH4_column_volume_mixing_ratio_dry_air']\n",
    "        \n",
    "    # CSP gHM: Global Human Modification ---------------------------------\n",
    "    if satellite == 'GlobalHumanModification':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 1000 # ok to upscale\n",
    "                \n",
    "        image = ee.ImageCollection(\"CSP/HM/GlobalHumanModification\")\\\n",
    "            .median()\n",
    "        \n",
    "        # Original name is \"gHM\", but because only one value, it takes the\n",
    "        # name of the reducer; we use mean\n",
    "        BANDS = ['mean']\n",
    "        \n",
    "    # WorldClim BIO Variables V1 ---------------------------------\n",
    "    if satellite == 'worldclim_bio':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 1000 # ok to upscale\n",
    "                \n",
    "        image = ee.Image('WORLDCLIM/V1/BIO')\n",
    "        \n",
    "        BANDS = ['bio01', 'bio02', 'bio03', 'bio04', 'bio05', 'bio06', 'bio07', 'bio08', 'bio09', 'bio10',\\\n",
    "                 'bio11', 'bio12', 'bio13', 'bio14', 'bio15', 'bio16', 'bio17', 'bio18', 'bio19']\n",
    "        \n",
    "    # Elevation - SRTM ------------------------------------------\n",
    "    if satellite == 'elevation':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 1000 # ok to upscale\n",
    "                \n",
    "        image = ee.Image('USGS/SRTMGL1_003') # CGIAR/SRTM90_V4\n",
    "        \n",
    "        # elevation?\n",
    "        BANDS = ['mean']\n",
    "        \n",
    "    # Elevation - SRTM ------------------------------------------\n",
    "    if satellite == 'slope':\n",
    "        # https://developers.google.com/earth-engine/datasets/catalog/CGIAR_SRTM90_V4#description\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 500 # ok to upscale\n",
    "                \n",
    "        image_raw = ee.Image('USGS/SRTMGL1_003') # CGIAR/SRTM90_V4\n",
    "        image_elev = image_raw.select('elevation')\n",
    "        image = ee.Terrain.slope(image_elev)\n",
    "                \n",
    "        # mean?\n",
    "        BANDS = ['mean']\n",
    "        \n",
    "    # Prep l8 ---------------------------------------------------\n",
    "    if satellite == 'l8':\n",
    "        \n",
    "        SCALE = 100 # ok to upscale\n",
    "        #SCALE = 2000\n",
    "        \n",
    "        # Year\n",
    "        # landsat 8 starts in April 2013; if year is less than\n",
    "        # 2014, use 2014 as year (to ensure have year before and after)\n",
    "        if year < 2014:\n",
    "            year_use = 2014\n",
    "        else:\n",
    "            year_use = year\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .map(cloud_mask_landsatSR)\\\n",
    "            .median()\\\n",
    "            .multiply(0.0001)\n",
    "\n",
    "        # https://www.linkedin.com/pulse/ndvi-ndbi-ndwi-calculation-using-landsat-7-8-tek-bahadur-kshetri\n",
    "        ndvi = image.normalizedDifference(['B5', 'B4']).rename('NDVI');\n",
    "        ndbi = image.normalizedDifference(['B6', 'B5']).rename('NDBI');\n",
    "        image = image.addBands(ndvi)\n",
    "        image = image.addBands(ndbi)\n",
    "        \n",
    "        bu = image.select('NDBI').subtract(image.select('NDVI')).rename('BU')\n",
    "        image = image.addBands(bu)\n",
    "        \n",
    "        BANDS = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B10', 'B11', 'NDVI', 'NDBI', 'BU']\n",
    "        #BANDS = ['NDVI']\n",
    "        \n",
    "    # Prep s2 ---------------------------------------------------\n",
    "    if satellite == 's2':\n",
    "        \n",
    "        SCALE = 100 # ok to upscale\n",
    "        \n",
    "        # Year\n",
    "        # sentinel starts in March 2017; juse use 2018\n",
    "        year_use = 2018\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-12-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('COPERNICUS/S2_SR')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .map(cloud_mask_sentinel2)\\\n",
    "            .median()\\\n",
    "            .multiply(0.0001)\n",
    "        \n",
    "        ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI');\n",
    "        image = image.addBands(ndvi)\n",
    "        \n",
    "        BANDS = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12', 'AOT', 'NDVI']\n",
    "\n",
    "        image = image.select(BANDS) \n",
    "        \n",
    "    # Prep drought ---------------------------------------------------\n",
    "    if satellite == 'gridmet_drought':\n",
    "        \n",
    "        SCALE = 5000 \n",
    "\n",
    "        year_minus_str = str(year) + '-01-01'\n",
    "        year_plus_str = str(year) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection(\"GRIDMET/DROUGHT\")\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['pdsi', 'z', 'eddi1y', 'eddi2y', 'eddi5y']\n",
    "    \n",
    "    \n",
    "    # Prep drought ---------------------------------------------------\n",
    "    if satellite == 'ecmwf_weather':\n",
    "        \n",
    "        SCALE = 1000 \n",
    "\n",
    "        year_minus_str = str(year) + '-01-01'\n",
    "        year_plus_str = str(year) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection(\"ECMWF/ERA5/DAILY\")\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .mean()\n",
    "        \n",
    "        BANDS = ['mean_2m_air_temperature', \n",
    "                 'minimum_2m_air_temperature', \n",
    "                 'maximum_2m_air_temperature',\n",
    "                 'total_precipitation']\n",
    "        \n",
    "    # Prep drought ---------------------------------------------------\n",
    "    if satellite == 'ecmwf_weather_q1':\n",
    "        \n",
    "        SCALE = 1000 \n",
    "\n",
    "        year_minus_str = str(year) + '-01-01'\n",
    "        year_plus_str = str(year) + '-03-31'\n",
    "        \n",
    "        image = ee.ImageCollection(\"ECMWF/ERA5/DAILY\")\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .mean()\n",
    "        \n",
    "        BANDS = ['mean_2m_air_temperature', \n",
    "                 'minimum_2m_air_temperature', \n",
    "                 'maximum_2m_air_temperature',\n",
    "                 'total_precipitation']\n",
    "        \n",
    "    # Prep drought ---------------------------------------------------\n",
    "    if satellite == 'ecmwf_weather_q2':\n",
    "        \n",
    "        SCALE = 1000 \n",
    "\n",
    "        year_minus_str = str(year) + '-04-01'\n",
    "        year_plus_str = str(year) + '-06-30'\n",
    "        \n",
    "        image = ee.ImageCollection(\"ECMWF/ERA5/DAILY\")\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .mean()\n",
    "        \n",
    "        BANDS = ['mean_2m_air_temperature', \n",
    "                 'minimum_2m_air_temperature', \n",
    "                 'maximum_2m_air_temperature',\n",
    "                 'total_precipitation']\n",
    "        \n",
    "    # Prep drought ---------------------------------------------------\n",
    "    if satellite == 'ecmwf_weather_q3':\n",
    "        \n",
    "        SCALE = 1000 \n",
    "\n",
    "        year_minus_str = str(year) + '-07-01'\n",
    "        year_plus_str = str(year) + '-09-30'\n",
    "        \n",
    "        image = ee.ImageCollection(\"ECMWF/ERA5/DAILY\")\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .mean()\n",
    "        \n",
    "        BANDS = ['mean_2m_air_temperature', \n",
    "                 'minimum_2m_air_temperature', \n",
    "                 'maximum_2m_air_temperature',\n",
    "                 'total_precipitation']\n",
    "        \n",
    "    # Prep drought ---------------------------------------------------\n",
    "    if satellite == 'ecmwf_weather_q4':\n",
    "        \n",
    "        SCALE = 1000 \n",
    "\n",
    "        year_minus_str = str(year) + '-10-01'\n",
    "        year_plus_str = str(year) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection(\"ECMWF/ERA5/DAILY\")\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .mean()\n",
    "        \n",
    "        BANDS = ['mean_2m_air_temperature', \n",
    "                 'minimum_2m_air_temperature', \n",
    "                 'maximum_2m_air_temperature',\n",
    "                 'total_precipitation']\n",
    "    \n",
    "    # Prep viirs ---------------------------------------------------\n",
    "    if satellite == 'viirs':\n",
    "        \n",
    "        SCALE = 500 \n",
    "        \n",
    "        # Year\n",
    "        # VIIRS starts in April 2012; if year is less than\n",
    "        # 2013, use 2013 as year (to ensure have year before and after)\n",
    "        if year < 2013:\n",
    "            year_use = 2013\n",
    "        else:\n",
    "            year_use = year\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('NOAA/VIIRS/DNB/MONTHLY_V1/VCMCFG')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['avg_rad']\n",
    "        \n",
    "    # Prep DMSP ---------------------------------------------------\n",
    "    if satellite == 'dmsp':\n",
    "        \n",
    "        SCALE = 1000 \n",
    "        \n",
    "        # Year\n",
    "        # DMSP-OLS starts in 2013; if year is more than\n",
    "        # 2012, use 2012 as year (to ensure have year before and after)\n",
    "        if year > 2012:\n",
    "            year_use = 2012\n",
    "        else:\n",
    "            year_use = year\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('NOAA/DMSP-OLS/NIGHTTIME_LIGHTS')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['stable_lights', 'avg_lights_x_pct']\n",
    "    \n",
    "    # Prep Survey ---------------------------------------------------\n",
    "    survey_fc = survey_to_fc_buffer(survey_df, buffer_size_urban, buffer_size_rural)\n",
    "    \n",
    "    # Extract Values ---------------------------------------------------\n",
    "    if satellite == 'worldpop':\n",
    "        vals = image.reduceRegions(collection = survey_fc,\n",
    "                                   reducer = ee.Reducer.sum(),\n",
    "                                   scale = SCALE,\n",
    "                                   tileScale = 8)\n",
    "    else:\n",
    "        vals = image.reduceRegions(collection = survey_fc,\n",
    "                                   reducer = ee.Reducer.mean(),\n",
    "                                   scale = SCALE,\n",
    "                                   tileScale = 8)\n",
    "\n",
    "    # OLD =============\n",
    "    # Survey dataset that only contains the uid variable\n",
    "    #survey_df = survey_df[['uid']]\n",
    "            \n",
    "    #for band_i in BANDS:\n",
    "    #    survey_df[satellite + '_' + band_i] = vals.aggregate_array(band_i).getInfo()\n",
    "        \n",
    "    # NEW =============\n",
    "    #df_out = fc2df(vals)\n",
    "    #print(df_out)\n",
    "    #df_out = pd.DataFrame()\n",
    "    \n",
    "    bands_to_export = BANDS.copy()\n",
    "    bands_to_export.append('uid')\n",
    "    bands_to_export.append('year')\n",
    "    #print(bands_to_export)\n",
    "    \n",
    "    task = ee.batch.Export.table.toDrive(collection=vals, \n",
    "                                         folder='satellite_data_from_gee_' + survey_name.lower(), \n",
    "                                         description=satellite + \"_ubuff\" + str(buffer_size_urban) + '_rbuff' + str(buffer_size_rural) + \"_\" + str(year) + '_' + str(chunk), \n",
    "                                         fileFormat='CSV',\n",
    "                                         selectors = bands_to_export)\n",
    "    # selectors=props\n",
    "    task.start()\n",
    "    #ee.batch.data.startProcessing(mytask.id, mytask.config)\n",
    "    \n",
    "    if False:\n",
    "        time_elapsed = 0\n",
    "        while task.active():\n",
    "            if((time_elapsed % 60) == 0):\n",
    "                print('Polling for task (id: {}).'.format(task.id))\n",
    "            time.sleep(5)\n",
    "            time_elapsed = time_elapsed + 5\n",
    "        \n",
    "    return task\n",
    "\n",
    "def extract_satellite_in_chunks(survey_df, buffer_size_urban, buffer_size_rural, satellite, year, survey_name):\n",
    "    \n",
    "    vals_df_list = []\n",
    "    \n",
    "    for chunk_i in list(np.unique(survey_df.chunk_id)):\n",
    "        #print(chunk_i)\n",
    "        #time.sleep(5)\n",
    "\n",
    "        survey_df_i = survey_df[survey_df['chunk_id'] == chunk_i]\n",
    "        #print(survey_df_i.shape)\n",
    "        vals_i_df = extract_sat(survey_df_i, buffer_size_urban, buffer_size_rural, satellite, year, chunk_i, survey_name)\n",
    "\n",
    "        vals_df_list.append(vals_i_df)\n",
    "\n",
    "    #vals_df = pd.concat(vals_df_list)\n",
    "    \n",
    "    return vals_df_list\n",
    "\n",
    "def extract_satellite_by_year(survey_df, buffer_size_urban, buffer_size_rural, satellite, survey_name):\n",
    "    \n",
    "    vals_df_list = []\n",
    "    \n",
    "    for year_i in list(np.unique(survey_df.year)):\n",
    "        #print(year_i)\n",
    "        #time.sleep(5)\n",
    "\n",
    "        survey_df_i = survey_df[survey_df['year'] == year_i]\n",
    "        vals_i_df = extract_satellite_in_chunks(survey_df_i, buffer_size_urban, buffer_size_rural, satellite, year_i, survey_name)\n",
    "\n",
    "        vals_df_list.append(vals_i_df)\n",
    "\n",
    "    #vals_df = pd.concat(vals_df_list)\n",
    "    \n",
    "    return vals_df_list\n",
    "\n",
    "def chunk_ids(total_length, chunk_size):\n",
    "    n_numbers = np.ceil(total_length / chunk_size)\n",
    "    n_numbers = int(n_numbers)\n",
    "    \n",
    "    chunk_ids = list(range(0,n_numbers)) * chunk_size\n",
    "    chunk_ids.sort()\n",
    "    chunk_ids = chunk_ids[:total_length]\n",
    "    \n",
    "    return chunk_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load/Prep Survey Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df = pd.read_csv(os.path.join(cf.DROPBOX_DIRECTORY, 'Data', SURVEY_NAME, 'FinalData', 'Individual Datasets', 'survey_socioeconomic.csv'))\n",
    "survey_df = survey_df[['uid', 'year', 'urban_rural', 'latitude', 'longitude']]\n",
    "survey_df = survey_df.sort_values('year')\n",
    "#survey_df = survey_df[survey_df.uid != 'IA201400180012']\n",
    "\n",
    "CHUNK_SIZE = 5000\n",
    "survey_years = list(survey_df.year.unique())\n",
    "survey_df['chunk_id'] = chunk_ids(survey_df.shape[0], CHUNK_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  uid  year urban_rural   latitude  longitude  chunk_id\n",
      "0       1_PAK.7.7.1_1  2011           U  33.300222  71.919333         0\n",
      "960   320_PAK.2.2.1_1  2011           U  25.388972  64.321778         0\n",
      "963   320_PAK.8.1.4_1  2011           U  25.388083  68.319667         0\n",
      "2186   61_PAK.7.8.1_1  2011           R  31.714861  71.041139         0\n",
      "967   321_PAK.8.1.3_1  2011           U  25.377028  68.373583         0\n",
      "(2383, 6)\n"
     ]
    }
   ],
   "source": [
    "print(survey_df.head())\n",
    "print(survey_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If re-extract, delete existing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting existing files\n"
     ]
    }
   ],
   "source": [
    "if REEXTRACT_IF_FILE_EXISTS:\n",
    "    print(\"Deleting existing files from Google Drive\")\n",
    "\n",
    "    ## Path with files\n",
    "    OUT_PATH = os.path.join(cf.GOOGLEDRIVE_DIRECTORY, \n",
    "                            'Data', \n",
    "                             SURVEY_NAME, \n",
    "                             'FinalData', \n",
    "                             'Individual Datasets',\n",
    "                             'satellite_data_from_gee_' + SURVEY_NAME.lower())\n",
    "\n",
    "    ## Grab csv files\n",
    "    files_to_rm = [x for x in os.listdir(OUT_PATH) if '.csv' in x]\n",
    "\n",
    "    ## Delete files\n",
    "    for file_i in files_to_rm:\n",
    "\n",
    "        path_i = os.path.join(OUT_PATH, file_i)\n",
    "        os.remove(path_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of files already extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['viirs_ubuff5000_rbuff5000',\n",
       " 'viirs_ubuff2000_rbuff2000',\n",
       " 'ecmwf_weather_ubuff10000_rbuff10000',\n",
       " 'ecmwf_weather_q2_ubuff10000_rbuff10000',\n",
       " 'worldpop_ubuff10000_rbuff10000',\n",
       " 'NO2_ubuff2500_rbuff2500',\n",
       " 'uv_aer_ubuff2500_rbuff2500',\n",
       " 'HCHO_ubuff2500_rbuff2500',\n",
       " 'l8_ubuff2500_rbuff2500',\n",
       " 'CH4_ubuff2500_rbuff2500',\n",
       " 'ecmwf_weather_q1_ubuff10000_rbuff10000',\n",
       " 'elevation_ubuff5000_rbuff5000',\n",
       " 'SO2_ubuff2500_rbuff2500',\n",
       " 'slope_ubuff5000_rbuff5000',\n",
       " 'viirs_ubuff2500_rbuff2500',\n",
       " 'GlobalHumanModification_ubuff10000_rbuff10000',\n",
       " 'ecmwf_weather_q3_ubuff10000_rbuff10000',\n",
       " 'ozone_ubuff2500_rbuff2500',\n",
       " 'CO_ubuff2500_rbuff2500',\n",
       " 'ecmwf_weather_q4_ubuff10000_rbuff10000']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaned files\n",
    "DB_DATA_PATH = os.path.join(cf.DROPBOX_DIRECTORY, 'Data', \n",
    "                            SURVEY_NAME, 'FinalData', \n",
    "                            'Individual Datasets', 'satellite_data_from_gee')\n",
    "\n",
    "\n",
    "files_already_extracted = [x.replace('.Rds', '') for x in os.listdir(DB_DATA_PATH)]\n",
    "files_already_extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elevation\n",
      "elevation_ubuff5000_rbuff5000\n",
      "slope\n",
      "slope_ubuff5000_rbuff5000\n",
      "viirs_2000\n",
      "viirs_ubuff2000_rbuff2000\n",
      "viirs_2500\n",
      "viirs_ubuff2500_rbuff2500\n",
      "viirs_5000\n",
      "viirs_ubuff5000_rbuff5000\n",
      "GlobalHumanModification\n",
      "GlobalHumanModification_ubuff10000_rbuff10000\n",
      "worldpop\n",
      "worldpop_ubuff10000_rbuff10000\n",
      "l8\n",
      "l8_ubuff2500_rbuff2500\n",
      "ecmwf_weather\n",
      "ecmwf_weather_ubuff10000_rbuff10000\n",
      "ecmwf_weather_q1\n",
      "ecmwf_weather_q1_ubuff10000_rbuff10000\n",
      "ecmwf_weather_q2\n",
      "ecmwf_weather_q2_ubuff10000_rbuff10000\n",
      "ecmwf_weather_q3\n",
      "ecmwf_weather_q3_ubuff10000_rbuff10000\n",
      "ecmwf_weather_q4\n",
      "ecmwf_weather_q4_ubuff10000_rbuff10000\n",
      "NO2\n",
      "NO2_ubuff2500_rbuff2500\n",
      "uv_aer\n",
      "uv_aer_ubuff2500_rbuff2500\n",
      "CO\n",
      "CO_ubuff2500_rbuff2500\n",
      "HCHO\n",
      "HCHO_ubuff2500_rbuff2500\n",
      "ozone\n",
      "ozone_ubuff2500_rbuff2500\n",
      "SO2\n",
      "SO2_ubuff2500_rbuff2500\n",
      "CH4\n",
      "CH4_ubuff2500_rbuff2500\n"
     ]
    }
   ],
   "source": [
    "to_extract = ['elevation', \n",
    "              'slope',\n",
    "              'viirs_2000',\n",
    "              'viirs_2500',\n",
    "              'viirs_5000',\n",
    "              'GlobalHumanModification',\n",
    "              'worldpop',\n",
    "              'l8',\n",
    "              'ecmwf_weather',\n",
    "              'ecmwf_weather_q1',\n",
    "              'ecmwf_weather_q2',\n",
    "              'ecmwf_weather_q3',\n",
    "              'ecmwf_weather_q4',\n",
    "              'NO2', 'uv_aer', 'CO', 'HCHO', 'ozone', 'SO2', 'CH4']\n",
    "\n",
    "tasks_all = []\n",
    "for sat in to_extract:\n",
    "    print(sat)\n",
    "        \n",
    "    if sat in ['NO2', 'uv_aer', 'CO', 'HCHO', 'ozone', 'SO2', 'CH4', 'l8']:\n",
    "        buffer_u = 2500\n",
    "        buffer_r = 2500\n",
    "    \n",
    "    if sat in ['elevation', 'slope']:\n",
    "        buffer_u = 5000\n",
    "        buffer_r = 5000\n",
    "        \n",
    "    if sat in ['GlobalHumanModification', \n",
    "               'ecmwf_weather',\n",
    "               'ecmwf_weather_q1', 'ecmwf_weather_q2', 'ecmwf_weather_q3', 'ecmwf_weather_q4']:\n",
    "        buffer_u = 10000\n",
    "        buffer_r = 10000\n",
    "                \n",
    "    if sat == 'viirs_2000':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 2000\n",
    "        buffer_r = 2000\n",
    "        \n",
    "    if sat == 'viirs_2500':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 2500\n",
    "        buffer_r = 2500\n",
    "        \n",
    "    if sat == 'viirs_5000':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 5000\n",
    "        buffer_r = 5000\n",
    "                \n",
    "    file_root = sat + '_ubuff' + str(buffer_u) + '_rbuff' + str(buffer_r)\n",
    "    \n",
    "    # Check if should extract data\n",
    "    if (file_root not in files_already_extracted) | REEXTRACT_IF_FILE_EXISTS:\n",
    "        print(file_root)\n",
    "        \n",
    "        tasks_i = extract_satellite_by_year(survey_df, buffer_u, buffer_r, sat, SURVEY_NAME)\n",
    "        tasks_all.append(tasks_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Failed Tasks\n",
    "for task_list in tasks_all:\n",
    "    for task_i in task_list:\n",
    "        \n",
    "        task_i_status = task_i[0].status()\n",
    "        if task_i_status['state'] == 'FAILED':\n",
    "            print(task_i[0].status())\n",
    "            print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Task SFTLOAB5GQI2IXWDA6RG2XPT EXPORT_FEATURES: elevation_ubuff5000_rbuff5000_2011_0 (UNSUBMITTED)>\n",
      "<Task JTPMVAC3EWVJC5BFXVFNM5PD EXPORT_FEATURES: elevation_ubuff5000_rbuff5000_2013_0 (UNSUBMITTED)>\n",
      "<Task PICA5ARKS2EFQASBYSIMZ3VX EXPORT_FEATURES: elevation_ubuff5000_rbuff5000_2014_0 (UNSUBMITTED)>\n",
      "<Task Y2WW6T3NUAGCA4CSDQA3R6P4 EXPORT_FEATURES: elevation_ubuff5000_rbuff5000_2016_0 (UNSUBMITTED)>\n",
      "<Task O4PKCPYTIRZTLG7L5R4T3CZW EXPORT_FEATURES: slope_ubuff5000_rbuff5000_2011_0 (UNSUBMITTED)>\n",
      "<Task AZ76CP5QWZDPNF4FF3PFNFWE EXPORT_FEATURES: slope_ubuff5000_rbuff5000_2013_0 (UNSUBMITTED)>\n",
      "<Task BVFDML4GT2E4L7MEI2CB5VXT EXPORT_FEATURES: slope_ubuff5000_rbuff5000_2014_0 (UNSUBMITTED)>\n",
      "<Task OPDB26JJNL7T2YGEZETXEQYH EXPORT_FEATURES: slope_ubuff5000_rbuff5000_2016_0 (UNSUBMITTED)>\n",
      "<Task QL2TICVOXWLKJVOTQPD6JIS7 EXPORT_FEATURES: viirs_ubuff2000_rbuff2000_2011_0 (UNSUBMITTED)>\n",
      "<Task YCPI5UPRRK3RYLMGKKPAJ2IS EXPORT_FEATURES: viirs_ubuff2000_rbuff2000_2013_0 (UNSUBMITTED)>\n",
      "<Task SJPUW4SO4EFZ3KXULHSBYO2S EXPORT_FEATURES: viirs_ubuff2000_rbuff2000_2014_0 (UNSUBMITTED)>\n",
      "<Task BG55AUNI5LAGTBLHC2EVIEF7 EXPORT_FEATURES: viirs_ubuff2000_rbuff2000_2016_0 (UNSUBMITTED)>\n",
      "<Task BGEY5DNJRAOV3FL5LBBBHCVK EXPORT_FEATURES: viirs_ubuff2500_rbuff2500_2011_0 (UNSUBMITTED)>\n",
      "<Task LBY2LVKSUNRFOAIBUZFYZVWN EXPORT_FEATURES: viirs_ubuff2500_rbuff2500_2013_0 (UNSUBMITTED)>\n",
      "<Task QWBAWQHZIWQKXIAZPBWIZFOV EXPORT_FEATURES: viirs_ubuff2500_rbuff2500_2014_0 (UNSUBMITTED)>\n",
      "<Task 6FC234R6EY73T4SVRBOQGQYN EXPORT_FEATURES: viirs_ubuff2500_rbuff2500_2016_0 (UNSUBMITTED)>\n",
      "<Task DYF7S7JSMNZZLAXXG7ORGM7G EXPORT_FEATURES: viirs_ubuff5000_rbuff5000_2011_0 (UNSUBMITTED)>\n",
      "<Task HPYIRQ42SLIUPXCMQWA2IA5B EXPORT_FEATURES: viirs_ubuff5000_rbuff5000_2013_0 (UNSUBMITTED)>\n",
      "<Task 2EYSXQN3II3KISCNTKHQA6QT EXPORT_FEATURES: viirs_ubuff5000_rbuff5000_2014_0 (UNSUBMITTED)>\n",
      "<Task FXSIUD6STZGEGULTR4IZY3Q7 EXPORT_FEATURES: viirs_ubuff5000_rbuff5000_2016_0 (UNSUBMITTED)>\n",
      "<Task IVOYHGLVXSQCSGL377TV5YEV EXPORT_FEATURES: GlobalHumanModification_ubuff10000_rbuff10000_2011_0 (UNSUBMITTED)>\n",
      "<Task VAGDVGIVT6T2LLREOIW7PZ6K EXPORT_FEATURES: GlobalHumanModification_ubuff10000_rbuff10000_2013_0 (UNSUBMITTED)>\n",
      "<Task CHYKNRTFYSQMIBURDC5OFGDL EXPORT_FEATURES: GlobalHumanModification_ubuff10000_rbuff10000_2014_0 (UNSUBMITTED)>\n",
      "<Task 5WQXSPDBZ3YISFXD6IVJ7BJA EXPORT_FEATURES: GlobalHumanModification_ubuff10000_rbuff10000_2016_0 (UNSUBMITTED)>\n",
      "<Task YDQYP53YCVH4377CPLMDKSYY EXPORT_FEATURES: worldpop_ubuff10000_rbuff10000_2011_0 (UNSUBMITTED)>\n",
      "<Task BCPRJ6TPWSTNXKC3THNF6FPC EXPORT_FEATURES: worldpop_ubuff10000_rbuff10000_2013_0 (UNSUBMITTED)>\n",
      "<Task RTU2K3HSQ5HLDNSNLTHUWI5R EXPORT_FEATURES: worldpop_ubuff10000_rbuff10000_2014_0 (UNSUBMITTED)>\n",
      "<Task WJ3UXHYZ7UIEU7VVQSW6URNB EXPORT_FEATURES: worldpop_ubuff10000_rbuff10000_2016_0 (UNSUBMITTED)>\n",
      "<Task LF4PTF7T2BLWWKC42Q7FX5SZ EXPORT_FEATURES: l8_ubuff2500_rbuff2500_2011_0 (UNSUBMITTED)>\n",
      "<Task QG76IURO3JCAABN7DOPDBFCU EXPORT_FEATURES: l8_ubuff2500_rbuff2500_2013_0 (UNSUBMITTED)>\n",
      "<Task BUGX33HAQG6K5STEPKUBEEVC EXPORT_FEATURES: l8_ubuff2500_rbuff2500_2014_0 (UNSUBMITTED)>\n",
      "<Task Q77MTYCZGP4HJ6VC6PURFA3J EXPORT_FEATURES: l8_ubuff2500_rbuff2500_2016_0 (UNSUBMITTED)>\n",
      "<Task 2ZVCREZWWY6ILYZRBL2B3IAP EXPORT_FEATURES: ecmwf_weather_ubuff10000_rbuff10000_2011_0 (UNSUBMITTED)>\n",
      "<Task FBXQRT4IMTNAPZMA4BEFXUKM EXPORT_FEATURES: ecmwf_weather_ubuff10000_rbuff10000_2013_0 (UNSUBMITTED)>\n",
      "<Task 6FFONCVN6PMEJHL5I746G4CA EXPORT_FEATURES: ecmwf_weather_ubuff10000_rbuff10000_2014_0 (UNSUBMITTED)>\n",
      "<Task NL7BA4R33JW336O7VUQ4MOF6 EXPORT_FEATURES: ecmwf_weather_ubuff10000_rbuff10000_2016_0 (UNSUBMITTED)>\n",
      "<Task P4TPAFJ5ZY55NTURA6JLG6MB EXPORT_FEATURES: ecmwf_weather_q1_ubuff10000_rbuff10000_2011_0 (UNSUBMITTED)>\n",
      "<Task EO6QJLKROHTUKIBJHW2I7JN5 EXPORT_FEATURES: ecmwf_weather_q1_ubuff10000_rbuff10000_2013_0 (UNSUBMITTED)>\n",
      "<Task H47CLUJLS3LCMOXBNKRGIXG4 EXPORT_FEATURES: ecmwf_weather_q1_ubuff10000_rbuff10000_2014_0 (UNSUBMITTED)>\n",
      "<Task A5WKAC3SNTH6PJPRTNL3ONRB EXPORT_FEATURES: ecmwf_weather_q1_ubuff10000_rbuff10000_2016_0 (UNSUBMITTED)>\n",
      "<Task JLKOHXQHEUK3WLQFLMXA2WEJ EXPORT_FEATURES: ecmwf_weather_q2_ubuff10000_rbuff10000_2011_0 (UNSUBMITTED)>\n",
      "<Task JELFEL6O54FMWHDUSIRMDM3N EXPORT_FEATURES: ecmwf_weather_q2_ubuff10000_rbuff10000_2013_0 (UNSUBMITTED)>\n",
      "<Task FB34RI6GU4AGE5DAIZYKMMSC EXPORT_FEATURES: ecmwf_weather_q2_ubuff10000_rbuff10000_2014_0 (UNSUBMITTED)>\n",
      "<Task X4HTB3QE5KEIUDGET4U4BNHZ EXPORT_FEATURES: ecmwf_weather_q2_ubuff10000_rbuff10000_2016_0 (UNSUBMITTED)>\n",
      "<Task PJRKQXI4V4JQNERIG2CJFRQ2 EXPORT_FEATURES: ecmwf_weather_q3_ubuff10000_rbuff10000_2011_0 (UNSUBMITTED)>\n",
      "<Task EJZ6BORGKZU7QBW7SEQZ5KHA EXPORT_FEATURES: ecmwf_weather_q3_ubuff10000_rbuff10000_2013_0 (UNSUBMITTED)>\n",
      "<Task CXHC43DL27SXSWWU3TW6GHZ3 EXPORT_FEATURES: ecmwf_weather_q3_ubuff10000_rbuff10000_2014_0 (UNSUBMITTED)>\n",
      "<Task IFUHD56X42XETGUVUL5XD5IA EXPORT_FEATURES: ecmwf_weather_q3_ubuff10000_rbuff10000_2016_0 (UNSUBMITTED)>\n",
      "<Task R2M4XCRIMDKKBFKKDNCGEECB EXPORT_FEATURES: ecmwf_weather_q4_ubuff10000_rbuff10000_2011_0 (UNSUBMITTED)>\n",
      "<Task J5KBNTE4GDF4S3FLZCUNF6EC EXPORT_FEATURES: ecmwf_weather_q4_ubuff10000_rbuff10000_2013_0 (UNSUBMITTED)>\n",
      "<Task PCUMWOVZD3EXCR3JNQBCZLPR EXPORT_FEATURES: ecmwf_weather_q4_ubuff10000_rbuff10000_2014_0 (UNSUBMITTED)>\n",
      "<Task 7ZXRTP24MCGHJVHQ2GVQWI7R EXPORT_FEATURES: ecmwf_weather_q4_ubuff10000_rbuff10000_2016_0 (UNSUBMITTED)>\n",
      "<Task 7CSLOZNFS7FMSCBTHHLRL7KN EXPORT_FEATURES: NO2_ubuff2500_rbuff2500_2011_0 (UNSUBMITTED)>\n",
      "<Task VYCFVBFKO4IIXUTPKRDWZF6Z EXPORT_FEATURES: NO2_ubuff2500_rbuff2500_2013_0 (UNSUBMITTED)>\n",
      "<Task 6MH5HXMAEDH5SHFRPQ2RUHVJ EXPORT_FEATURES: NO2_ubuff2500_rbuff2500_2014_0 (UNSUBMITTED)>\n",
      "<Task PTOBNPC6WEQLHKIOQOOAT7FK EXPORT_FEATURES: NO2_ubuff2500_rbuff2500_2016_0 (UNSUBMITTED)>\n",
      "<Task LTK5H2UQY6ZXXOHWSCLZSWY6 EXPORT_FEATURES: uv_aer_ubuff2500_rbuff2500_2011_0 (UNSUBMITTED)>\n",
      "<Task HX3PWWJF2HKYGFVSYBA5CD3I EXPORT_FEATURES: uv_aer_ubuff2500_rbuff2500_2013_0 (UNSUBMITTED)>\n",
      "<Task K7XAIQS5PJ5R7OVZKSKO4OA4 EXPORT_FEATURES: uv_aer_ubuff2500_rbuff2500_2014_0 (UNSUBMITTED)>\n",
      "<Task K2TE6PCAO2RD7OMX3FOUSQO3 EXPORT_FEATURES: uv_aer_ubuff2500_rbuff2500_2016_0 (UNSUBMITTED)>\n",
      "<Task SDIBDCMC3JTTFFEEINFOMHPT EXPORT_FEATURES: CO_ubuff2500_rbuff2500_2011_0 (UNSUBMITTED)>\n",
      "<Task B2UWVLR7QMHHMOHFTE4ANQ7G EXPORT_FEATURES: CO_ubuff2500_rbuff2500_2013_0 (UNSUBMITTED)>\n",
      "<Task T2E23UCV2MFHJOOLM3LYC5YQ EXPORT_FEATURES: CO_ubuff2500_rbuff2500_2014_0 (UNSUBMITTED)>\n",
      "<Task SR6MHNZXHIFM22I6VMHUSNWM EXPORT_FEATURES: CO_ubuff2500_rbuff2500_2016_0 (UNSUBMITTED)>\n",
      "<Task 3T46NEYW6M3JALWYUQUSMLGY EXPORT_FEATURES: HCHO_ubuff2500_rbuff2500_2011_0 (UNSUBMITTED)>\n",
      "<Task BABM2GG67CXZ2PCOPUZ7VRHI EXPORT_FEATURES: HCHO_ubuff2500_rbuff2500_2013_0 (UNSUBMITTED)>\n",
      "<Task OEKT4WBQNPYFOUJU3ZGHJ4YE EXPORT_FEATURES: HCHO_ubuff2500_rbuff2500_2014_0 (UNSUBMITTED)>\n",
      "<Task QK5WLMIGUZOL5VLXM3FXAR64 EXPORT_FEATURES: HCHO_ubuff2500_rbuff2500_2016_0 (UNSUBMITTED)>\n",
      "<Task HXPUFG7B77FUWEFVFC3VZTXD EXPORT_FEATURES: ozone_ubuff2500_rbuff2500_2011_0 (UNSUBMITTED)>\n",
      "<Task NHWAWZURGVCAEEDKT7IV2L3J EXPORT_FEATURES: ozone_ubuff2500_rbuff2500_2013_0 (UNSUBMITTED)>\n",
      "<Task QFQMCVQHDAHVCI44VYL6WQFB EXPORT_FEATURES: ozone_ubuff2500_rbuff2500_2014_0 (UNSUBMITTED)>\n",
      "<Task DCJ3RO5H5G5FUTY7FMVIAIJH EXPORT_FEATURES: ozone_ubuff2500_rbuff2500_2016_0 (UNSUBMITTED)>\n",
      "<Task 36OYHX2DOBEEY4BZVJKKVXHM EXPORT_FEATURES: SO2_ubuff2500_rbuff2500_2011_0 (UNSUBMITTED)>\n",
      "<Task NRTJMDAHFYOEIN66OYKLQGEO EXPORT_FEATURES: SO2_ubuff2500_rbuff2500_2013_0 (UNSUBMITTED)>\n",
      "<Task PMNGBDOWDZZF3MDVBG5KEQ7J EXPORT_FEATURES: SO2_ubuff2500_rbuff2500_2014_0 (UNSUBMITTED)>\n",
      "<Task TL6SPKLHDRWEBCEUTWTSV3BK EXPORT_FEATURES: SO2_ubuff2500_rbuff2500_2016_0 (UNSUBMITTED)>\n",
      "<Task 2SQB2GQOSG7J4DIPJDRLNC4N EXPORT_FEATURES: CH4_ubuff2500_rbuff2500_2011_0 (UNSUBMITTED)>\n",
      "<Task Q53UETTNCF5W7GG7UBKYD4BP EXPORT_FEATURES: CH4_ubuff2500_rbuff2500_2013_0 (UNSUBMITTED)>\n",
      "<Task DDNNGWA6MOTLRQFM74QFUFXP EXPORT_FEATURES: CH4_ubuff2500_rbuff2500_2014_0 (UNSUBMITTED)>\n",
      "<Task AVTIXUG3VVWEL3J22623IDLC EXPORT_FEATURES: CH4_ubuff2500_rbuff2500_2016_0 (UNSUBMITTED)>\n"
     ]
    }
   ],
   "source": [
    "## Tasks not started\n",
    "for task_list in tasks_all:\n",
    "    for task_i in task_list:\n",
    "        \n",
    "        task_i_status = task_i[0].status()\n",
    "        if task_i_status['state'] == 'READY':\n",
    "            print(task_i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tasks completed\n",
    "for task_list in tasks_all:\n",
    "    for task_i in task_list:\n",
    "        \n",
    "        task_i_status = task_i[0].status()\n",
    "        if task_i_status['state'] == 'COMPLETED':\n",
    "            print(task_i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n"
     ]
    }
   ],
   "source": [
    "## See all tasks\n",
    "for task_list in tasks_all:\n",
    "    for task_i in task_list:\n",
    "        \n",
    "        print(task_i[0].status()['state'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cancel Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    for task_list in tasks_all:\n",
    "        for task_i in task_list:\n",
    "\n",
    "            task_i[0].cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
