{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4DA6XKs-DESD"
   },
   "source": [
    "# Estimate CNN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vOjYjPI4iy0H"
   },
   "outputs": [],
   "source": [
    "#TODO:\n",
    "#* Lets have forcnn be training and nocnn be validation. No need for separate test set. Hmmmm no., wont have all classes.\n",
    "#*In prepping, may want to ensure balance within (a) train and (b) validation\n",
    "\n",
    "# Functions up top, then parameters / for loop below (some stuff doesn't need to be repeated for the for loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2bzIvfOYrVP"
   },
   "source": [
    "Adapted from:\n",
    "\n",
    "https://codelabs.developers.google.com/codelabs/keras-flowers-transfer-learning#0\n",
    "\n",
    "https://colab.research.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/04_Keras_Flowers_transfer_learning_solution.ipynb#scrollTo=M3G-2aUBQJ-H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JJTMmXsEf8n"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2Tdoud0HJNx1"
   },
   "outputs": [],
   "source": [
    "## Satellite and survey params\n",
    "SURVEY_NAME = 'DHS_nga_policy_experiment'\n",
    "\n",
    "# Parameters ------------------------------\n",
    "VERSION = 2\n",
    "\n",
    "if VERSION == 1:\n",
    "\n",
    "    SATELLITE         = 's2' \n",
    "    OUTCOME_VAR       = \"viirs\" \n",
    "    UNDERSAMPLE_INDIA = True\n",
    "    \n",
    "if VERSION == 2:\n",
    "\n",
    "    SATELLITE         = 'landsat' \n",
    "    OUTCOME_VAR       = \"ntlharmon\" \n",
    "    UNDERSAMPLE_INDIA = True\n",
    "\n",
    "# Objects based on parameters ------------\n",
    "OUT_NAME_SUFFIX   = SATELLITE + '_' + OUTCOME_VAR + '_underia' + str(UNDERSAMPLE_INDIA)\n",
    "\n",
    "## CNN params\n",
    "if SATELLITE == 's2':\n",
    "    IMAGE_SIZE = [224, 224]\n",
    "elif SATELLITE == 'landsat':\n",
    "    IMAGE_SIZE = [224, 224]\n",
    "\n",
    "if OUTCOME_VAR == 'viirs':\n",
    "    NUM_GROUPS = 5\n",
    "elif OUTCOME_VAR == 'ntlharmon':\n",
    "    NUM_GROUPS = 5\n",
    "\n",
    "EPOCHS           = 200\n",
    "BATCH_SIZE       = 16 #16, 32\n",
    "PATIENCE         = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aixnvSxAEfc0",
    "outputId": "c7c23065-d656-4686-a2a4-563ab8096963"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.4.0\n"
     ]
    }
   ],
   "source": [
    "import os, sys, math\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "from skimage import exposure\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import config as cf\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"Tensorflow version \" + tf.__version__)\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hBdsPW_tEkN9",
    "outputId": "1536ddbe-5437-4b4b-af79-c95a9d4ee306"
   },
   "outputs": [],
   "source": [
    "# Authenticate Google Drive\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "d67D5MGjEkQi"
   },
   "outputs": [],
   "source": [
    "# Authenticate Google Cloud\n",
    "#from google.colab import auth\n",
    "#auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "T4dgVWXFEm9L"
   },
   "outputs": [],
   "source": [
    "#GOOGLEDRIVE_DIRECTORY = os.path.join('/Volumes/GoogleDrive/My Drive/World Bank/IEs/Pakistan Poverty Estimation')\n",
    "#GOOGLEDRIVE_DIRECTORY = os.path.join('/content/gdrive/My Drive/World Bank/IEs/Pakistan Poverty Estimation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "083GuGrVFw_d"
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSigR3d-EpvJ"
   },
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "IhRluY8pErdJ"
   },
   "outputs": [],
   "source": [
    "# Get actual values function\n",
    "# https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset\n",
    "\n",
    "#### NTL Group\n",
    "def decode_fn_ntl_group(record_bytes):\n",
    "    return tf.io.parse_single_example(\n",
    "        # Data\n",
    "        record_bytes,\n",
    "\n",
    "        # Schema\n",
    "        {\"viirs_ntl_group\": tf.io.FixedLenFeature([], dtype=tf.int64)}\n",
    "    )\n",
    "\n",
    "def extract_ntl_group(TF_FILES):\n",
    "    actual_values = []\n",
    "    for batch in tf.data.TFRecordDataset([TF_FILES]).map(decode_fn_ntl_group):\n",
    "        value = batch['viirs_ntl_group'].numpy()\n",
    "        actual_values.append(value)\n",
    "\n",
    "    return actual_values\n",
    "\n",
    "#### UID\n",
    "def decode_fn_uid(record_bytes):\n",
    "    return tf.io.parse_single_example(\n",
    "        # Data\n",
    "        record_bytes,\n",
    "\n",
    "        # Schema\n",
    "        {\"uid\": tf.io.FixedLenFeature([], dtype=tf.string)}\n",
    "    )\n",
    "\n",
    "def extract_uid(TF_FILES):\n",
    "    actual_values = []\n",
    "    for batch in tf.data.TFRecordDataset([TF_FILES]).map(decode_fn_uid):\n",
    "        value = batch['uid'].numpy()\n",
    "        actual_values.append(value)\n",
    "\n",
    "    return actual_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_ZIHgcH7ErgF"
   },
   "outputs": [],
   "source": [
    "def dataset_to_numpy_util(dataset, N, process_image = True):\n",
    "    dataset = dataset.batch(N)\n",
    "    \n",
    "    for images, labels in dataset:\n",
    "        numpy_images = images.numpy()\n",
    "        numpy_labels = labels.numpy()\n",
    "\n",
    "        if process_image:\n",
    "            p2, p98 = np.percentile(numpy_images, (2,98))\n",
    "            numpy_images = exposure.rescale_intensity(numpy_images, in_range=(p2, p98)) \n",
    "        break;\n",
    "\n",
    "    return numpy_images, numpy_labels\n",
    "\n",
    "def display_one_image(image, title, subplot, red=False):\n",
    "    plt.subplot(subplot)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)\n",
    "    plt.title(title, fontsize=16, color='red' if red else 'black')\n",
    "    return subplot+1\n",
    "\n",
    "def display_9_images_from_dataset(dataset):\n",
    "    subplot=331\n",
    "    plt.figure(figsize=(13,13))\n",
    "    images, labels = dataset_to_numpy_util(dataset, 9)\n",
    "    for i, image in enumerate(images):\n",
    "        title = labels[i] # CLASSES[labels[i]]\n",
    "        subplot = display_one_image(image, title, subplot)\n",
    "        if i >= 8:\n",
    "            break;\n",
    "              \n",
    "    #plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "    plt.show()\n",
    "\n",
    "def display_training_curves(training, validation, title, subplot):\n",
    "    if subplot%10==1: # set up the subplots on the first call\n",
    "        plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n",
    "        #plt.tight_layout()\n",
    "    ax = plt.subplot(subplot)\n",
    "    ax.set_facecolor('#F8F8F8')\n",
    "    ax.plot(training)\n",
    "    ax.plot(validation)\n",
    "    ax.set_title('model '+ title)\n",
    "    ax.set_ylabel(title)\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.legend(['train', 'valid.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6FdImv2lG6sa"
   },
   "outputs": [],
   "source": [
    "## To extract uid & ntl_group\n",
    "def dataset_to_numpy_util_single_val(dataset, N):\n",
    "    dataset = dataset.batch(N)\n",
    "    \n",
    "    for val in dataset:\n",
    "        val = val.numpy()\n",
    "        break;\n",
    "\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "niVFllCzY-d6"
   },
   "outputs": [],
   "source": [
    "# https://www.geeksforgeeks.org/break-list-chunks-size-n-python/\n",
    "def divide_chunks(l, n):\n",
    "        \n",
    "    # looping till length l\n",
    "    for i in range(0, len(l), n): \n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjWMxaUZE6ll"
   },
   "source": [
    "### Functions for reading images and labels from TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "QMtjiV7TEriI"
   },
   "outputs": [],
   "source": [
    "def load_dataset(filenames, sat_var, exp_det, train):\n",
    "    # read from TFRecords. For optimal performance, read from multiple\n",
    "    # TFRecord files at once and set the option experimental_deterministic = False\n",
    "    # to allow order-altering optimizations.\n",
    "\n",
    "    #### Define read_tfrcord\n",
    "    # Define here. Later map over this function, and not sure how to\n",
    "    # enter sat_var into the mapping\n",
    "    def read_tfrecord(example, sat_var = sat_var):\n",
    "        features = {'viirs_ntl_group': tf.io.FixedLenFeature([], tf.int64),\n",
    "                    sat_var: tf.io.FixedLenFeature([], tf.string)}\n",
    "        parsed_features = tf.io.parse_single_example(example, features)\n",
    "\n",
    "        image = tf.io.decode_png(parsed_features[sat_var], dtype=tf.dtypes.uint16)\n",
    "        image = image / 10000 # within 0 and 1\n",
    "\n",
    "        if sat_var != 'b_rgb':\n",
    "            image = tf.repeat(image, repeats = 3, axis=2)\n",
    "\n",
    "        # If training sample, augment the data\n",
    "        if train:\n",
    "            image = tf.image.random_flip_left_right(image)\n",
    "            image = tf.image.random_flip_up_down(image)\n",
    "            image = tf.image.random_brightness(image, 0.025)\n",
    "\n",
    "            if sat_var == 'b_rgb':\n",
    "                image = tf.image.random_contrast(image, 0.5, 1.5)\n",
    "\n",
    "        label = tf.one_hot(parsed_features[\"viirs_ntl_group\"], NUM_GROUPS)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    #### load_dataset function\n",
    "    option_no_order = tf.data.Options()\n",
    "    option_no_order.experimental_deterministic = exp_det\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTOTUNE)\n",
    "    dataset = dataset.with_options(option_no_order)\n",
    "\n",
    "    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5XqDCjwuE_zt"
   },
   "outputs": [],
   "source": [
    "def read_tfrecord_uid(example):\n",
    "    features = {'uid': tf.io.FixedLenFeature([], tf.string)}\n",
    "\n",
    "    parsed_features = tf.io.parse_single_example(example, features)\n",
    "\n",
    "    return parsed_features['uid']\n",
    "\n",
    "def load_dataset_uid(filenames, exp_det):\n",
    "    # read from TFRecords. For optimal performance, read from multiple\n",
    "    # TFRecord files at once and set the option experimental_deterministic = False\n",
    "    # to allow order-altering optimizations.\n",
    "\n",
    "    option_no_order = tf.data.Options()\n",
    "    option_no_order.experimental_deterministic = exp_det\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTOTUNE)\n",
    "    dataset = dataset.with_options(option_no_order)\n",
    "    dataset = dataset.map(read_tfrecord_uid, num_parallel_calls=AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "nPym-f4fE_31"
   },
   "outputs": [],
   "source": [
    "def read_tfrecord_ntl_group(example):\n",
    "    features = {'viirs_ntl_group': tf.io.FixedLenFeature([], tf.int64)}\n",
    "    parsed_features = tf.io.parse_single_example(example, features)\n",
    "\n",
    "    label = tf.one_hot(parsed_features[\"viirs_ntl_group\"], NUM_GROUPS)\n",
    "\n",
    "    return label\n",
    "\n",
    "def load_dataset_ntl_group(filenames, exp_det):\n",
    "    # read from TFRecords. For optimal performance, read from multiple\n",
    "    # TFRecord files at once and set the option experimental_deterministic = False\n",
    "    # to allow order-altering optimizations.\n",
    "\n",
    "    option_no_order = tf.data.Options()\n",
    "    option_no_order.experimental_deterministic = exp_det\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTOTUNE)\n",
    "    dataset = dataset.with_options(option_no_order)\n",
    "    dataset = dataset.map(read_tfrecord_ntl_group, num_parallel_calls=AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8RQkl4MFsLk"
   },
   "source": [
    "### Functions to create batched datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "wspCMcDKFrjd"
   },
   "outputs": [],
   "source": [
    "def get_batched_dataset(filenames, sat_var, exp_det, train=False):\n",
    "    dataset = load_dataset(filenames, sat_var, exp_det = exp_det, train = train)\n",
    "    dataset = dataset.cache() # This dataset fits in RAM\n",
    "    if train:\n",
    "        # Best practices for Keras:\n",
    "        # Training dataset: repeat then batch\n",
    "        # Evaluation dataset: do not repeat\n",
    "        dataset = dataset.repeat()\n",
    "\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTOTUNE) # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    # should shuffle too but this dataset was well shuffled on disk already\n",
    "    return dataset\n",
    "    # source: Dataset performance guide: https://www.tensorflow.org/guide/performance/datasets\n",
    "\n",
    "def get_batched_dataset_uid(filenames, exp_det, train=False):\n",
    "    dataset = load_dataset_uid(filenames, exp_det = exp_det)\n",
    "    dataset = dataset.cache() # This dataset fits in RAM\n",
    "    if train:\n",
    "        # Best practices for Keras:\n",
    "        # Training dataset: repeat then batch\n",
    "        # Evaluation dataset: do not repeat\n",
    "        dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTOTUNE) # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    # should shuffle too but this dataset was well shuffled on disk already\n",
    "    return dataset\n",
    "    # source: Dataset performance guide: https://www.tensorflow.org/guide/performance/datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Nmao2iZFMo6"
   },
   "source": [
    "## Load TFRecords and divide into train/test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZaTeXk4rFRiM",
    "outputId": "6a0e34b9-f42d-4779-da1c-81fa0085bd02"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TF_PATH = os.path.join(cf.DROPBOX_DIRECTORY, 'Data', SURVEY_NAME, 'FinalData', 'Individual Datasets',\n",
    "                       'cnn_' + OUT_NAME_SUFFIX, 'tfrecords')\n",
    "GCS_PATTERN = os.path.join(TF_PATH, '*.tfrecord')\n",
    "\n",
    "#GCS_PATTERN = 'gs://ieconnectpovest/cnn_' + OUT_NAME_SUFFIX + '/tfrecords/*.tfrecord'\n",
    "all_filenames = tf.io.gfile.glob(GCS_PATTERN)\n",
    "len(all_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/robmarty/Dropbox/World Bank/IEs/Pakistan Poverty Estimation from Satellites/Data/DHS_nga_policy_experiment/FinalData/Individual Datasets/cnn_landsat_ntlharmon_underiaTrue/tfrecords/forcnn_test_NG_5_1_all.tfrecord',\n",
       " '/Users/robmarty/Dropbox/World Bank/IEs/Pakistan Poverty Estimation from Satellites/Data/DHS_nga_policy_experiment/FinalData/Individual Datasets/cnn_landsat_ntlharmon_underiaTrue/tfrecords/nocnn_NG_3_1_all.tfrecord',\n",
       " '/Users/robmarty/Dropbox/World Bank/IEs/Pakistan Poverty Estimation from Satellites/Data/DHS_nga_policy_experiment/FinalData/Individual Datasets/cnn_landsat_ntlharmon_underiaTrue/tfrecords/forcnn_test_NG_2_1_all.tfrecord',\n",
       " '/Users/robmarty/Dropbox/World Bank/IEs/Pakistan Poverty Estimation from Satellites/Data/DHS_nga_policy_experiment/FinalData/Individual Datasets/cnn_landsat_ntlharmon_underiaTrue/tfrecords/nocnn_NG_4_1_all.tfrecord',\n",
       " '/Users/robmarty/Dropbox/World Bank/IEs/Pakistan Poverty Estimation from Satellites/Data/DHS_nga_policy_experiment/FinalData/Individual Datasets/cnn_landsat_ntlharmon_underiaTrue/tfrecords/nocnn_NG_5_1_all.tfrecord',\n",
       " '/Users/robmarty/Dropbox/World Bank/IEs/Pakistan Poverty Estimation from Satellites/Data/DHS_nga_policy_experiment/FinalData/Individual Datasets/cnn_landsat_ntlharmon_underiaTrue/tfrecords/forcnn_test_NG_3_1_all.tfrecord',\n",
       " '/Users/robmarty/Dropbox/World Bank/IEs/Pakistan Poverty Estimation from Satellites/Data/DHS_nga_policy_experiment/FinalData/Individual Datasets/cnn_landsat_ntlharmon_underiaTrue/tfrecords/nocnn_NG_2_1_all.tfrecord',\n",
       " '/Users/robmarty/Dropbox/World Bank/IEs/Pakistan Poverty Estimation from Satellites/Data/DHS_nga_policy_experiment/FinalData/Individual Datasets/cnn_landsat_ntlharmon_underiaTrue/tfrecords/forcnn_test_NG_4_1_all.tfrecord',\n",
       " '/Users/robmarty/Dropbox/World Bank/IEs/Pakistan Poverty Estimation from Satellites/Data/DHS_nga_policy_experiment/FinalData/Individual Datasets/cnn_landsat_ntlharmon_underiaTrue/tfrecords/forcnn_test_NG_1_1_all.tfrecord',\n",
       " '/Users/robmarty/Dropbox/World Bank/IEs/Pakistan Poverty Estimation from Satellites/Data/DHS_nga_policy_experiment/FinalData/Individual Datasets/cnn_landsat_ntlharmon_underiaTrue/tfrecords/nocnn_NG_1_1_all.tfrecord',\n",
       " '/Users/robmarty/Dropbox/World Bank/IEs/Pakistan Poverty Estimation from Satellites/Data/DHS_nga_policy_experiment/FinalData/Individual Datasets/cnn_landsat_ntlharmon_underiaTrue/tfrecords/forcnn_train_NG_2_1_all.tfrecord',\n",
       " '/Users/robmarty/Dropbox/World Bank/IEs/Pakistan Poverty Estimation from Satellites/Data/DHS_nga_policy_experiment/FinalData/Individual Datasets/cnn_landsat_ntlharmon_underiaTrue/tfrecords/nocnn_NG_1_3_all.tfrecord',\n",
       " '/Users/robmarty/Dropbox/World Bank/IEs/Pakistan Poverty Estimation from Satellites/Data/DHS_nga_policy_experiment/FinalData/Individual Datasets/cnn_landsat_ntlharmon_underiaTrue/tfrecords/forcnn_train_NG_5_1_all.tfrecord',\n",
       " '/Users/robmarty/Dropbox/World Bank/IEs/Pakistan Poverty Estimation from Satellites/Data/DHS_nga_policy_experiment/FinalData/Individual Datasets/cnn_landsat_ntlharmon_underiaTrue/tfrecords/forcnn_train_NG_4_1_all.tfrecord',\n",
       " '/Users/robmarty/Dropbox/World Bank/IEs/Pakistan Poverty Estimation from Satellites/Data/DHS_nga_policy_experiment/FinalData/Individual Datasets/cnn_landsat_ntlharmon_underiaTrue/tfrecords/nocnn_NG_1_2_all.tfrecord',\n",
       " '/Users/robmarty/Dropbox/World Bank/IEs/Pakistan Poverty Estimation from Satellites/Data/DHS_nga_policy_experiment/FinalData/Individual Datasets/cnn_landsat_ntlharmon_underiaTrue/tfrecords/forcnn_train_NG_3_1_all.tfrecord',\n",
       " '/Users/robmarty/Dropbox/World Bank/IEs/Pakistan Poverty Estimation from Satellites/Data/DHS_nga_policy_experiment/FinalData/Individual Datasets/cnn_landsat_ntlharmon_underiaTrue/tfrecords/nocnn_NG_5_3_all.tfrecord',\n",
       " '/Users/robmarty/Dropbox/World Bank/IEs/Pakistan Poverty Estimation from Satellites/Data/DHS_nga_policy_experiment/FinalData/Individual Datasets/cnn_landsat_ntlharmon_underiaTrue/tfrecords/forcnn_train_NG_1_1_all.tfrecord',\n",
       " '/Users/robmarty/Dropbox/World Bank/IEs/Pakistan Poverty Estimation from Satellites/Data/DHS_nga_policy_experiment/FinalData/Individual Datasets/cnn_landsat_ntlharmon_underiaTrue/tfrecords/nocnn_NG_3_2_all.tfrecord',\n",
       " '/Users/robmarty/Dropbox/World Bank/IEs/Pakistan Poverty Estimation from Satellites/Data/DHS_nga_policy_experiment/FinalData/Individual Datasets/cnn_landsat_ntlharmon_underiaTrue/tfrecords/nocnn_NG_4_2_all.tfrecord',\n",
       " '/Users/robmarty/Dropbox/World Bank/IEs/Pakistan Poverty Estimation from Satellites/Data/DHS_nga_policy_experiment/FinalData/Individual Datasets/cnn_landsat_ntlharmon_underiaTrue/tfrecords/nocnn_NG_2_3_all.tfrecord',\n",
       " '/Users/robmarty/Dropbox/World Bank/IEs/Pakistan Poverty Estimation from Satellites/Data/DHS_nga_policy_experiment/FinalData/Individual Datasets/cnn_landsat_ntlharmon_underiaTrue/tfrecords/nocnn_NG_3_3_all.tfrecord',\n",
       " '/Users/robmarty/Dropbox/World Bank/IEs/Pakistan Poverty Estimation from Satellites/Data/DHS_nga_policy_experiment/FinalData/Individual Datasets/cnn_landsat_ntlharmon_underiaTrue/tfrecords/nocnn_NG_5_2_all.tfrecord',\n",
       " '/Users/robmarty/Dropbox/World Bank/IEs/Pakistan Poverty Estimation from Satellites/Data/DHS_nga_policy_experiment/FinalData/Individual Datasets/cnn_landsat_ntlharmon_underiaTrue/tfrecords/nocnn_NG_2_2_all.tfrecord',\n",
       " '/Users/robmarty/Dropbox/World Bank/IEs/Pakistan Poverty Estimation from Satellites/Data/DHS_nga_policy_experiment/FinalData/Individual Datasets/cnn_landsat_ntlharmon_underiaTrue/tfrecords/nocnn_NG_4_3_all.tfrecord']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "WK1cNuLWFRkW"
   },
   "outputs": [],
   "source": [
    "#forcnn_filenames = [x for x in all_filenames if 'forcnn_' in x]\n",
    "#split = int(len(forcnn_filenames) * VALIDATION_SPLIT)\n",
    "#training_filenames = forcnn_filenames[split:]\n",
    "#validation_filenames = forcnn_filenames[:split]\n",
    "\n",
    "# forcnn_filenames = [x for x in all_filenames if 'forcnn_' in x] # TODO: Not sure need?\n",
    "\n",
    "training_filenames = [x for x in all_filenames if 'forcnn_train_' in x]\n",
    "validation_filenames = [x for x in all_filenames if 'forcnn_test_' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVv4_7GhFRmm",
    "outputId": "281c9975-6df3-4ef9-faaf-e5c059a5915f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-27 09:27:31.344092: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-27 09:27:31.395050: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215\n",
      "435\n"
     ]
    }
   ],
   "source": [
    "#TOTAL_OBS = len(extract_uid(all_filenames))\n",
    "#print(TOTAL_OBS)\n",
    "\n",
    "TOTAL_OBS_VALIDATION = len(extract_uid(validation_filenames))\n",
    "print(TOTAL_OBS_VALIDATION)\n",
    "\n",
    "TOTAL_OBS_TRAINING = len(extract_uid(training_filenames))\n",
    "print(TOTAL_OBS_TRAINING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "k9kEz9YRFiAO"
   },
   "outputs": [],
   "source": [
    "validation_steps = TOTAL_OBS_VALIDATION // BATCH_SIZE\n",
    "steps_per_epoch  = TOTAL_OBS_TRAINING   // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYgQff2pF8pX"
   },
   "source": [
    "## Example Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fsbsj6FdF44c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b_rgb\n",
      "0\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: '/Users/robmarty/Dropbox/World Bank/IEs/Pakistan Poverty Estimation from Satellites/Data/DHS_nga_policy_experiment/FinalData/Individual Datasets/cnn_features/split_into_data_subsets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     features_i_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m dataset_to_numpy_util_single_val(load_dataset_uid(all_filenames_i, exp_det \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m),features_i_df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     33\u001b[0m     features_i_df\u001b[38;5;241m.\u001b[39mto_csv(FEATURES_PATH \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 36\u001b[0m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_dataset_exdtT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_filenames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFEATURES_PATH\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36mextract_features\u001b[0;34m(all_dataset_i_exdtT, all_filenames_i, i, FEATURES_PATH)\u001b[0m\n\u001b[1;32m     31\u001b[0m features_i_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(features_i)\u001b[38;5;241m.\u001b[39madd_prefix(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcnn_feat_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     32\u001b[0m features_i_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m dataset_to_numpy_util_single_val(load_dataset_uid(all_filenames_i, exp_det \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m),features_i_df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m \u001b[43mfeatures_i_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mFEATURES_PATH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py:3563\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3552\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[1;32m   3554\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3555\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m   3556\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3560\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[1;32m   3561\u001b[0m )\n\u001b[0;32m-> 3563\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mline_terminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mline_terminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3566\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3568\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3573\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3575\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3576\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3577\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3578\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3579\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3580\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/formats/format.py:1180\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1159\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1161\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[1;32m   1162\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[1;32m   1163\u001b[0m     line_terminator\u001b[38;5;241m=\u001b[39mline_terminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[1;32m   1179\u001b[0m )\n\u001b[0;32m-> 1180\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1183\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/formats/csvs.py:241\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    249\u001b[0m \n\u001b[1;32m    250\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[1;32m    252\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m    253\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline_terminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[1;32m    259\u001b[0m     )\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py:697\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[0;32m--> 697\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    701\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py:571\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    569\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[0;32m--> 571\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mfr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: '/Users/robmarty/Dropbox/World Bank/IEs/Pakistan Poverty Estimation from Satellites/Data/DHS_nga_policy_experiment/FinalData/Individual Datasets/cnn_features/split_into_data_subsets'"
     ]
    }
   ],
   "source": [
    "for sat_var in ['b_rgb', 'b_ndvi', 'b_bu']: # 'b_rgb', 'b_ndvi', 'b_bu'\n",
    "    \n",
    "    print(sat_var)\n",
    "    \n",
    "    # File names -----------------------------------------------------------------\n",
    "    # Paths for saving model, predictions (on test) and featurs (on training)\n",
    "    name_suffix = OUT_NAME_SUFFIX + \"_\" + sat_var\n",
    "\n",
    "    CNN_MODEL_PATH = os.path.join(cf.DROPBOX_DIRECTORY, 'Data', 'DHS', 'FinalData', \"Individual Datasets\",\n",
    "                                    'cnn_models', \n",
    "                                    'model_' + name_suffix + '.h5')\n",
    "\n",
    "    FEATURES_PATH = os.path.join(cf.DROPBOX_DIRECTORY, 'Data', SURVEY_NAME, 'FinalData', \"Individual Datasets\",\n",
    "                                    'cnn_features', \"split_into_data_subsets\",\n",
    "                                    'features_' + name_suffix)\n",
    "    \n",
    "    ## Load model ------------------------------------------------------------------\n",
    "    model = load_model(CNN_MODEL_PATH)\n",
    "\n",
    "    ## Load data -------------------------------------------------------------------\n",
    "    all_dataset_exdtT = get_batched_dataset(all_filenames, sat_var, exp_det = True, train=False)\n",
    "\n",
    "    ## Grab features ---------------------------------------------------------------\n",
    "    feature_extractor = Model(inputs=model.inputs,\n",
    "                    outputs=model.get_layer(name='fc1').output,)\n",
    "\n",
    "    def extract_features(all_dataset_i_exdtT, all_filenames_i, i, FEATURES_PATH):\n",
    "        print(i)\n",
    "\n",
    "        features_i = feature_extractor.predict(all_dataset_i_exdtT)\n",
    "        features_i_df = pd.DataFrame(features_i).add_prefix('cnn_feat_')\n",
    "        features_i_df['uid'] = dataset_to_numpy_util_single_val(load_dataset_uid(all_filenames_i, exp_det = True),features_i_df.shape[0])\n",
    "        features_i_df.to_csv(FEATURES_PATH + '_' + str(i) + '.csv', index=False)\n",
    "\n",
    "\n",
    "    extract_features(all_dataset_exdtT, all_filenames, 0, FEATURES_PATH)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WgzW_FeNkOBg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijdyyze7LLwC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "02_cnn_CLEAN_LOOP.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
