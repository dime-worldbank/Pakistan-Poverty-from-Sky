{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Predicting NTL using DTL__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "\n",
    "import os, datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re \n",
    "\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from keras import models\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model, Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow.keras as K\n",
    "\n",
    "import logging, os \n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seeds. Note that using a GPU can still introduce randomness.\n",
    "# (also not taking into account tensorflow randomness)\n",
    "seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    '''\n",
    "    Normalizes features.\n",
    "    '''\n",
    "    return X.astype('float32') / 255.0\n",
    "\n",
    "def define_model_imagenet(height, width, num_classes):\n",
    "    '''\n",
    "    Defines and compiles CNN model.\n",
    "    \n",
    "    Inputs:\n",
    "        height, width, channels, num_classes (int)\n",
    "    Returns:\n",
    "        model (keras.Model object)\n",
    "    '''\n",
    "\n",
    "    # https://medium.com/abraia/first-steps-with-transfer-learning-for-custom-image-classification-with-keras-b941601fcad5\n",
    "    # https://towardsdatascience.com/cnn-transfer-learning-fine-tuning-9f3e7c5806b2\n",
    "\n",
    "    #### Base model\n",
    "    input_shape = (height, width, 3)\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape, pooling = \"max\")\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    #### Model Customization\n",
    "    # We take the last layer of our the model and add it to our classifier\n",
    "    last = base_model.layers[-1].output\n",
    "    x = Flatten()(last)\n",
    "    x = Dense(100, activation='relu', name='fc1')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(num_classes, activation='softmax', name='predictions')(x)\n",
    "    model = Model(base_model.input, x)\n",
    "    # We compile the model\n",
    "    model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, trainX, trainY, testX, testY, CNN_MODEL_CHECKPOINT):\n",
    "    '''\n",
    "    Fits model, evaluates model, saves best model over epochs and cross-validations.\n",
    "    \n",
    "    Inputs:\n",
    "        model (CNN model) keras.Model object\n",
    "        trainX, trainY (numpy.ndarray) 4D array of DTL features and 2D array of targets for training\n",
    "        testX, testY (numpy.ndarray) 4D array of DTL features and 2D array of targets for testing\n",
    "        current_kfold (int) iteration in kfold cross-val, default=None for no cross-val\n",
    "        display_metrics (bool) Default=False\n",
    "    Returns:\n",
    "        None\n",
    "    # https://towardsdatascience.com/step-by-step-guide-to-using-pretrained-models-in-keras-c9097b647b29\n",
    "    '''\n",
    "\n",
    "    # Use early stopping to help with overfitting\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', patience=2, verbose=False)\n",
    "\n",
    "    # Save best model based on accuracy\n",
    "    mc = ModelCheckpoint(CNN_MODEL_CHECKPOINT, monitor='val_loss', mode='min', \n",
    "                         verbose=True, save_best_only=True)\n",
    "\n",
    "    # Fit model\n",
    "    history = model.fit(trainX, trainY, \n",
    "            epochs=50, \n",
    "            batch_size=32, \n",
    "            validation_data=(testX, testY), \n",
    "            callbacks=[es, mc], \n",
    "            verbose=False)\n",
    "\n",
    "    # Show accuracy\n",
    "    loss, accuracy = model.evaluate(testX, testY, verbose=False)\n",
    "    print(f'                              Accuracy: {accuracy}')\n",
    "\n",
    "    return history\n",
    "        \n",
    "def evaluate_with_crossval(model, dataX, dataY, k=2):\n",
    "    '''\n",
    "    Performs evaulation with K-fold cross validation.\n",
    "    \n",
    "    Inputs:\n",
    "        model (keras.Model object)\n",
    "        dataX, dataY (numpy.ndarray) 4D array of DTL features and 2D array of targets \n",
    "                                     for training\n",
    "        k (int)\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    # Define k-fold cross-val\n",
    "    kfold = KFold(k, shuffle=True, random_state=1)\n",
    "    # Loop through folds\n",
    "    count = 1\n",
    "    for train_idx, test_idx in kfold.split(dataX):\n",
    "        print(f'{datetime.datetime.now()}    --- Current K-fold: {count} ---')\n",
    "        # Select subsets for training and testing\n",
    "        trainX, trainY, testX, testY = dataX[train_idx], dataY[train_idx], \\\n",
    "                                       dataX[test_idx], dataY[test_idx]\n",
    "        # Pass to evaluate_model function\n",
    "        evaluate_model(model, trainX, trainY, testX, testY)\n",
    "        count += 1\n",
    "\n",
    "def display_eval_metrics(model, testX, testY, n_ntl_bins):\n",
    "    '''\n",
    "    Displays evaluation metrics for a given trained model.\n",
    "    '''\n",
    "    # Get predictions\n",
    "    predY = model.predict(testX)\n",
    "    predY = np.argmax(predY, axis = 1)\n",
    "    testY_bins = np.argmax(testY, axis = 1)\n",
    "    # Generate classification report\n",
    "    classes = ['Radiance Level %01d' %i for i in range(1,n_ntl_bins+1)]\n",
    "    print(classification_report(testY_bins, predY, target_names=classes))\n",
    "    \n",
    "def plot_model_accuracy_loss(model_fit):\n",
    "    '''\n",
    "    Plots the validated and training accuracy as well as loss function side by side.\n",
    "    \n",
    "    Inputs:\n",
    "        model_fit (model fit object) output from evaluate_model\n",
    "    Outputs: \n",
    "        None\n",
    "    '''\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(15, 5), ncols=2, sharey = False)\n",
    "    ax1.plot(model_fit.history['accuracy'])\n",
    "    ax1.plot(model_fit.history['val_accuracy'])\n",
    "    ax1.set_title(\"CNN Model Accuracy\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Accuracy\")\n",
    "    ax1.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "    ax2.plot(model_fit.history['loss'])\n",
    "    ax2.plot(model_fit.history['val_loss'])\n",
    "    ax2.set_title(\"CNN Model Loss\")\n",
    "    ax2.set_xlabel(\"Epochs\")\n",
    "    ax2.set_ylabel(\"Loss\")\n",
    "    ax2.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_model_architecture(model_object, path_to_graphviz_app, landscape = \"Vertical\"):\n",
    "    '''\n",
    "    Plots CNN model architecture in a visual diagram\n",
    "\n",
    "    Inputs:\n",
    "        model_object (CNN model) keras.Model object that comes from the output of define_model_imagenet or define_model_singlechannel. \n",
    "        path_to_graphviz_app (string) Local Pathway to the bin folder of graphviz application (https://graphviz.org/download/)\n",
    "        landscape (string) Pass in \"Vertical\" to display the CNN model architecture vertically or type any other string to dsiplay results horizontally\n",
    "    Returns:\n",
    "        plot object\n",
    "    '''\n",
    "    \n",
    "    os.environ[\"PATH\"] += os.pathsep + path_to_graphviz_app\n",
    "    if landscape == \"Vertical\":\n",
    "        plot = plot_model(model_object, show_shapes=True, show_layer_names=True)\n",
    "    else:\n",
    "        plot = plot_model(model_object, show_shapes=True, show_layer_names=True, rankdir='LR')\n",
    "    return plot\n",
    "    \n",
    "def split_train_test(DTL, NTL):\n",
    "    # SPLIT DATA INTO TRAINING AND TESTING\n",
    "    trainX, testX, raw_trainY, raw_testY = train_test_split(DTL, NTL, test_size=0.2)\n",
    "    \n",
    "    # PREP TRAINING AND TESTING DATA\n",
    "    trainY = to_categorical(raw_trainY)\n",
    "    testY = to_categorical(raw_testY)\n",
    "    \n",
    "    # PREP PIXELS IN FEATURES\n",
    "    trainX, testX = normalize(trainX), normalize(testX)\n",
    "    return (trainX, testX, trainY, testY)\n",
    "\n",
    "def make_predictions(testX, testY):\n",
    "    '''\n",
    "    Loads best model, makes predictions on testY using testX, saves predicted values to s3.\n",
    "    \n",
    "    Inputs:\n",
    "        testX\n",
    "        testY \n",
    "    Outputs: \n",
    "        None\n",
    "    '''\n",
    "    ## Save dataframe of predicted versus actual values\n",
    "\n",
    "    best_model = load_model(CNN_MODEL_CHECKPOINT)\n",
    "    \n",
    "    # Predict Values\n",
    "    predY = best_model.predict(testX) # model.predict(testX)\n",
    "    predY = np.argmax(predY, axis = 1)\n",
    "    testY_bins = np.argmax(testY, axis = 1)\n",
    "    \n",
    "    # Make Dataframe\n",
    "    results_df = pd.DataFrame({'predY': predY, 'testY': testY_bins})\n",
    "    \n",
    "    # Save locally\n",
    "    results_df.to_csv(os.path.join(LOCAL_DIR, PREDICTION_FILENAME), index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_height = 48\n",
    "image_width = 48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Models - VIIRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to CNN files\n",
    "import config as cf\n",
    "CNN_FOLDER = os.path.join(cf.GD_CNN_DIRECTORY, 'VIIRS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VIIRS (dependent variable)\n",
    "viirs = pd.read_pickle(os.path.join(CNN_FOLDER, 'dep_var.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of DTL files\n",
    "cnn_files = os.listdir(os.path.join(CNN_FOLDER))\n",
    "reg = re.compile(r'^dtl.*npy$')                   \n",
    "dtl_files = list(filter(reg.search, cnn_files)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 48, 48, 3)\n",
      "(200, 48, 48, 3)\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.20901, saving model to /Users/robmarty/Google Drive/World Bank/IEs/Pakistan Poverty Estimation/Data/CNN/VIIRS\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.20901 to 0.20619, saving model to /Users/robmarty/Google Drive/World Bank/IEs/Pakistan Poverty Estimation/Data/CNN/VIIRS\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.20619 to 0.19908, saving model to /Users/robmarty/Google Drive/World Bank/IEs/Pakistan Poverty Estimation/Data/CNN/VIIRS\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.19908\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.19908\n",
      "                              Accuracy: 0.9200000166893005\n"
     ]
    }
   ],
   "source": [
    "# Loop through files/parameters and estimate CNN model\n",
    "\n",
    "#for dtl_file_i = dtl_files:\n",
    "dtl_file_i = dtl_files[0]\n",
    "DTL = np.load(os.path.join(CNN_FOLDER, dtl_file_i))\n",
    "\n",
    "if DTL.shape[3] == 1:\n",
    "    DTL = np.repeat(DTL, 3, -1)\n",
    "    \n",
    "#for dep_var in ['median_rad_2014_5bin']:\n",
    "dep_var = 'median_rad_2014_5bin'\n",
    "   \n",
    "n_uniq_ntl_vals = viirs[dep_var].value_counts().shape[0]  \n",
    "\n",
    "DEPVAR = viirs[dep_var].to_numpy()\n",
    "\n",
    "if n_uniq_ntl_vals <= 10:\n",
    "    # Classification\n",
    "    model = define_model_imagenet(image_height, image_width, n_uniq_ntl_vals)\n",
    "else:\n",
    "    # Regression\n",
    "    # TODO: Define regression model\n",
    "\n",
    "trainX, testX, trainY, testY = split_train_test(DTL, DEPVAR)\n",
    "print(trainX.shape)\n",
    "print(testX.shape)\n",
    "\n",
    "# Evaluate Model\n",
    "history = evaluate_model(model, trainX, trainY, testX, testY, CNN_FOLDER)\n",
    "\n",
    "# Evaluation Stats\n",
    "display_eval_metrics(model, testX, testY, n_ntl_bins)\n",
    "plot_model_accuracy_loss(history)\n",
    "make_predictions(testX, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.functional.Functional at 0x7fb7145fbf28>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
