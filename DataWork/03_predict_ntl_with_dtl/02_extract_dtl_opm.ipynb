{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "innovative-technician",
   "metadata": {},
   "source": [
    "# Extract DTL Data to OPM\n",
    "\n",
    "Extracts numpy array of DTL around BISP coordinates and saves a pandas dataframe (pickled) of the UIDs corresponding to each numpy array. Processes data so that features can be extracted from the numpy array using the trained CNN model.\n",
    "\n",
    "TODO: Still need to get working with sagemaker -- (1) load raster tiles and (2) resolve issues installing geopandas and rasterio\n",
    "\n",
    "__Use the conda_python3 kernel; works better for install geopandas and rasterio__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explicit-private",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "municipal-shanghai",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes a while\n",
    "#%conda install geopandas\n",
    "#%conda install rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dress-buyer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, pickle, datetime, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "\n",
    "from geopandas import GeoDataFrame\n",
    "from shapely.geometry import Point\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (BaggingClassifier, AdaBoostClassifier, \n",
    "                              GradientBoostingClassifier, RandomForestClassifier)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import (accuracy_score, precision_score, \n",
    "                             recall_score, classification_report)\n",
    "#from keras.models import load_model\n",
    "\n",
    "import warnings\n",
    "import random\n",
    "#import tensorflow as tf\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "from s3fs.core import S3FileSystem \n",
    "s3 = S3FileSystem()\n",
    "role = get_execution_role()\n",
    "\n",
    "## User Defined\n",
    "import config as cf\n",
    "import feature_extraction as fe\n",
    "\n",
    "bucket = 'worldbank-pakistan-data'\n",
    "LOCAL_DIR = '/home/ec2-user/SageMaker/'\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-induction",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "postal-truck",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pd_to_gdp(df, lat_name = 'latitude', lon_name = 'longitude'):\n",
    "    '''\n",
    "    Converts a pandas dataframe with lat and long variables into\n",
    "    geopandas point data\n",
    "\n",
    "    Input:  df - pandas dataframe\n",
    "            lat_name - name of latitude variable in df\n",
    "            lon_name - name of longitude variable in df\n",
    "    Output: geopandas dataframe\n",
    "    '''\n",
    "\n",
    "    geometry = [Point(xy) for xy in zip(df[lon_name], df[lat_name])]\n",
    "    df = df.drop([lon_name, lat_name], axis=1)\n",
    "    gdf = GeoDataFrame(df, crs=\"EPSG:4326\", geometry=geometry)\n",
    "\n",
    "    return gdf\n",
    "\n",
    "def extract_dtl_opm(bands_type, year):\n",
    "\n",
    "    # 0. Load Bands\n",
    "    if bands_type == \"RGB\":\n",
    "        bands = ['4', '3', '2']\n",
    "\n",
    "    # 1. Load Grid for DTL Tiles\n",
    "    dtl_tiles = gpd.read_file(s3.open('{}/{}'.format(bucket, os.path.join('Country Grid', 'FinalData', 'pak_grid_200km.geojson'))))\n",
    "    #dtl_tiles = gpd.read_file(os.path.join(cf.DROPBOX_DIRECTORY, 'Data', 'Country Grid', 'FinalData', 'pak_grid_200km.geojson'))\n",
    "    dtl_tiles.rename(columns = {'id': 'tile_id'}, inplace=True)\n",
    "\n",
    "    # 2. Prep BISP data\n",
    "    # Load, convert to geopandas, extract dtl tile\n",
    "    bisp_df = pd.read_csv(s3.open('{}/{}'.format(bucket, os.path.join('OPM', 'FinalData - PII', 'GPS_uid_crosswalk.csv'))), engine = 'python')\n",
    "    #bisp_df = pd.read_csv(os.path.join(cf.SECURE_DATA_DIRECTORY, 'Data', 'BISP', 'FinalData - PII', 'GPS_uid_crosswalk.csv'), engine = 'python')\n",
    "    bisp_gdf = pd_to_gdp(bisp_df)\n",
    "    bisp_gdf = gpd.sjoin(bisp_gdf, dtl_tiles, how=\"inner\", op='intersects').reset_index(drop=True)\n",
    "    bisp_gdf['geometry'] = bisp_gdf.buffer(distance = 0.75/111.12).envelope\n",
    "    \n",
    "    # 3. Extract DTL to BISP Coordinates\n",
    "\n",
    "    # Load CNN parameters \n",
    "    # Use this for paramers; just effects img_height and img_weidth\n",
    "    param_name = \"Nbands3_nNtlBins3_minNTLbinCount16861\"\n",
    "    PARAM_PATH_JSON = os.path.join('CNN', param_name, 'CNN_parameters.json')\n",
    "\n",
    "    content_object = boto3.resource('s3').Object(bucket, PARAM_PATH_JSON)\n",
    "    file_content = content_object.get()['Body'].read().decode('utf-8')\n",
    "    cnn_param_dict = json.loads(file_content)\n",
    "    \n",
    "    #with open(PARAM_PATH_JSON, 'r') as fp:\n",
    "    #    cnn_param_dict = json.load(fp)\n",
    "\n",
    "    # Extract\n",
    "    DTL, bisp_gdf_processed = fe.map_DTL_NTL(input_gdf = bisp_gdf, \n",
    "                                        directory = os.path.join('Landsat', 'l8', str(year)), \n",
    "                                        bands = bands, \n",
    "                                        img_height = cnn_param_dict['image_height'], \n",
    "                                        img_width = cnn_param_dict['image_width'],\n",
    "                                        year = year)\n",
    "\n",
    "    bisp_gdf_processed = bisp_gdf_processed[['uid']]\n",
    "\n",
    "    # 4. Export\n",
    "    np.save(os.path.join(LOCAL_DIR, 'bisp_dtl_bands' + bands_type + \"_\" + str(year) + '.npy'), DTL)\n",
    "    bisp_gdf_processed.to_pickle(os.path.join(LOCAL_DIR, 'bisp_dtl_uids_bands' + bands_type + \"_\" + str(year) + '.pkl'))\n",
    "\n",
    "    boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join('OPM', 'FinalData', 'Individual Datasets', 'bisp_dtl_bands' + bands_type + \"_\" + str(year) + '.npy')).upload_file(os.path.join(LOCAL_DIR, 'bisp_dtl_bands' + bands_type + \"_\" + str(year) + '.npy'))\n",
    "    boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join('OPM', 'FinalData', 'Individual Datasets', 'bisp_dtl_uids_bands' + bands_type + \"_\" + str(year) + '.pkl')).upload_file(os.path.join(LOCAL_DIR, 'bisp_dtl_uids_bands' + bands_type + \"_\" + str(year) + '.pkl'))\n",
    "\n",
    "    #np.save(os.path.join(        cf.DROPBOX_DIRECTORY, 'Data', 'BISP' , 'FinalData', 'Individual Datasets', 'bisp_dtl_bands' + bands_type + \"_\" + str(year) + '.npy'), DTL)\n",
    "    #bisp_gdf_processed.to_pickle(os.path.join(cf.DROPBOX_DIRECTORY, 'Data', 'BISP' , 'FinalData', 'Individual Datasets', 'bisp_dtl_uids_bands' + bands_type + \"_\" + str(year) + '.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monthly-vegetable",
   "metadata": {},
   "source": [
    "## Extract Daytime Imagery to OPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "headed-enforcement",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CNN_PARAMS_FILENAME' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7b21d8995b07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mextract_dtl_opm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2014\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-32eac1ac3c76>\u001b[0m in \u001b[0;36mextract_dtl_opm\u001b[0;34m(bands_type, year)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mPARAM_PATH_JSON\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CNN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CNN_parameters.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mcontent_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboto3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m's3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCNN_PARAMS_FILENAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mfile_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontent_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mcnn_param_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CNN_PARAMS_FILENAME' is not defined"
     ]
    }
   ],
   "source": [
    "extract_dtl_opm(\"RGB\", 2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "lined-volleyball",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'worldbank-pakistan-data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "appointed-pharmaceutical",
   "metadata": {},
   "outputs": [],
   "source": [
    "bisp_df = rasterio.open(s3.open('{}/{}'.format(bucket, os.path.join('Landsat', 'l8', '2014', 'l8_2014_tile1_b1.tif'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "parental-third",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Landsat/l8/2014/l8_2014_tile1_b1.tif'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join('Landsat', 'l8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "difficult-pledge",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-validity",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
