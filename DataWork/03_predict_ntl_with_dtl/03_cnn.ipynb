{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z34KtcX5Pp7o"
   },
   "source": [
    "# __Predicting NTL using DTL__\n",
    "\n",
    "CNN Model for predicting nighttime lights using daytime images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybUBTyiNDore"
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UOdgNXFG8EmH"
   },
   "outputs": [],
   "source": [
    "#PARAM_NAME = \"Nbands3_nNtlBins3_minNTLbinCount16861\"\n",
    "PARAM_NAME = \"Nbands3_nNtlBins3_minNTLbinCount100\"\n",
    "YEAR = 2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vds1yHMwLJp2"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "\n",
    "import os, datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, GlobalAveragePooling2D, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "import logging, os \n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "from s3fs.core import S3FileSystem \n",
    "s3 = S3FileSystem()\n",
    "role = get_execution_role()\n",
    "\n",
    "# Set seeds. Note that using a GPU can still introduce randomness.\n",
    "# (also not taking into account tensorflow randomness)\n",
    "seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'worldbank-pakistan-data'\n",
    "\n",
    "#### CNN directry in s3 bucket with data\n",
    "CNN_DIR = os.path.join('Rus/CNN', PARAM_NAME)\n",
    "LOCAL_DIR = '/home/ec2-user/SageMaker/'\n",
    "\n",
    "#### FILES ALREADY CREATED\n",
    "\n",
    "# JSON file with parameters for CNN\n",
    "CNN_PARAMS_FILENAME = os.path.join(CNN_DIR, 'CNN_parameters.json')\n",
    "\n",
    "# Nighttime lights and daytime imagery path (numpy files prepped for CNN)\n",
    "NTL_FILENAME = os.path.join(CNN_DIR, f'ntl_{str(YEAR)}.npy')\n",
    "DTL_FILENAME = os.path.join(CNN_DIR, f'dtl_{str(YEAR)}.npy')\n",
    "\n",
    "#### FILES TO CREATE\n",
    "\n",
    "# CNN model (h5 file) \n",
    "# -- 1. Name\n",
    "# -- 2. Checkpoint (where to store model locally; cnn model saves best model durig training)\n",
    "# -- 3. s3 path, where to upload back to s3 bucket\n",
    "CNN_MODEL_NAME = f'script_CNN_{str(YEAR)}_rus.h5'\n",
    "CNN_MODEL_CHECKPOINT = os.path.join(LOCAL_DIR, CNN_MODEL_NAME)\n",
    "CNN_MODEL_S3_PATH = os.path.join(CNN_DIR, CNN_MODEL_NAME)\n",
    "\n",
    "# CSV file with predicted NTL values from CNN (upload to s3)\n",
    "PREDICTION_FILENAME = f'cnn_predictions_truth_values_{str(YEAR)}_rus.csv'\n",
    "#PREDICTION_FILENAME = os.path.join(CNN_DIR, f'cnn_predictions_truth_values_{str(YEAR)}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5k-bEJvLVvj"
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fkAYWJCg0gut"
   },
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    '''\n",
    "    Normalizes features.\n",
    "    '''\n",
    "    return X.astype('float32') / 255.0\n",
    "\n",
    "def define_model_imagenet(height, width, channels, num_classes):\n",
    "    '''\n",
    "    Defines and compiles CNN model.\n",
    "    \n",
    "    Inputs:\n",
    "        height, width, channels, num_classes (int)\n",
    "    Returns:\n",
    "        model (keras.Model object)\n",
    "    '''\n",
    "\n",
    "    # https://medium.com/abraia/first-steps-with-transfer-learning-for-custom-image-classification-with-keras-b941601fcad5\n",
    "    # https://towardsdatascience.com/cnn-transfer-learning-fine-tuning-9f3e7c5806b2\n",
    "\n",
    "    #### Base model\n",
    "    input_shape = (height, width, channels)\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    #### Model Customization\n",
    "    # We take the last layer of our the model and add it to our classifier\n",
    "    last = base_model.layers[-1].output\n",
    "    x = Flatten()(last)\n",
    "    x = Dense(100, activation='relu', name='fc1')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(num_classes, activation='softmax', name='predictions')(x)\n",
    "    model = Model(base_model.input, x)\n",
    "    # We compile the model\n",
    "    model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, trainX, trainY, testX, testY, CNN_MODEL_CHECKPOINT):\n",
    "    '''\n",
    "    Fits model, evaluates model, saves best model over epochs and cross-validations.\n",
    "    \n",
    "    Inputs:\n",
    "        model (CNN model) keras.Model object\n",
    "        trainX, trainY (numpy.ndarray) 4D array of DTL features and 2D array of targets for training\n",
    "        testX, testY (numpy.ndarray) 4D array of DTL features and 2D array of targets for testing\n",
    "        current_kfold (int) iteration in kfold cross-val, default=None for no cross-val\n",
    "        display_metrics (bool) Default=False\n",
    "    Returns:\n",
    "        None\n",
    "    # https://towardsdatascience.com/step-by-step-guide-to-using-pretrained-models-in-keras-c9097b647b29\n",
    "    '''\n",
    "\n",
    "    # Use early stopping to help with overfitting\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', patience=10, verbose=False)\n",
    "\n",
    "    # Save best model based on accuracy\n",
    "    mc = ModelCheckpoint(CNN_MODEL_CHECKPOINT, monitor='val_loss', mode='min', \n",
    "                         verbose=True, save_best_only=True)\n",
    "\n",
    "    # Fit model\n",
    "    model.fit(trainX, trainY, \n",
    "            epochs=100, \n",
    "            batch_size=500, \n",
    "            validation_data=(testX, testY), \n",
    "            callbacks=[es, mc], \n",
    "            verbose=False)\n",
    "\n",
    "    # Show accuracy\n",
    "    loss, accuracy = model.evaluate(testX, testY, verbose=False)\n",
    "    print(f'                              Accuracy: {accuracy}')\n",
    "\n",
    "    #return model\n",
    "        \n",
    "def evaluate_with_crossval(model, dataX, dataY, k=2):\n",
    "    '''\n",
    "    Performs evaulation with K-fold cross validation.\n",
    "    \n",
    "    Inputs:\n",
    "        model (keras.Model object)\n",
    "        dataX, dataY (numpy.ndarray) 4D array of DTL features and 2D array of targets \n",
    "                                     for training\n",
    "        k (int)\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    # Define k-fold cross-val\n",
    "    kfold = KFold(k, shuffle=True, random_state=1)\n",
    "    # Loop through folds\n",
    "    count = 1\n",
    "    for train_idx, test_idx in kfold.split(dataX):\n",
    "        print(f'{datetime.datetime.now()}    --- Current K-fold: {count} ---')\n",
    "        # Select subsets for training and testing\n",
    "        trainX, trainY, testX, testY = dataX[train_idx], dataY[train_idx], \\\n",
    "                                       dataX[test_idx], dataY[test_idx]\n",
    "        # Pass to evaluate_model function\n",
    "        evaluate_model(model, trainX, trainY, testX, testY)\n",
    "        count += 1\n",
    "\n",
    "def display_eval_metrics(model, testX, testY, n_ntl_bins):\n",
    "    '''\n",
    "    Displays evaluation metrics for a given trained model.\n",
    "    '''\n",
    "    # Get predictions\n",
    "    predY = model.predict(testX)\n",
    "    predY = np.argmax(predY, axis = 1)\n",
    "    testY_bins = np.argmax(testY, axis = 1)\n",
    "    # Generate classification report\n",
    "    classes = ['Radiance Level %01d' %i for i in range(1,n_ntl_bins+1)]\n",
    "    print(classification_report(testY_bins, predY, target_names=classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4lWj5moLdQW"
   },
   "source": [
    "## Load Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "S7mQb5uJ6BEq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_height': 48,\n",
       " 'image_width': 48,\n",
       " 'bands': ['4', '3', '2'],\n",
       " 'N_bands': 3,\n",
       " 'n_ntl_bins': 3,\n",
       " 'min_ntl_bin_count': 100}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load json from s3\n",
    "#s3 = boto3.resource('s3')\n",
    "content_object = boto3.resource('s3').Object(bucket, CNN_PARAMS_FILENAME)\n",
    "file_content = content_object.get()['Body'].read().decode('utf-8')\n",
    "cnn_param_dict = json.loads(file_content)\n",
    "cnn_param_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab parameters\n",
    "N_bands = cnn_param_dict['N_bands']\n",
    "n_ntl_bins = cnn_param_dict['n_ntl_bins']\n",
    "image_height = cnn_param_dict['image_height']\n",
    "image_width = cnn_param_dict['image_width']\n",
    "bands = cnn_param_dict['bands']\n",
    "min_ntl_bin_count = cnn_param_dict['bands']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZ0rFqCiLhM0"
   },
   "source": [
    "## Load and Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "vuCKwuARKt_E"
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "NTL = np.load(s3.open('{}/{}'.format(bucket, NTL_FILENAME)))\n",
    "DTL = np.load(s3.open('{}/{}'.format(bucket, DTL_FILENAME)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT DATA INTO TRAINING AND TESTING\n",
    "trainX, testX, raw_trainY, raw_testY = train_test_split(DTL, NTL, test_size=0.2)\n",
    "\n",
    "# PREP TRAINING AND TESTING DATA\n",
    "trainY = to_categorical(raw_trainY)\n",
    "testY = to_categorical(raw_testY)\n",
    "\n",
    "# PREP PIXELS IN FEATURES\n",
    "trainX, testX = normalize(trainX), normalize(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 48, 48, 3)\n",
      "(60, 48, 48, 3)\n"
     ]
    }
   ],
   "source": [
    "print(trainX.shape)\n",
    "print(testX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (image_height, image_width, N_bands)\n",
    "num_classes = n_ntl_bins\n",
    "input_shape\n",
    "from keras.applications.vgg19 import VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "80142336/80134624 [==============================] - 1s 0us/step\n",
      "Model: \"vgg19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_17 (InputLayer)        [(None, 48, 48, 3)]       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 48, 48, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 48, 48, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 24, 24, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 24, 24, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 12, 12, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 12, 12, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 12, 12, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 12, 12, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 6, 6, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 3, 3, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 3, 3, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 3, 3, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 3, 3, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_5 ( (None, 512)               0         \n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 20,024,384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "(None, 512)\n"
     ]
    }
   ],
   "source": [
    "base_model = VGG19(weights='imagenet', include_top=False, pooling = \"avg\", input_shape=input_shape)\n",
    "print(base_model.summary())\n",
    "for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "#for layer in base_model.layers:\n",
    "    #if layer.name == 'block5_conv1':\n",
    "        #break\n",
    "    #layer.trainable = False\n",
    "    #print('Layer ' + layer.name + ' frozen.')\n",
    "\n",
    "#### Model Customization\n",
    "# We take the last layer of our the model and add it to our classifier\n",
    "last = base_model.layers[-1].output\n",
    "print(last.shape)\n",
    "x = Flatten()(last)\n",
    "x = Dense(1000, activation='relu', name='fc1')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "#x = Dense(1000, activation='relu', name='fc1')(x)\n",
    "#x = Dropout(0.3)(x)\n",
    "#x = Dense(500, activation='relu', name='fc2')(x)\n",
    "#x = Dropout(0.3)(x)\n",
    "x = Dense(64, activation='relu', name='fc2')(x)\n",
    "#x = Dropout(0.3)(x)\n",
    "#x = BatchNormalization()\n",
    "x = Dense(num_classes, activation='softmax', name='predictions')(x)\n",
    "model = Model(base_model.input, x)\n",
    "model.compile(optimizer='rmsprop', # rmsprop\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.10421, saving model to /home/ec2-user/SageMaker/script_CNN_2014_rus.h5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 3.10421\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.10421 to 1.00873, saving model to /home/ec2-user/SageMaker/script_CNN_2014_rus.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00873 to 0.93748, saving model to /home/ec2-user/SageMaker/script_CNN_2014_rus.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.93748 to 0.92488, saving model to /home/ec2-user/SageMaker/script_CNN_2014_rus.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.92488 to 0.90620, saving model to /home/ec2-user/SageMaker/script_CNN_2014_rus.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.90620\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.90620 to 0.89281, saving model to /home/ec2-user/SageMaker/script_CNN_2014_rus.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.89281\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.89281\n",
      " Accuracy: 0.6166666746139526\n"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', patience=2, verbose=False)\n",
    "\n",
    "# Save best model based on accuracy\n",
    "mc = ModelCheckpoint(CNN_MODEL_CHECKPOINT, monitor='val_loss', mode='min', verbose=True, save_best_only=True)\n",
    "\n",
    "# Fit model\n",
    "model.fit(trainX, trainY, \n",
    "          epochs=100, \n",
    "          batch_size=500, \n",
    "          validation_data=(testX, testY), \n",
    "          callbacks=[es, mc], \n",
    "          verbose=False)\n",
    "\n",
    "# Show accuracy\n",
    "loss, accuracy = model.evaluate(testX, testY, verbose=False)\n",
    "print(f' Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Radiance Level 1       0.67      0.64      0.65        22\n",
      "Radiance Level 2       0.53      0.62      0.57        16\n",
      "Radiance Level 3       0.60      0.55      0.57        22\n",
      "\n",
      "        accuracy                           0.60        60\n",
      "       macro avg       0.60      0.60      0.60        60\n",
      "    weighted avg       0.60      0.60      0.60        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_eval_metrics(model, testX, testY, n_ntl_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-10 22:51:52.430434    --- Current K-fold: 1 ---\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.70974, saving model to /home/ec2-user/SageMaker/script_CNN_2014_rus.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.70974 to 0.70544, saving model to /home/ec2-user/SageMaker/script_CNN_2014_rus.h5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.70544\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.70544\n",
      " Accuracy: 0.699543297290802\n",
      "2021-05-10 22:54:33.556336    --- Current K-fold: 2 ---\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.65866, saving model to /home/ec2-user/SageMaker/script_CNN_2014_rus.h5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.65866\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.65866\n",
      " Accuracy: 0.7173951864242554\n",
      "2021-05-10 22:56:39.518852    --- Current K-fold: 3 ---\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.65393, saving model to /home/ec2-user/SageMaker/script_CNN_2014_rus.h5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.65393\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.65393\n",
      " Accuracy: 0.7262914180755615\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(3, shuffle=True, random_state=1)\n",
    "# Loop through folds\n",
    "count = 1\n",
    "for train_idx, test_idx in kfold.split(DTL):\n",
    "    print(f'{datetime.datetime.now()}    --- Current K-fold: {count} ---')\n",
    "    # Select subsets for training and testing\n",
    "    trainX, trainY, testX, testY = DTL[train_idx], NTL[train_idx], DTL[test_idx], NTL[test_idx]\n",
    "    \n",
    "    # PREP TRAINING AND TESTING DATA\n",
    "    trainY = to_categorical(trainY)\n",
    "    testY = to_categorical(testY)\n",
    "\n",
    "    # PREP PIXELS IN FEATURES\n",
    "    trainX, testX = normalize(trainX), normalize(testX)\n",
    "    \n",
    "    # Pass to evaluate_model function\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', patience=2, verbose=False)\n",
    "    # Save best model based on accuracy\n",
    "    mc = ModelCheckpoint(CNN_MODEL_CHECKPOINT, monitor='val_loss', mode='min', verbose=True, save_best_only=True)\n",
    "\n",
    "    # Fit model\n",
    "    model.fit(trainX, trainY, \n",
    "              epochs=100, \n",
    "              batch_size=500, \n",
    "              validation_data=(testX, testY), \n",
    "              callbacks=[es, mc], \n",
    "              verbose=False)\n",
    "\n",
    "    # Show accuracy\n",
    "    loss, accuracy = model.evaluate(testX, testY, verbose=False)\n",
    "    print(f' Accuracy: {accuracy}')\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKlWDX3gLkde"
   },
   "source": [
    "## Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "icaSC1870guy"
   },
   "outputs": [],
   "source": [
    "model = define_model_imagenet(image_height, image_width, N_bands, n_ntl_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DTt_30BEMrss",
    "outputId": "71355b8c-4ec6-43c9-9850-d5b01086ddbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.18979, saving model to /home/ec2-user/SageMaker/script_CNN_2014_rus.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.18979 to 1.05734, saving model to /home/ec2-user/SageMaker/script_CNN_2014_rus.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.05734 to 1.00073, saving model to /home/ec2-user/SageMaker/script_CNN_2014_rus.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00073 to 0.98679, saving model to /home/ec2-user/SageMaker/script_CNN_2014_rus.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.98679\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.98679 to 0.96472, saving model to /home/ec2-user/SageMaker/script_CNN_2014_rus.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.96472 to 0.96412, saving model to /home/ec2-user/SageMaker/script_CNN_2014_rus.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.96412 to 0.95736, saving model to /home/ec2-user/SageMaker/script_CNN_2014_rus.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.95736\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.95736 to 0.94776, saving model to /home/ec2-user/SageMaker/script_CNN_2014_rus.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.94776 to 0.94449, saving model to /home/ec2-user/SageMaker/script_CNN_2014_rus.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.94449 to 0.94235, saving model to /home/ec2-user/SageMaker/script_CNN_2014_rus.h5\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.94235\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.94235 to 0.93679, saving model to /home/ec2-user/SageMaker/script_CNN_2014_rus.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.93679\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.93679\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.93679\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.93679\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.93679\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.93679\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.93679\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.93679\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.93679\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.93679\n",
      "                              Accuracy: 0.5333333611488342\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, trainX, trainY, testX, testY, CNN_MODEL_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0akMTs7NK4iJ",
    "outputId": "d5c373b4-e22c-4191-c7c1-e1ec29593311"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Radiance Level 1       0.74      0.78      0.76      5572\n",
      "Radiance Level 2       0.61      0.73      0.67      5573\n",
      "Radiance Level 3       0.72      0.55      0.63      5716\n",
      "\n",
      "        accuracy                           0.69     16861\n",
      "       macro avg       0.69      0.69      0.68     16861\n",
      "    weighted avg       0.69      0.69      0.68     16861\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DISPLAY IN-DEPTH EVALUTAION METRICS\n",
    "best_model = load_model(CNN_MODEL_CHECKPOINT)\n",
    "display_eval_metrics(model, testX, testY, n_ntl_bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_smbAxFWvWrP"
   },
   "source": [
    "## Save Best Model and Predicted Values to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "7VYvJlW58z-M"
   },
   "outputs": [],
   "source": [
    "## Save predicted values to s3\n",
    "# TODO: I save file locally then send to s3; might be a way to send df directly to s3\n",
    "\n",
    "# Predict Values\n",
    "predY = best_model.predict(testX) # model.predict(testX)\n",
    "predY = np.argmax(predY, axis = 1)\n",
    "testY_bins = np.argmax(testY, axis = 1)\n",
    "\n",
    "# Make Dataframe\n",
    "results_df = pd.DataFrame({'predY': predY, 'testY': testY_bins})\n",
    "\n",
    "# Save locally\n",
    "results_df.to_csv(os.path.join(LOCAL_DIR, PREDICTION_FILENAME), index=False) \n",
    "\n",
    "# Send to s3\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(CNN_DIR, PREDICTION_FILENAME)).upload_file(os.path.join(LOCAL_DIR, PREDICTION_FILENAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "YUJ79Dvkvb-3"
   },
   "outputs": [],
   "source": [
    "## Save best model to s3\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(CNN_MODEL_S3_PATH).upload_file(CNN_MODEL_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "OIaYzmPcvjTS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ABUz9Hk8wPmd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "01_cnn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow2_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
