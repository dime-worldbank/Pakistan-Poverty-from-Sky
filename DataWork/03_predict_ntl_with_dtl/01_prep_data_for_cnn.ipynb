{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sufficient-lawrence",
   "metadata": {},
   "source": [
    "# Prepare Data for CNN\n",
    "\n",
    "Prepares data for CNN. \n",
    "1. Outputs numpy arrays of DTL values and NTL labels\n",
    "2. Creates parameter dictionary (eg, number of NTL labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-fighter",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "north-gravity",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Libraries ###\n",
    "import os, datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import logging, os \n",
    "\n",
    "### User Defined Libraries ###\n",
    "import config as cf\n",
    "import feature_extraction as fe\n",
    "\n",
    "### Set Seeds ###\n",
    "seed_value = 42\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "### Parameters / Paths ###\n",
    "FINAL_TARGET_NAME = 'ntl_bins'\n",
    "#VIIRS_GDF_FILEPATH = cf.VIIRS_GDF_FILEPATH\n",
    "#DTL_DIRECTORY = cf.DTL_DIRECTORY\n",
    "\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "from s3fs.core import S3FileSystem \n",
    "s3 = S3FileSystem()\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket = 'worldbank-pakistan-data'\n",
    "LOCAL_DIR = '/home/ec2-user/SageMaker/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acquired-syntax",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-conflict",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_target(gdf, orig_target_name, n_bins):\n",
    "    '''\n",
    "    Creates log NTL variable and bins into 5 classes using k-means clutering.\n",
    "    '''\n",
    "    # Perform log(x+1) for defined domain\n",
    "    transformed_target_name = f'log_{orig_target_name}'\n",
    "    gdf[transformed_target_name] = np.log(gdf[orig_target_name] + 1)\n",
    "    # Bin target\n",
    "    target = gdf[transformed_target_name].to_numpy().reshape(-1,1)\n",
    "    discretizer = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='kmeans')\n",
    "    gdf[FINAL_TARGET_NAME] = discretizer.fit_transform(target)\n",
    "\n",
    "def sample_by_target(input_gdf, target_col_name, n):\n",
    "    '''\n",
    "    Create a sample dataframe containing n observations from each target bin.\n",
    "    '''\n",
    "\n",
    "    gdf = gpd.GeoDataFrame()\n",
    "    for x in input_gdf[target_col_name].unique():\n",
    "        bin_gdf = input_gdf[input_gdf[target_col_name] == x]\n",
    "        sample_gdf = bin_gdf.sample(n=n, random_state=1)\n",
    "        gdf = gdf.append(sample_gdf)\n",
    "    return gdf\n",
    "\n",
    "def normalize(X):\n",
    "    '''\n",
    "    Normalizes features.\n",
    "    '''\n",
    "    return X.astype('float32') / 255.0\n",
    "\n",
    "def prep_cnn_data(bands, n_ntl_bins, min_ntl_bin_count, year):\n",
    "\n",
    "    # PARAMETERS -------------------------------------------------------------\n",
    "\n",
    "    ## Define Parameters\n",
    "    # Daytime image parameters\n",
    "    image_height = 48 # VGG16 needs images to be rescale to 224x224\n",
    "    image_width = 48\n",
    "    N_bands = len(bands)\n",
    "\n",
    "    ## Save parameters for later use\n",
    "    cnn_param_dict = {'image_height': image_height, \n",
    "                    'image_width': image_width,\n",
    "                    'bands': bands,\n",
    "                    'N_bands': N_bands,\n",
    "                    'n_ntl_bins': n_ntl_bins,\n",
    "                    'min_ntl_bin_count': min_ntl_bin_count}\n",
    "\n",
    "    # Make directory for these parameters\n",
    "    params_str = 'Nbands' + str(N_bands) + \"_nNtlBins\" + str(n_ntl_bins) + \"_minNTLbinCount\" + str(min_ntl_bin_count)\n",
    "\n",
    "    # Save Locally\n",
    "    with open(os.path.join(LOCAL_DIR, 'CNN_parameters.json'), 'w') as fp:\n",
    "        json.dump(cnn_param_dict, fp)\n",
    "        \n",
    "    # Send to s3\n",
    "    boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join('CNN', params_str, 'CNN_parameters.json')).upload_file(os.path.join(LOCAL_DIR, 'CNN_parameters.json'))\n",
    "\n",
    "    # Run --------------------------------------------------------------------\n",
    "\n",
    "    ## LOAD DATA\n",
    "    viirs = pd.read_pickle(VIIRS_GDF_FILEPATH)\n",
    "    viirs_gdf = gpd.GeoDataFrame(viirs, geometry='geometry')\n",
    "    viirs_gdf = viirs_gdf[ ~ np.isnan(viirs_gdf['tile_id'])]\n",
    "\n",
    "    ## PREP NTL\n",
    "    transform_target(viirs_gdf, 'median_rad_' + str(year), n_ntl_bins)\n",
    "\n",
    "    ## Total pixels in each category\n",
    "    print(viirs_gdf[FINAL_TARGET_NAME].value_counts())\n",
    "\n",
    "    ## Create Sample\n",
    "    # Subsets VIIRS dataframe\n",
    "    min_bin_count = min(viirs_gdf[FINAL_TARGET_NAME].value_counts())\n",
    "    gdf = sample_by_target(viirs_gdf, FINAL_TARGET_NAME, min_ntl_bin_count)\n",
    "\n",
    "    ## Path to DTL Files\n",
    "    DTL_DIRECTORY_DATA = os.path.join(DTL_DIRECTORY, str(year))\n",
    "    \n",
    "    ## Match DTL TO NTL\n",
    "    DTL, processed_gdf = fe.map_DTL_NTL(gdf, DTL_DIRECTORY_DATA, bands, image_height, image_width, year)\n",
    "    NTL = processed_gdf[FINAL_TARGET_NAME].to_numpy()\n",
    "    NTL_continuous = processed_gdf['median_rad_'+ str(year)].to_numpy()\n",
    "\n",
    "    ## Save - Dropbox\n",
    "    print(\"Saving NTL\")\n",
    "    np.save(os.path.join(CNN_DIR_WITH_PARAMS, f'ntl_{str(year)}.npy' ), NTL)\n",
    "    np.save(os.path.join(CNN_DIR_WITH_PARAMS, f'ntl_continuous_{str(year)}.npy'), NTL_continuous)\n",
    "\n",
    "    print(\"Saving DTL\")\n",
    "    np.save(os.path.join(CNN_DIR_WITH_PARAMS, f'dtl_{str(year)}.npy'), DTL)\n",
    "\n",
    "    ## Save - Google Drive\n",
    "    print(\"Saving NTL\")\n",
    "    np.save(os.path.join(CNN_DIR_GD_WITH_PARAMS, f'ntl_{str(year)}.npy' ), NTL)\n",
    "    np.save(os.path.join(CNN_DIR_GD_WITH_PARAMS, f'ntl_continuous_{str(year)}.npy'), NTL_continuous)\n",
    "\n",
    "    print(\"Saving DTL\")\n",
    "    np.save(os.path.join(CNN_DIR_GD_WITH_PARAMS, f'dtl_{str(year)}.npy'), DTL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-copyright",
   "metadata": {},
   "source": [
    "## Implement Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-adobe",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_cnn_data(bands = ['4', '3', '2'], n_ntl_bins = 3, min_ntl_bin_count = 50, year = 2014)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-month",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-integration",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parliamentary-search",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "printable-discharge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Save parameters for later use\n",
    "cnn_param_dict = {'image_height': 1, \n",
    "                'image_width': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "approximate-hierarchy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Locally\n",
    "with open(os.path.join(LOCAL_DIR, 'CNN_parameters.json'), 'w') as fp:\n",
    "    json.dump(cnn_param_dict, fp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "received-manhattan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send to s3\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join('CNN', 'testdelete123', 'CNN_parameters.json')).upload_file(os.path.join(LOCAL_DIR, 'CNN_parameters.json'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historical-diploma",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-playing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-hazard",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
