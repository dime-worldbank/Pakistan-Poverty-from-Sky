{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data for CNN\n",
    "\n",
    "Prepares data for CNN. \n",
    "1. Outputs numpy arrays of DTL values and NTL labels\n",
    "2. Creates parameter dictionary (eg, number of NTL labels)\n",
    "\n",
    "\n",
    "Some notes on AWS:\n",
    "1. Use the conda_python3 environment to install geopandas and rasterio (Takes a while).\n",
    "2. Large minimum bin sizes will take a long time. A single band with a mostly full 16814 minimum bin size takes about 10-12 hours. \n",
    "3. A more powerful instance type does not seem to affect runtime.\n",
    "4. Might be worth babysitting it while it runs. Time outs or connection issues seem to interrupt the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Libraries ###\n",
    "import os, datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import logging, os \n",
    "\n",
    "### User Defined Libraries ###\n",
    "import config as cf\n",
    "import feature_extraction as fe\n",
    "\n",
    "### Set Seeds ###\n",
    "seed_value = 42\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "### Parameters / Paths ###\n",
    "FINAL_TARGET_NAME = 'ntl_bins'\n",
    "VIIRS_GDF_FILEPATH = cf.VIIRS_GDF_FILEPATH\n",
    "DTL_DIRECTORY = cf.DTL_DIRECTORY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    '''\n",
    "    Normalizes features.\n",
    "    '''\n",
    "    return X.astype('float32') / 255.0\n",
    "\n",
    "def prep_cnn_data(bands, n_ntl_bins, min_ntl_bin_count, year):\n",
    "\n",
    "    # PARAMETERS -------------------------------------------------------------\n",
    "\n",
    "    ## Define Parameters\n",
    "    # Daytime image parameters\n",
    "    image_height = 48 # VGG16 needs images to be rescale to 224x224\n",
    "    image_width = 48\n",
    "    N_bands = len(bands)\n",
    "\n",
    "    ## Save parameters for later use\n",
    "    cnn_param_dict = {'image_height': image_height, \n",
    "                    'image_width': image_width,\n",
    "                    'bands': bands,\n",
    "                    'N_bands': N_bands,\n",
    "                    'n_ntl_bins': n_ntl_bins,\n",
    "                    'min_ntl_bin_count': min_ntl_bin_count}\n",
    "\n",
    "    # Make directory for these parameters\n",
    "    params_str = 'Nbands' + str(N_bands) + \"_nNtlBins\" + str(n_ntl_bins) + \"_minNTLbinCount\" + str(min_ntl_bin_count)\n",
    "\n",
    "    # Save Locally\n",
    "    with open(os.path.join(LOCAL_DIR, 'CNN_parameters.json'), 'w') as fp:\n",
    "        json.dump(cnn_param_dict, fp)\n",
    "        \n",
    "    # Send to s3\n",
    "    boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join('CNN', params_str, 'CNN_parameters.json')).upload_file(os.path.join(LOCAL_DIR, 'CNN_parameters.json'))\n",
    "\n",
    "    # Run --------------------------------------------------------------------\n",
    "\n",
    "    ## LOAD DATA\n",
    "    viirs = pd.read_pickle(s3.open('{}/{}'.format(bucket, os.path.join('VIIRS', 'FinalData', 'viirs_annual_polygon.pkl'))))\n",
    "    viirs_gdf = gpd.GeoDataFrame(viirs, geometry='geometry')\n",
    "    viirs_gdf = viirs_gdf[ ~ np.isnan(viirs_gdf['tile_id'])]\n",
    "\n",
    "    ## PREP NTL\n",
    "    transform_target(viirs_gdf, 'median_rad_' + str(year), n_ntl_bins)\n",
    "\n",
    "    ## Total pixels in each category\n",
    "    print(viirs_gdf[FINAL_TARGET_NAME].value_counts())\n",
    "\n",
    "    ## Create Sample\n",
    "    # Subsets VIIRS dataframe\n",
    "    min_bin_count = min(viirs_gdf[FINAL_TARGET_NAME].value_counts())\n",
    "    gdf = sample_by_target(viirs_gdf, FINAL_TARGET_NAME, min_ntl_bin_count)\n",
    "\n",
    "    ## Path to DTL Files\n",
    "    DTL_DIRECTORY_DATA = os.path.join('Landsat','l8', str(year))\n",
    "    \n",
    "    ## Match DTL TO NTL\n",
    "    DTL, processed_gdf = fe.map_DTL_NTL(gdf, DTL_DIRECTORY_DATA, bands, image_height, image_width, year)\n",
    "    NTL = processed_gdf[FINAL_TARGET_NAME].to_numpy()\n",
    "    NTL_continuous = processed_gdf['median_rad_'+ str(year)].to_numpy()\n",
    "    \n",
    "    ## Save Locally\n",
    "    print(\"Saving\")\n",
    "    np.save(os.path.join(LOCAL_DIR, f'ntl_{str(year)}.npy'), NTL)\n",
    "    np.save(os.path.join(LOCAL_DIR, f'ntl_continuous_{str(year)}.npy'), NTL_continuous)\n",
    "    np.save(os.path.join(LOCAL_DIR, f'dtl_{str(year)}.npy'), DTL)\n",
    "    \n",
    "    ## Send to s3\n",
    "    print(\"Sending to s3\")\n",
    "    boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join('CNN', params_str, f'ntl_{str(year)}.npy')).upload_file(os.path.join(LOCAL_DIR, f'ntl_{str(year)}.npy'))\n",
    "    boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join('CNN', params_str, f'ntl_continuous_{str(year)}.npy')).upload_file(os.path.join(LOCAL_DIR, f'ntl_continuous_{str(year)}.npy'))\n",
    "    boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join('CNN', params_str, f'dtl_{str(year)}.npy')).upload_file(os.path.join(LOCAL_DIR, f'dtl_{str(year)}.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample NTL Locations and Prep NTL Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD DATA\n",
    "viirs = pd.read_pickle(VIIRS_GDF_FILEPATH)\n",
    "viirs_gdf = gpd.GeoDataFrame(viirs, geometry='geometry')\n",
    "viirs_gdf = viirs_gdf[ ~ np.isnan(viirs_gdf['tile_id'])]\n",
    "\n",
    "## PREP NTL\n",
    "transform_target(viirs_gdf, 'median_rad_' + str(year), n_ntl_bins)\n",
    "\n",
    "## Total pixels in each category\n",
    "print(viirs_gdf[FINAL_TARGET_NAME].value_counts())\n",
    "\n",
    "## Create Sample\n",
    "# Subsets VIIRS dataframe\n",
    "min_bin_count = min(viirs_gdf[FINAL_TARGET_NAME].value_counts())\n",
    "gdf = sample_by_target(viirs_gdf, FINAL_TARGET_NAME, min_ntl_bin_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "viirs = pd.read_pickle(VIIRS_GDF_FILEPATH)\n",
    "viirs_gdf = gpd.GeoDataFrame(viirs, geometry='geometry')\n",
    "viirs_gdf = viirs_gdf[ ~ np.isnan(viirs_gdf['tile_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "viirs_gdf['median_rad_2014_3bin'] = transform_target(viirs_gdf['median_rad_2014'], 4)\n",
    "viirs_gdf['median_rad_2014_5bin'] = transform_target(viirs_gdf['median_rad_2014'], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_bin_count = min(viirs_gdf['median_rad_2014_3bin'].value_counts())\n",
    "gdf = sample_by_target(viirs_gdf, 'median_rad_2014_3bin', min_bin_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0    9468\n",
       "2.0    9468\n",
       "1.0    9468\n",
       "0.0    9468\n",
       "Name: median_rad_2014_3bin, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf.median_rad_2014_3bin.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Daytime Imagery to OPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    1629520\n",
      "1.0     166553\n",
      "2.0      16814\n",
      "Name: ntl_bins, dtype: int64\n",
      "0/75\n",
      "10/75\n",
      "20/75\n",
      "30/75\n",
      "40/75\n",
      "50/75\n",
      "60/75\n",
      "70/75\n",
      "Saving\n",
      "Sending to s3\n"
     ]
    }
   ],
   "source": [
    "prep_cnn_data(bands = ['4', '3', '2'], n_ntl_bins = 3, min_ntl_bin_count = 25, year = 2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New function to prep data for each individual band\n",
    "def prep_individband_cnn_data_(band, n_ntl_bins, min_ntl_bin_count, year):\n",
    "\n",
    "    # PARAMETERS -------------------------------------------------------------\n",
    "\n",
    "    ## Define Parameters\n",
    "    # Daytime image parameters\n",
    "    image_height = 48 # VGG16 needs images to be rescale to 224x224\n",
    "    image_width = 48\n",
    "    N_band = len(band)\n",
    "\n",
    "    ## Save parameters for later use\n",
    "    cnn_param_dict = {'image_height': image_height, \n",
    "                    'image_width': image_width,\n",
    "                    'bands': band,\n",
    "                    'N_bands': N_band,\n",
    "                    'n_ntl_bins': n_ntl_bins,\n",
    "                    'min_ntl_bin_count': min_ntl_bin_count}\n",
    "\n",
    "    # Make directory for these parameters\n",
    "    params_str = 'Band' + band[0] + \"_nNtlBins\" + str(n_ntl_bins) + \"_minNTLbinCount\" + str(min_ntl_bin_count)\n",
    "\n",
    "    # Save Locally\n",
    "    with open(os.path.join(LOCAL_DIR, 'CNN_parameters.json'), 'w') as fp:\n",
    "        json.dump(cnn_param_dict, fp)\n",
    "        \n",
    "    # Send to s3\n",
    "    boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join('CNN', params_str, 'CNN_parameters.json')).upload_file(os.path.join(LOCAL_DIR, 'CNN_parameters.json'))\n",
    "\n",
    "    # Run --------------------------------------------------------------------\n",
    "\n",
    "    ## LOAD DATA\n",
    "    viirs = pd.read_pickle(s3.open('{}/{}'.format(bucket, os.path.join('VIIRS', 'FinalData', 'viirs_annual_polygon.pkl'))))\n",
    "    viirs_gdf = gpd.GeoDataFrame(viirs, geometry='geometry')\n",
    "    viirs_gdf = viirs_gdf[ ~ np.isnan(viirs_gdf['tile_id'])]\n",
    "\n",
    "    ## PREP NTL\n",
    "    transform_target(viirs_gdf, 'median_rad_' + str(year), n_ntl_bins)\n",
    "\n",
    "    ## Total pixels in each category\n",
    "    print(viirs_gdf[FINAL_TARGET_NAME].value_counts())\n",
    "\n",
    "    ## Create Sample\n",
    "    # Subsets VIIRS dataframe\n",
    "    min_bin_count = min(viirs_gdf[FINAL_TARGET_NAME].value_counts())\n",
    "    gdf = sample_by_target(viirs_gdf, FINAL_TARGET_NAME, min_ntl_bin_count)\n",
    "\n",
    "    ## Path to DTL Files\n",
    "    DTL_DIRECTORY_DATA = os.path.join('Landsat','l8', str(year))\n",
    "    \n",
    "    ## Match DTL TO NTL\n",
    "    DTL, processed_gdf = fe.map_DTL_NTL(gdf, DTL_DIRECTORY_DATA, band, image_height, image_width, year)\n",
    "    NTL = processed_gdf[FINAL_TARGET_NAME].to_numpy()\n",
    "    NTL_continuous = processed_gdf['median_rad_'+ str(year)].to_numpy()\n",
    "    \n",
    "    ## Save Locally\n",
    "    print(\"Saving\")\n",
    "    np.save(os.path.join(LOCAL_DIR, f'ntl_{str(year)}.npy'), NTL)\n",
    "    np.save(os.path.join(LOCAL_DIR, f'ntl_continuous_{str(year)}.npy'), NTL_continuous)\n",
    "    np.save(os.path.join(LOCAL_DIR, f'dtl_{str(year)}.npy'), DTL)\n",
    "    \n",
    "    ## Send to s3\n",
    "    print(\"Sending to s3\")\n",
    "    boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join('CNN', params_str, f'ntl_{str(year)}.npy')).upload_file(os.path.join(LOCAL_DIR, f'ntl_{str(year)}.npy'))\n",
    "    boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join('CNN', params_str, f'ntl_continuous_{str(year)}.npy')).upload_file(os.path.join(LOCAL_DIR, f'ntl_continuous_{str(year)}.npy'))\n",
    "    boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join('CNN', params_str, f'dtl_{str(year)}.npy')).upload_file(os.path.join(LOCAL_DIR, f'dtl_{str(year)}.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_individband_cnn_data_(band = ['7'], n_ntl_bins = 3, min_ntl_bin_count = 16814, year = 2014) #Takes a long time with large bin counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
